{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import zipfile\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cuda\n",
      "Current : 0\n",
      "Count : 1\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "print('device :', device)\n",
    "print('Current :', torch.cuda.current_device())\n",
    "print('Count :', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>등록번호</th>\n",
       "      <th>생년월일</th>\n",
       "      <th>성별</th>\n",
       "      <th>진료의</th>\n",
       "      <th>검사 시 나이</th>\n",
       "      <th>신장</th>\n",
       "      <th>체중</th>\n",
       "      <th>BMI</th>\n",
       "      <th>처방일자</th>\n",
       "      <th>시행일자</th>\n",
       "      <th>BA 1</th>\n",
       "      <th>BA 2</th>\n",
       "      <th>Unnamed: 14</th>\n",
       "      <th>No</th>\n",
       "      <th>boneage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1698</td>\n",
       "      <td>8255049</td>\n",
       "      <td>2007-08-03</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>9.969863</td>\n",
       "      <td>129.5</td>\n",
       "      <td>26.9</td>\n",
       "      <td>16.1</td>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>2017-07-20</td>\n",
       "      <td>9.75</td>\n",
       "      <td>9.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.jpg</td>\n",
       "      <td>9.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1897</td>\n",
       "      <td>8537405</td>\n",
       "      <td>2008-08-22</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>9.989041</td>\n",
       "      <td>132.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>2018-02-28</td>\n",
       "      <td>2018-08-16</td>\n",
       "      <td>10.50</td>\n",
       "      <td>11.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.jpg</td>\n",
       "      <td>10.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1422</td>\n",
       "      <td>7942635</td>\n",
       "      <td>2005-01-19</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10.008219</td>\n",
       "      <td>136.4</td>\n",
       "      <td>33.2</td>\n",
       "      <td>17.9</td>\n",
       "      <td>2015-01-20</td>\n",
       "      <td>2015-01-20</td>\n",
       "      <td>11.00</td>\n",
       "      <td>11.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.jpg</td>\n",
       "      <td>11.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1475</td>\n",
       "      <td>7995857</td>\n",
       "      <td>2005-02-09</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10.049315</td>\n",
       "      <td>133.5</td>\n",
       "      <td>31.2</td>\n",
       "      <td>17.6</td>\n",
       "      <td>2015-02-25</td>\n",
       "      <td>2015-02-25</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.jpg</td>\n",
       "      <td>10.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1888</td>\n",
       "      <td>8520261</td>\n",
       "      <td>2008-09-11</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10.060274</td>\n",
       "      <td>130.6</td>\n",
       "      <td>23.7</td>\n",
       "      <td>13.9</td>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>10.00</td>\n",
       "      <td>9.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.jpg</td>\n",
       "      <td>9.875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Group     등록번호        생년월일 성별 진료의    검사 시 나이     신장    체중   BMI  \\\n",
       "0   1698  8255049  2007-08-03  F   1   9.969863  129.5  26.9  16.1   \n",
       "1   1897  8537405  2008-08-22  F   1   9.989041  132.0  31.0  17.8   \n",
       "2   1422  7942635  2005-01-19  F   1  10.008219  136.4  33.2  17.9   \n",
       "3   1475  7995857  2005-02-09  F   1  10.049315  133.5  31.2  17.6   \n",
       "4   1888  8520261  2008-09-11  F   1  10.060274  130.6  23.7  13.9   \n",
       "\n",
       "         처방일자        시행일자   BA 1   BA 2 Unnamed: 14     No  boneage  \n",
       "0  2017-01-09  2017-07-20   9.75   9.75         NaN  1.jpg    9.750  \n",
       "1  2018-02-28  2018-08-16  10.50  11.00         NaN  2.jpg   10.750  \n",
       "2  2015-01-20  2015-01-20  11.00  11.25         NaN  3.jpg   11.125  \n",
       "3  2015-02-25  2015-02-25  10.00  10.25         NaN  4.jpg   10.125  \n",
       "4  2018-10-01  2018-10-01  10.00   9.75         NaN  5.jpg    9.875  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('골밀도 데이터/total_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_img(r1,r2,r3,r4):\n",
    "    tmp_binary_img = []\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8)) #CLAHE 생성\n",
    "    for img in [r1,r2,r3,r4]:\n",
    "        if img is not None:\n",
    "            # resized_img = cv2.resize(img,(1000,400)) # (400, 500)\n",
    "            blured_img = cv2.GaussianBlur(img,(5,5),0)            \n",
    "            clahed_img = clahe.apply(blured_img)          #CLAHE 적용\n",
    "            # _,binary_img = cv2.threshold(clahed_img,clahed_img.mean()*1.25,255,cv2.THRESH_BINARY)\n",
    "            \n",
    "            target_length = 500\n",
    "            (original_height, original_width) = clahed_img.shape\n",
    "            # 가로세로 비율을 유지하면서 긴 부분을 target_length로 조정합니다.\n",
    "            if original_width > original_height:\n",
    "                # 가로가 길 경우\n",
    "                new_width = target_length\n",
    "                new_height = int((new_width / original_width) * original_height)\n",
    "            else:\n",
    "                # 세로가 길 경우\n",
    "                new_height = target_length\n",
    "                new_width = int((new_height / original_height) * original_width)\n",
    "\n",
    "            # 이미지 크기 조정\n",
    "            resized_image = cv2.resize(clahed_img, (new_width, new_height))\n",
    "            \n",
    "            # 최종 이미지 크기\n",
    "            target_size = 600\n",
    "            old_size = resized_image.shape\n",
    "\n",
    "            # 새로운 이미지 생성 (검은색 배경)\n",
    "            new_image = np.zeros((target_size, target_size), dtype=np.uint8)\n",
    "\n",
    "            # 이미지 중앙에 배치하기 위한 좌표 계산\n",
    "            start_x = (target_size - old_size[1]) // 2\n",
    "            start_y = (target_size - old_size[0]) // 2\n",
    "\n",
    "            # 원본 이미지를 중앙에 배치\n",
    "            new_image[start_y:start_y+old_size[0], start_x:start_x+old_size[1]] = resized_image\n",
    "            \n",
    "            tmp_binary_img.append(new_image)\n",
    "                \n",
    "    return np.array(tmp_binary_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_img(img, roi_1, roi_2, roi_3, roi_4):\n",
    "    \n",
    "    cropped_roi_1_img = img[roi_1[0][1]:roi_1[1][1],roi_1[0][0]:roi_1[1][0]]\n",
    "    cropped_roi_2_img = img[roi_2[0][1]:roi_2[1][1],roi_2[0][0]:roi_2[1][0]]\n",
    "    cropped_roi_3_img = img[roi_3[0][1]:roi_3[1][1],roi_3[0][0]:roi_3[1][0]]\n",
    "    cropped_roi_4_img = img[roi_4[0][1]:roi_4[1][1],roi_4[0][0]:roi_4[1][0]]\n",
    "\n",
    "    optimzed_imgs = optimize_img(cropped_roi_1_img, cropped_roi_2_img, cropped_roi_3_img, cropped_roi_4_img)\n",
    "    return optimzed_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data augmentation - rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, y_data = [], []\n",
    "X_aug_rot, y_aug_rot = [], []\n",
    "X_aug_noise, y_aug_noise = [], []\n",
    "\n",
    "for k in range(len(data)):\n",
    "    if k == 354 or k == 355 or k == 916:\n",
    "        continue\n",
    "    img0 = cv2.imread('골밀도 데이터/rotate_image/' + data.No[k], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # 이미지 thresholding\n",
    "    r_img = np.copy(img0)\n",
    "    height, width = img0.shape\n",
    "    img = img0[0:(int)(height*0.9),0:(int)(width*0.95)]\n",
    "    ret, img = cv2.threshold(img, img0.mean(), 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # 이미지 contouring\n",
    "    contours, hierarchy = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    max_cnt = max(contours, key = cv2.contourArea)\n",
    "    mask = np.zeros(img.shape, dtype= np.uint8)\n",
    "    cv2.drawContours(mask, [max_cnt], -1, (255, 255, 255), -1)\n",
    "\n",
    "    # 볼록한 점 구하기\n",
    "    hull = cv2.convexHull(max_cnt, returnPoints= False)\n",
    "    hull1 = cv2.convexHull(max_cnt)\n",
    "\n",
    "    # 오목한 지점 구하기\n",
    "    defects = cv2.convexityDefects(max_cnt, hull) # 인덱스로 반환\n",
    "\n",
    "    # 거리를 저장할 수 있는 공간 생성\n",
    "    di = []\n",
    "\n",
    "    for index in range(defects.shape[0]):\n",
    "        # 시작점, 끝점, far 점, 거리 할당\n",
    "        sp, ep, fp, distance = defects[index, 0]\n",
    "        \n",
    "        # 거리 저장\n",
    "        di.append(distance)\n",
    "\n",
    "    far_xrange = []\n",
    "    far_yrange = []\n",
    "    start_xrange = []\n",
    "    start_yrange = []\n",
    "\n",
    "    # 가장 오목하게 들어가 있는 부분을 찾기 위해 sorting(내림차순)\n",
    "    di = np.array(di)\n",
    "    s_di = np.sort(di)[::-1]\n",
    "\n",
    "    # 내림차순된 거리들을 6개만 뽑아내기 위해 slice\n",
    "    for i in list(s_di[:6]):\n",
    "        index = np.where(di == i)[0][0]\n",
    "        \n",
    "        sp, ep, fp, _ = defects[index, 0]\n",
    "        \n",
    "        far_xrange.append(max_cnt[fp][0][0])\n",
    "        far_yrange.append(max_cnt[fp][0][1])\n",
    "        \n",
    "        start_xrange.append(max_cnt[sp][0][0])\n",
    "        start_yrange.append(max_cnt[sp][0][1])\n",
    "        \n",
    "\n",
    "    #손목뼈 ROI\n",
    "    carpus_sp = ((int)(min(far_xrange[4:6])*0.90),(int)(max(far_yrange[4:6])*0.90))\n",
    "    carpus_ep = (int(max(far_xrange[4:6])*1.05),(int)(max(far_yrange[4:6])*1.15))\n",
    "\n",
    "    #손목뼈 위쪽에 있는 관절 4개를 추출\n",
    "    four_sp = ((int)(min(far_xrange[0:4])*0.70),int(min(far_yrange[0:4])*0.95))\n",
    "    four_ep = (int(max(far_xrange[0:4])*1.05),(int)(max(far_yrange[0:4])*1.05))\n",
    "\n",
    "    #중지 ROI 추출\n",
    "    #중지 끝 좌표 구하기\n",
    "    for y,x_r in enumerate(mask) :\n",
    "        if 255 in x_r:\n",
    "            #y에 따른 x rows 중 255인 x값 추출\n",
    "            x_255_indexs = np.where(x_r == 255)[0]\n",
    "\n",
    "            #255인 x값들 중 median 추출\n",
    "            x_255_mid_index = x_255_indexs[(int)(len(x_255_indexs)/2)]\n",
    "            first_255_x_point = x_255_mid_index\n",
    "\n",
    "            first_255_y_point = y\n",
    "            break\n",
    "        \n",
    "    ## 중지 끝 좌표에서 처음 오목한 곳의 x 좌표를 뺀 간격만큼\n",
    "    sub = min(abs(first_255_x_point - far_xrange[0]), abs(first_255_x_point - far_xrange[1]))\n",
    "    middle_finger_sp = (int((first_255_x_point - sub*1.5)), int(first_255_y_point*0.85))\n",
    "    middle_finger_ep = (int((first_255_x_point + sub*1.5)), int(far_yrange[0]*1.05))\n",
    "\n",
    "    # # 새끼손가락 좌표\n",
    "    # little_finger_sp = (int(min(end_xrange)*0.7), int(end_yrange[end_xrange.index(min(end_xrange[0:4]))]*0.9))\n",
    "    # little_finger_ep = (int(min(far_xrange[0:4])*0.95), int(far_yrange[far_xrange.index(min(far_xrange[0:4]))]*1.05))\n",
    "\n",
    "    #엄지손가락 좌표\n",
    "    thumb_sp = (int(max(far_xrange[0:4])*1.05), int(start_yrange[start_xrange.index(max(start_xrange))]*0.95))\n",
    "    thumb_ep = (int(max(start_xrange)*1.05), int(max(far_yrange)*0.9))\n",
    "    \n",
    "    optimized_imgs = crop_img(img0,(carpus_sp,carpus_ep), (four_sp, four_ep), (middle_finger_sp,middle_finger_ep), (thumb_sp, thumb_ep))\n",
    "    \n",
    "    row1 = np.vstack((optimized_imgs[0], optimized_imgs[1]))\n",
    "    row2 = np.vstack((optimized_imgs[2], optimized_imgs[3]))\n",
    "\n",
    "    combined_image = np.hstack((row1, row2))\n",
    "    combined_image = cv2.resize(combined_image, (256, 256))\n",
    "\n",
    "    X_data.append(combined_image)\n",
    "    y_data.append(data.boneage[k])\n",
    "    \n",
    "    # data augmentation - rotation\n",
    "    aug_rot_img = []\n",
    "    angle = np.random.randint(0, 90)\n",
    "    for crop_image in optimized_imgs:\n",
    "        # 이미지 중심을 계산\n",
    "        center = (crop_image.shape[1] // 2, crop_image.shape[0] // 2)\n",
    "        rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "\n",
    "        # 회전된 이미지의 경계가 잘리지 않도록 출력 이미지의 크기 조정\n",
    "        cos = np.abs(rotation_matrix[0, 0])\n",
    "        sin = np.abs(rotation_matrix[0, 1])\n",
    "\n",
    "        # 새로운 경계 차원 계산\n",
    "        new_width = int((crop_image.shape[0] * sin) + (crop_image.shape[1] * cos))\n",
    "        new_height = int((crop_image.shape[0] * cos) + (crop_image.shape[1] * sin))\n",
    "\n",
    "        # 변환 행렬의 이동 부분 조정\n",
    "        rotation_matrix[0, 2] += (new_width / 2) - center[0]\n",
    "        rotation_matrix[1, 2] += (new_height / 2) - center[1]\n",
    "\n",
    "        # 회전된 이미지 얻기\n",
    "        rotated_img = cv2.warpAffine(crop_image, rotation_matrix, (new_width, new_height))\n",
    "        rotated_img = cv2.resize(rotated_img, (600, 600))\n",
    "        aug_rot_img.append(rotated_img)\n",
    "        \n",
    "    row1 = np.vstack((aug_rot_img[0], aug_rot_img[1]))\n",
    "    row2 = np.vstack((aug_rot_img[2], aug_rot_img[3]))\n",
    "\n",
    "    combined_image_rot = np.hstack((row1, row2))\n",
    "    combined_image_rot = cv2.resize(combined_image_rot, (256, 256))\n",
    "    \n",
    "    X_aug_rot.append(combined_image_rot)\n",
    "    y_aug_rot.append(data.boneage[k])\n",
    "    \n",
    "    # data augmentation - add noise\n",
    "    aug_noise_img = []\n",
    "    sigma = np.random.randint(10, 50)\n",
    "    for crop_image in optimized_imgs:\n",
    "        row, col = crop_image.shape\n",
    "        \n",
    "        # 노이즈 강도 조정\n",
    "        gaussian = np.random.normal(0, sigma, (row, col))\n",
    "        \n",
    "        # 노이즈 추가된 이미지 얻기\n",
    "        noisy_image = np.clip(crop_image + gaussian, 0, 255)\n",
    "        aug_noise_img.append(noisy_image)\n",
    "        \n",
    "    row1 = np.vstack((aug_noise_img[0], aug_noise_img[1]))\n",
    "    row2 = np.vstack((aug_noise_img[2], aug_noise_img[3]))\n",
    "\n",
    "    combined_image_noise = np.hstack((row1, row2))\n",
    "    combined_image_noise = cv2.resize(combined_image_noise, (256, 256))\n",
    "    \n",
    "    X_aug_noise.append(combined_image_noise)\n",
    "    y_aug_noise.append(data.boneage[k])\n",
    "        \n",
    "X_data = np.array(X_data)\n",
    "y_data = np.array(y_data)\n",
    "X_aug_rot = np.array(X_aug_rot)\n",
    "y_aug_rot = np.array(y_aug_rot)\n",
    "X_aug_noise = np.array(X_aug_noise)\n",
    "y_aug_noise = np.array(y_aug_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 모델 생성 - Attention-Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-05 00:42:13.136172: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-05 00:42:13.136226: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-05 00:42:13.137975: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-05 00:42:13.146235: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-05 00:42:14.079679: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separable 합성곱 함수\n",
    "def separable_conv(x, inchannel,outchannel):\n",
    "  x = keras.layers.Conv2D(inchannel, (3,3), strides=1, padding=\"same\")(x)\n",
    "  x = keras.layers.BatchNormalization()(x)\n",
    "  x = keras.layers.Conv2D(outchannel, (1,1), strides=1, padding=\"same\")(x)\n",
    "  x = keras.layers.BatchNormalization()(x)\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resiual_units 함수 \n",
    "def resiual_units(input_x):\n",
    "  x = keras.layers.ReLU()(input_x)\n",
    "  x = separable_conv(x,x.shape[-1],128)\n",
    "  x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x = keras.layers.ReLU()(x)\n",
    "  x = separable_conv(x,x.shape[-1],256)\n",
    "  x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x = keras.layers.ReLU()(x)\n",
    "  x = separable_conv(x,x.shape[-1],512)\n",
    "  x = keras.layers.BatchNormalization()(x)\n",
    "  \n",
    "  input_x = keras.layers.Add()([x,input_x])\n",
    "\n",
    "  return input_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model middle_flow 함수\n",
    "def middle_flow(input_x):\n",
    "  #encoder\n",
    "  x = keras.layers.MaxPool2D((2,2),padding=\"same\")(input_x)\n",
    "  x = resiual_units(x)\n",
    "  x = keras.layers.MaxPool2D((2,2),padding=\"same\")(x)\n",
    "  x = resiual_units(x)\n",
    "  x = keras.layers.MaxPool2D((2,2),padding=\"same\")(x)\n",
    "  x = resiual_units(x)\n",
    "  \n",
    "  #decoder\n",
    "  x = resiual_units(x)\n",
    "  x = keras.layers.UpSampling2D((2,2),interpolation='bilinear')(x)\n",
    "  x = resiual_units(x)\n",
    "  x = keras.layers.UpSampling2D((2,2),interpolation='bilinear')(x)\n",
    "  x = resiual_units(x)\n",
    "  x = keras.layers.UpSampling2D((2,2),interpolation='bilinear')(x)\n",
    "  \n",
    "  x = separable_conv(x,x.shape[-1],512)\n",
    "  x = separable_conv(x,x.shape[-1],512) \n",
    "  \n",
    "  #sigmoid \n",
    "  x = keras.activations.sigmoid(x)\n",
    "  x = keras.layers.Multiply()([input_x,x])\n",
    "  x = keras.layers.Add()([input_x,x])\n",
    "\n",
    "  x = resiual_units(x)\n",
    "\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-05 00:42:15.598615: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22991 MB memory:  -> device: 0, name: NVIDIA TITAN RTX, pci bus id: 0000:d9:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "#골연령 측정 모델\n",
    "# entry flow model\n",
    "input = keras.Input(shape=(256,256,1))\n",
    "x = keras.layers.Conv2D(32, (3,3), strides = 2)(input)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.ReLU()(x)\n",
    "\n",
    "x = keras.layers.Conv2D(64, (3,3), strides=1)(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.ReLU()(x)\n",
    "\n",
    "#첫번째\n",
    "x1 = keras.layers.Conv2D(128,(1,1),strides=2)(x) \n",
    "x1 = keras.layers.BatchNormalization()(x1)\n",
    "\n",
    "x = separable_conv(x,x.shape[-1],128)\n",
    "x = keras.layers.ReLU()(x)\n",
    "\n",
    "x = separable_conv(x,x.shape[-1],128)\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = keras.layers.MaxPool2D((2,2),2,padding=\"same\")(x)\n",
    "\n",
    "x = keras.layers.Add()([x,x1])\n",
    "\n",
    "#2번째\n",
    "x1 = keras.layers.Conv2D(512,(1,1),strides=2)(x)\n",
    "x1 = keras.layers.BatchNormalization()(x1)\n",
    "\n",
    "x = separable_conv(x,x.shape[-1],512)\n",
    "x = keras.layers.ReLU()(x)\n",
    "\n",
    "x = separable_conv(x,x.shape[-1],512)\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = keras.layers.MaxPool2D((2,2),2,padding=\"same\")(x)\n",
    "\n",
    "x = keras.layers.Add()([x,x1])\n",
    "\n",
    "\n",
    "#middle flow model\n",
    "x = middle_flow(x)\n",
    "\n",
    "\n",
    "#exit flow model\n",
    "x1 = keras.layers.Conv2D(1024,(1,1),strides=2)(x)\n",
    "\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = separable_conv(x,x.shape[-1],728)\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = separable_conv(x,x.shape[-1],1024)\n",
    "x = keras.layers.MaxPool2D((2,2),2,padding=\"same\")(x)\n",
    "\n",
    "x = keras.layers.Add()([x,x1])\n",
    "\n",
    "\n",
    "x = separable_conv(x,x.shape[-1],1536)\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = separable_conv(x,x.shape[-1],2048)\n",
    "x = keras.layers.ReLU()(x)\n",
    "\n",
    "x = keras.layers.GlobalAvgPool2D()(x)\n",
    "\n",
    "x = keras.layers.Dense(1000,activation='relu')(x)\n",
    "x = keras.layers.Dense(256,activation='relu')(x)\n",
    "x = keras.layers.Dense(1)(x)\n",
    "\n",
    "model = keras.models.Model(input,x)\n",
    "model.compile(optimizer='adam', loss='mae', metrics=['mae','mse'], run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiuser/.conda/envs/junoflow/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('./tjnet_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 256, 256, 1)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 127, 127, 32)         320       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 127, 127, 32)         128       ['conv2d[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                (None, 127, 127, 32)         0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 125, 125, 64)         18496     ['re_lu[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 125, 125, 64)         256       ['conv2d_1[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)              (None, 125, 125, 64)         0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 125, 125, 64)         36928     ['re_lu_1[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 125, 125, 64)         256       ['conv2d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 125, 125, 128)        8320      ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 125, 125, 128)        512       ['conv2d_4[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)              (None, 125, 125, 128)        0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 125, 125, 128)        147584    ['re_lu_2[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 125, 125, 128)        512       ['conv2d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, 125, 125, 128)        16512     ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 125, 125, 128)        512       ['conv2d_6[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)              (None, 125, 125, 128)        0         ['batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 63, 63, 128)          8320      ['re_lu_1[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2  (None, 63, 63, 128)          0         ['re_lu_3[0][0]']             \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 63, 63, 128)          512       ['conv2d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 63, 63, 128)          0         ['max_pooling2d[0][0]',       \n",
      "                                                                     'batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)           (None, 63, 63, 128)          147584    ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 63, 63, 128)          512       ['conv2d_8[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)           (None, 63, 63, 512)          66048     ['batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 63, 63, 512)          2048      ['conv2d_9[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)              (None, 63, 63, 512)          0         ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)          (None, 63, 63, 512)          2359808   ['re_lu_4[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, 63, 63, 512)          2048      ['conv2d_10[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)          (None, 63, 63, 512)          262656    ['batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, 63, 63, 512)          2048      ['conv2d_11[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)              (None, 63, 63, 512)          0         ['batch_normalization_11[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)           (None, 32, 32, 512)          66048     ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPoolin  (None, 32, 32, 512)          0         ['re_lu_5[0][0]']             \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 32, 32, 512)          2048      ['conv2d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 32, 32, 512)          0         ['max_pooling2d_1[0][0]',     \n",
      "                                                                     'batch_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPoolin  (None, 16, 16, 512)          0         ['add_1[0][0]']               \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " re_lu_6 (ReLU)              (None, 16, 16, 512)          0         ['max_pooling2d_2[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)          (None, 16, 16, 512)          2359808   ['re_lu_6[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_12 (Ba  (None, 16, 16, 512)          2048      ['conv2d_12[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)          (None, 16, 16, 128)          65664     ['batch_normalization_12[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_13 (Ba  (None, 16, 16, 128)          512       ['conv2d_13[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_14 (Ba  (None, 16, 16, 128)          512       ['batch_normalization_13[0][0]\n",
      " tchNormalization)                                                  ']                            \n",
      "                                                                                                  \n",
      " re_lu_7 (ReLU)              (None, 16, 16, 128)          0         ['batch_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)          (None, 16, 16, 128)          147584    ['re_lu_7[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_15 (Ba  (None, 16, 16, 128)          512       ['conv2d_14[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)          (None, 16, 16, 256)          33024     ['batch_normalization_15[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_16 (Ba  (None, 16, 16, 256)          1024      ['conv2d_15[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_17 (Ba  (None, 16, 16, 256)          1024      ['batch_normalization_16[0][0]\n",
      " tchNormalization)                                                  ']                            \n",
      "                                                                                                  \n",
      " re_lu_8 (ReLU)              (None, 16, 16, 256)          0         ['batch_normalization_17[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)          (None, 16, 16, 256)          590080    ['re_lu_8[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_18 (Ba  (None, 16, 16, 256)          1024      ['conv2d_16[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)          (None, 16, 16, 512)          131584    ['batch_normalization_18[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_19 (Ba  (None, 16, 16, 512)          2048      ['conv2d_17[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_20 (Ba  (None, 16, 16, 512)          2048      ['batch_normalization_19[0][0]\n",
      " tchNormalization)                                                  ']                            \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 16, 16, 512)          0         ['batch_normalization_20[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'max_pooling2d_2[0][0]']     \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPoolin  (None, 8, 8, 512)            0         ['add_2[0][0]']               \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " re_lu_9 (ReLU)              (None, 8, 8, 512)            0         ['max_pooling2d_3[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)          (None, 8, 8, 512)            2359808   ['re_lu_9[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_21 (Ba  (None, 8, 8, 512)            2048      ['conv2d_18[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)          (None, 8, 8, 128)            65664     ['batch_normalization_21[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_22 (Ba  (None, 8, 8, 128)            512       ['conv2d_19[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_23 (Ba  (None, 8, 8, 128)            512       ['batch_normalization_22[0][0]\n",
      " tchNormalization)                                                  ']                            \n",
      "                                                                                                  \n",
      " re_lu_10 (ReLU)             (None, 8, 8, 128)            0         ['batch_normalization_23[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)          (None, 8, 8, 128)            147584    ['re_lu_10[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_24 (Ba  (None, 8, 8, 128)            512       ['conv2d_20[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)          (None, 8, 8, 256)            33024     ['batch_normalization_24[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_25 (Ba  (None, 8, 8, 256)            1024      ['conv2d_21[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_26 (Ba  (None, 8, 8, 256)            1024      ['batch_normalization_25[0][0]\n",
      " tchNormalization)                                                  ']                            \n",
      "                                                                                                  \n",
      " re_lu_11 (ReLU)             (None, 8, 8, 256)            0         ['batch_normalization_26[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)          (None, 8, 8, 256)            590080    ['re_lu_11[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_27 (Ba  (None, 8, 8, 256)            1024      ['conv2d_22[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)          (None, 8, 8, 512)            131584    ['batch_normalization_27[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_28 (Ba  (None, 8, 8, 512)            2048      ['conv2d_23[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_29 (Ba  (None, 8, 8, 512)            2048      ['batch_normalization_28[0][0]\n",
      " tchNormalization)                                                  ']                            \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 8, 8, 512)            0         ['batch_normalization_29[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'max_pooling2d_3[0][0]']     \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPoolin  (None, 4, 4, 512)            0         ['add_3[0][0]']               \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " re_lu_12 (ReLU)             (None, 4, 4, 512)            0         ['max_pooling2d_4[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)          (None, 4, 4, 512)            2359808   ['re_lu_12[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_30 (Ba  (None, 4, 4, 512)            2048      ['conv2d_24[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)          (None, 4, 4, 128)            65664     ['batch_normalization_30[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_31 (Ba  (None, 4, 4, 128)            512       ['conv2d_25[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_32 (Ba  (None, 4, 4, 128)            512       ['batch_normalization_31[0][0]\n",
      " tchNormalization)                                                  ']                            \n",
      "                                                                                                  \n",
      " re_lu_13 (ReLU)             (None, 4, 4, 128)            0         ['batch_normalization_32[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)          (None, 4, 4, 128)            147584    ['re_lu_13[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_33 (Ba  (None, 4, 4, 128)            512       ['conv2d_26[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)          (None, 4, 4, 256)            33024     ['batch_normalization_33[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_34 (Ba  (None, 4, 4, 256)            1024      ['conv2d_27[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_35 (Ba  (None, 4, 4, 256)            1024      ['batch_normalization_34[0][0]\n",
      " tchNormalization)                                                  ']                            \n",
      "                                                                                                  \n",
      " re_lu_14 (ReLU)             (None, 4, 4, 256)            0         ['batch_normalization_35[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)          (None, 4, 4, 256)            590080    ['re_lu_14[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_36 (Ba  (None, 4, 4, 256)            1024      ['conv2d_28[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)          (None, 4, 4, 512)            131584    ['batch_normalization_36[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_37 (Ba  (None, 4, 4, 512)            2048      ['conv2d_29[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_38 (Ba  (None, 4, 4, 512)            2048      ['batch_normalization_37[0][0]\n",
      " tchNormalization)                                                  ']                            \n",
      "                                                                                                  \n",
      " add_4 (Add)                 (None, 4, 4, 512)            0         ['batch_normalization_38[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'max_pooling2d_4[0][0]']     \n",
      "                                                                                                  \n",
      " re_lu_15 (ReLU)             (None, 4, 4, 512)            0         ['add_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)          (None, 4, 4, 512)            2359808   ['re_lu_15[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_39 (Ba  (None, 4, 4, 512)            2048      ['conv2d_30[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)          (None, 4, 4, 128)            65664     ['batch_normalization_39[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_40 (Ba  (None, 4, 4, 128)            512       ['conv2d_31[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_41 (Ba  (None, 4, 4, 128)            512       ['batch_normalization_40[0][0]\n",
      " tchNormalization)                                                  ']                            \n",
      "                                                                                                  \n",
      " re_lu_16 (ReLU)             (None, 4, 4, 128)            0         ['batch_normalization_41[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)          (None, 4, 4, 128)            147584    ['re_lu_16[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_42 (Ba  (None, 4, 4, 128)            512       ['conv2d_32[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)          (None, 4, 4, 256)            33024     ['batch_normalization_42[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_43 (Ba  (None, 4, 4, 256)            1024      ['conv2d_33[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_44 (Ba  (None, 4, 4, 256)            1024      ['batch_normalization_43[0][0]\n",
      " tchNormalization)                                                  ']                            \n",
      "                                                                                                  \n",
      " re_lu_17 (ReLU)             (None, 4, 4, 256)            0         ['batch_normalization_44[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)          (None, 4, 4, 256)            590080    ['re_lu_17[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_45 (Ba  (None, 4, 4, 256)            1024      ['conv2d_34[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)          (None, 4, 4, 512)            131584    ['batch_normalization_45[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_46 (Ba  (None, 4, 4, 512)            2048      ['conv2d_35[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_47 (Ba  (None, 4, 4, 512)            2048      ['batch_normalization_46[0][0]\n",
      " tchNormalization)                                                  ']                            \n",
      "                                                                                                  \n",
      " add_5 (Add)                 (None, 4, 4, 512)            0         ['batch_normalization_47[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'add_4[0][0]']               \n",
      "                                                                                                  \n",
      " up_sampling2d (UpSampling2  (None, 8, 8, 512)            0         ['add_5[0][0]']               \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " re_lu_18 (ReLU)             (None, 8, 8, 512)            0         ['up_sampling2d[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)          (None, 8, 8, 512)            2359808   ['re_lu_18[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_48 (Ba  (None, 8, 8, 512)            2048      ['conv2d_36[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)          (None, 8, 8, 128)            65664     ['batch_normalization_48[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_49 (Ba  (None, 8, 8, 128)            512       ['conv2d_37[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_50 (Ba  (None, 8, 8, 128)            512       ['batch_normalization_49[0][0]\n",
      " tchNormalization)                                                  ']                            \n",
      "                                                                                                  \n",
      " re_lu_19 (ReLU)             (None, 8, 8, 128)            0         ['batch_normalization_50[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)          (None, 8, 8, 128)            147584    ['re_lu_19[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_51 (Ba  (None, 8, 8, 128)            512       ['conv2d_38[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)          (None, 8, 8, 256)            33024     ['batch_normalization_51[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_52 (Ba  (None, 8, 8, 256)            1024      ['conv2d_39[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_53 (Ba  (None, 8, 8, 256)            1024      ['batch_normalization_52[0][0]\n",
      " tchNormalization)                                                  ']                            \n",
      "                                                                                                  \n",
      " re_lu_20 (ReLU)             (None, 8, 8, 256)            0         ['batch_normalization_53[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)          (None, 8, 8, 256)            590080    ['re_lu_20[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_54 (Ba  (None, 8, 8, 256)            1024      ['conv2d_40[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)          (None, 8, 8, 512)            131584    ['batch_normalization_54[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_55 (Ba  (None, 8, 8, 512)            2048      ['conv2d_41[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_56 (Ba  (None, 8, 8, 512)            2048      ['batch_normalization_55[0][0]\n",
      " tchNormalization)                                                  ']                            \n",
      "                                                                                                  \n",
      " add_6 (Add)                 (None, 8, 8, 512)            0         ['batch_normalization_56[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'up_sampling2d[0][0]']       \n",
      "                                                                                                  \n",
      " up_sampling2d_1 (UpSamplin  (None, 16, 16, 512)          0         ['add_6[0][0]']               \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " re_lu_21 (ReLU)             (None, 16, 16, 512)          0         ['up_sampling2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)          (None, 16, 16, 512)          2359808   ['re_lu_21[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_57 (Ba  (None, 16, 16, 512)          2048      ['conv2d_42[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)          (None, 16, 16, 128)          65664     ['batch_normalization_57[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_58 (Ba  (None, 16, 16, 128)          512       ['conv2d_43[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_59 (Ba  (None, 16, 16, 128)          512       ['batch_normalization_58[0][0]\n",
      " tchNormalization)                                                  ']                            \n",
      "                                                                                                  \n",
      " re_lu_22 (ReLU)             (None, 16, 16, 128)          0         ['batch_normalization_59[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)          (None, 16, 16, 128)          147584    ['re_lu_22[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_60 (Ba  (None, 16, 16, 128)          512       ['conv2d_44[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)          (None, 16, 16, 256)          33024     ['batch_normalization_60[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_61 (Ba  (None, 16, 16, 256)          1024      ['conv2d_45[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_62 (Ba  (None, 16, 16, 256)          1024      ['batch_normalization_61[0][0]\n",
      " tchNormalization)                                                  ']                            \n",
      "                                                                                                  \n",
      " re_lu_23 (ReLU)             (None, 16, 16, 256)          0         ['batch_normalization_62[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)          (None, 16, 16, 256)          590080    ['re_lu_23[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_63 (Ba  (None, 16, 16, 256)          1024      ['conv2d_46[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)          (None, 16, 16, 512)          131584    ['batch_normalization_63[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_64 (Ba  (None, 16, 16, 512)          2048      ['conv2d_47[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_65 (Ba  (None, 16, 16, 512)          2048      ['batch_normalization_64[0][0]\n",
      " tchNormalization)                                                  ']                            \n",
      "                                                                                                  \n",
      " add_7 (Add)                 (None, 16, 16, 512)          0         ['batch_normalization_65[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'up_sampling2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " up_sampling2d_2 (UpSamplin  (None, 32, 32, 512)          0         ['add_7[0][0]']               \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)          (None, 32, 32, 512)          2359808   ['up_sampling2d_2[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_66 (Ba  (None, 32, 32, 512)          2048      ['conv2d_48[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)          (None, 32, 32, 512)          262656    ['batch_normalization_66[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_67 (Ba  (None, 32, 32, 512)          2048      ['conv2d_49[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)          (None, 32, 32, 512)          2359808   ['batch_normalization_67[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_68 (Ba  (None, 32, 32, 512)          2048      ['conv2d_50[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)          (None, 32, 32, 512)          262656    ['batch_normalization_68[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_69 (Ba  (None, 32, 32, 512)          2048      ['conv2d_51[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.math.sigmoid (TFOpLambd  (None, 32, 32, 512)          0         ['batch_normalization_69[0][0]\n",
      " a)                                                                 ']                            \n",
      "                                                                                                  \n",
      " multiply (Multiply)         (None, 32, 32, 512)          0         ['add_1[0][0]',               \n",
      "                                                                     'tf.math.sigmoid[0][0]']     \n",
      "                                                                                                  \n",
      " add_8 (Add)                 (None, 32, 32, 512)          0         ['add_1[0][0]',               \n",
      "                                                                     'multiply[0][0]']            \n",
      "                                                                                                  \n",
      " re_lu_24 (ReLU)             (None, 32, 32, 512)          0         ['add_8[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)          (None, 32, 32, 512)          2359808   ['re_lu_24[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_70 (Ba  (None, 32, 32, 512)          2048      ['conv2d_52[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)          (None, 32, 32, 128)          65664     ['batch_normalization_70[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_71 (Ba  (None, 32, 32, 128)          512       ['conv2d_53[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_72 (Ba  (None, 32, 32, 128)          512       ['batch_normalization_71[0][0]\n",
      " tchNormalization)                                                  ']                            \n",
      "                                                                                                  \n",
      " re_lu_25 (ReLU)             (None, 32, 32, 128)          0         ['batch_normalization_72[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)          (None, 32, 32, 128)          147584    ['re_lu_25[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_73 (Ba  (None, 32, 32, 128)          512       ['conv2d_54[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)          (None, 32, 32, 256)          33024     ['batch_normalization_73[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_74 (Ba  (None, 32, 32, 256)          1024      ['conv2d_55[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_75 (Ba  (None, 32, 32, 256)          1024      ['batch_normalization_74[0][0]\n",
      " tchNormalization)                                                  ']                            \n",
      "                                                                                                  \n",
      " re_lu_26 (ReLU)             (None, 32, 32, 256)          0         ['batch_normalization_75[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)          (None, 32, 32, 256)          590080    ['re_lu_26[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_76 (Ba  (None, 32, 32, 256)          1024      ['conv2d_56[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_57 (Conv2D)          (None, 32, 32, 512)          131584    ['batch_normalization_76[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_77 (Ba  (None, 32, 32, 512)          2048      ['conv2d_57[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_78 (Ba  (None, 32, 32, 512)          2048      ['batch_normalization_77[0][0]\n",
      " tchNormalization)                                                  ']                            \n",
      "                                                                                                  \n",
      " add_9 (Add)                 (None, 32, 32, 512)          0         ['batch_normalization_78[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'add_8[0][0]']               \n",
      "                                                                                                  \n",
      " re_lu_27 (ReLU)             (None, 32, 32, 512)          0         ['add_9[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_59 (Conv2D)          (None, 32, 32, 512)          2359808   ['re_lu_27[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_79 (Ba  (None, 32, 32, 512)          2048      ['conv2d_59[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_60 (Conv2D)          (None, 32, 32, 728)          373464    ['batch_normalization_79[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_80 (Ba  (None, 32, 32, 728)          2912      ['conv2d_60[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_28 (ReLU)             (None, 32, 32, 728)          0         ['batch_normalization_80[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_61 (Conv2D)          (None, 32, 32, 728)          4770584   ['re_lu_28[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_81 (Ba  (None, 32, 32, 728)          2912      ['conv2d_61[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_62 (Conv2D)          (None, 32, 32, 1024)         746496    ['batch_normalization_81[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_82 (Ba  (None, 32, 32, 1024)         4096      ['conv2d_62[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPoolin  (None, 16, 16, 1024)         0         ['batch_normalization_82[0][0]\n",
      " g2D)                                                               ']                            \n",
      "                                                                                                  \n",
      " conv2d_58 (Conv2D)          (None, 16, 16, 1024)         525312    ['add_9[0][0]']               \n",
      "                                                                                                  \n",
      " add_10 (Add)                (None, 16, 16, 1024)         0         ['max_pooling2d_5[0][0]',     \n",
      "                                                                     'conv2d_58[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_63 (Conv2D)          (None, 16, 16, 1024)         9438208   ['add_10[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_83 (Ba  (None, 16, 16, 1024)         4096      ['conv2d_63[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_64 (Conv2D)          (None, 16, 16, 1536)         1574400   ['batch_normalization_83[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_84 (Ba  (None, 16, 16, 1536)         6144      ['conv2d_64[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_29 (ReLU)             (None, 16, 16, 1536)         0         ['batch_normalization_84[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_65 (Conv2D)          (None, 16, 16, 1536)         2123520   ['re_lu_29[0][0]']            \n",
      "                                                          0                                       \n",
      "                                                                                                  \n",
      " batch_normalization_85 (Ba  (None, 16, 16, 1536)         6144      ['conv2d_65[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_66 (Conv2D)          (None, 16, 16, 2048)         3147776   ['batch_normalization_85[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_86 (Ba  (None, 16, 16, 2048)         8192      ['conv2d_66[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_30 (ReLU)             (None, 16, 16, 2048)         0         ['batch_normalization_86[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " global_average_pooling2d (  (None, 2048)                 0         ['re_lu_30[0][0]']            \n",
      " GlobalAveragePooling2D)                                                                          \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 1000)                 2049000   ['global_average_pooling2d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 256)                  256256    ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 1)                    257       ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 78285913 (298.64 MB)\n",
      "Trainable params: 78220217 (298.39 MB)\n",
      "Non-trainable params: 65696 (256.62 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7:2:1 == train:valid:test \n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_data.reshape(1234, 256, 256, 1), y_data, \n",
    "#                                                   random_state=42, test_size=0.2)\n",
    "\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, random_state=42, test_size = 0.66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data.reshape(1234, 256, 256, 1), \n",
    "                                                    y_data, random_state=42, test_size=0.2)\n",
    "\n",
    "X_rot_train, X_rot_test, y_rot_train, y_rot_test = train_test_split(X_aug_rot.reshape(1234, 256, 256, 1), \n",
    "                                                                    y_data, random_state=42, test_size=0.2)\n",
    "\n",
    "X_noise_train, X_noise_test, y_noise_train, y_noise_test = train_test_split(X_aug_noise.reshape(1234, 256, 256, 1), \n",
    "                                                                            y_data, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_aug_data = np.concatenate((X_train, X_rot_train, X_noise_train), axis = 0)\n",
    "y_aug_data = np.concatenate((y_train, y_rot_train, y_noise_train), axis = 0)\n",
    "\n",
    "X_aug_train, X_aug_val, y_aug_train, y_aug_val = train_test_split(X_aug_data, y_aug_data, random_state=42, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filename = 'checkpoint-50-epochs-8-batchs.h5'\n",
    "checkpoint = ModelCheckpoint(filename, mointor='val_loss', verbose=1, \n",
    "                             save_best_only = True, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('./checkpoint-50-epochs-8-batchs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-05 00:43:13.774949: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8903\n",
      "2024-02-05 00:43:22.464439: I external/local_xla/xla/service/service.cc:168] XLA service 0x3d037e60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-05 00:43:22.464474: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA TITAN RTX, Compute Capability 7.5\n",
      "2024-02-05 00:43:22.470091: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1707061402.584508 1681211 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f299957e4d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f299957e4d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "296/296 [==============================] - ETA: 0s - loss: 2.9045 - mae: 2.9045 - mse: 14.0279\n",
      "Epoch 1: val_loss improved from inf to 19.53097, saving model to checkpoint-50-epochs-8-batchs.h5\n",
      "296/296 [==============================] - 185s 502ms/step - loss: 2.9045 - mae: 2.9045 - mse: 14.0279 - val_loss: 19.5310 - val_mae: 19.5310 - val_mse: 453.4926\n",
      "Epoch 2/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 2.0082 - mae: 2.0082 - mse: 6.8161\n",
      "Epoch 2: val_loss improved from 19.53097 to 2.16710, saving model to checkpoint-50-epochs-8-batchs.h5\n",
      "296/296 [==============================] - 143s 482ms/step - loss: 2.0082 - mae: 2.0082 - mse: 6.8161 - val_loss: 2.1671 - val_mae: 2.1671 - val_mse: 9.3769\n",
      "Epoch 3/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 1.7902 - mae: 1.7902 - mse: 5.3984\n",
      "Epoch 3: val_loss did not improve from 2.16710\n",
      "296/296 [==============================] - 141s 476ms/step - loss: 1.7902 - mae: 1.7902 - mse: 5.3984 - val_loss: 8.9098 - val_mae: 8.9098 - val_mse: 102.4940\n",
      "Epoch 4/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 1.6569 - mae: 1.6569 - mse: 4.5838\n",
      "Epoch 4: val_loss improved from 2.16710 to 1.44046, saving model to checkpoint-50-epochs-8-batchs.h5\n",
      "296/296 [==============================] - 144s 488ms/step - loss: 1.6569 - mae: 1.6569 - mse: 4.5838 - val_loss: 1.4405 - val_mae: 1.4405 - val_mse: 3.6211\n",
      "Epoch 5/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 1.4992 - mae: 1.4992 - mse: 3.8344\n",
      "Epoch 5: val_loss did not improve from 1.44046\n",
      "296/296 [==============================] - 140s 474ms/step - loss: 1.4992 - mae: 1.4992 - mse: 3.8344 - val_loss: 2.6905 - val_mae: 2.6905 - val_mse: 10.8150\n",
      "Epoch 6/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 1.4347 - mae: 1.4347 - mse: 3.5034\n",
      "Epoch 6: val_loss did not improve from 1.44046\n",
      "296/296 [==============================] - 139s 471ms/step - loss: 1.4347 - mae: 1.4347 - mse: 3.5034 - val_loss: 2.3099 - val_mae: 2.3099 - val_mse: 7.8748\n",
      "Epoch 7/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 1.3490 - mae: 1.3490 - mse: 3.1020\n",
      "Epoch 7: val_loss did not improve from 1.44046\n",
      "296/296 [==============================] - 139s 471ms/step - loss: 1.3490 - mae: 1.3490 - mse: 3.1020 - val_loss: 3.3297 - val_mae: 3.3297 - val_mse: 14.8331\n",
      "Epoch 8/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 1.3105 - mae: 1.3105 - mse: 2.7886\n",
      "Epoch 8: val_loss improved from 1.44046 to 1.35329, saving model to checkpoint-50-epochs-8-batchs.h5\n",
      "296/296 [==============================] - 144s 485ms/step - loss: 1.3105 - mae: 1.3105 - mse: 2.7886 - val_loss: 1.3533 - val_mae: 1.3533 - val_mse: 3.1457\n",
      "Epoch 9/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 1.2283 - mae: 1.2283 - mse: 2.5234\n",
      "Epoch 9: val_loss did not improve from 1.35329\n",
      "296/296 [==============================] - 139s 469ms/step - loss: 1.2283 - mae: 1.2283 - mse: 2.5234 - val_loss: 2.3769 - val_mae: 2.3769 - val_mse: 8.0940\n",
      "Epoch 10/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 1.2041 - mae: 1.2041 - mse: 2.4907\n",
      "Epoch 10: val_loss did not improve from 1.35329\n",
      "296/296 [==============================] - 138s 467ms/step - loss: 1.2041 - mae: 1.2041 - mse: 2.4907 - val_loss: 1.4571 - val_mae: 1.4571 - val_mse: 3.4408\n",
      "Epoch 11/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 1.1696 - mae: 1.1696 - mse: 2.3112\n",
      "Epoch 11: val_loss did not improve from 1.35329\n",
      "296/296 [==============================] - 142s 481ms/step - loss: 1.1696 - mae: 1.1696 - mse: 2.3112 - val_loss: 1.7901 - val_mae: 1.7901 - val_mse: 5.0686\n",
      "Epoch 12/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 1.1551 - mae: 1.1551 - mse: 2.2643\n",
      "Epoch 12: val_loss did not improve from 1.35329\n",
      "296/296 [==============================] - 141s 478ms/step - loss: 1.1551 - mae: 1.1551 - mse: 2.2643 - val_loss: 1.6564 - val_mae: 1.6564 - val_mse: 4.1377\n",
      "Epoch 13/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 1.1185 - mae: 1.1185 - mse: 2.0654\n",
      "Epoch 13: val_loss did not improve from 1.35329\n",
      "296/296 [==============================] - 141s 475ms/step - loss: 1.1185 - mae: 1.1185 - mse: 2.0654 - val_loss: 1.7689 - val_mae: 1.7689 - val_mse: 4.9812\n",
      "Epoch 14/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 1.0895 - mae: 1.0895 - mse: 2.0544\n",
      "Epoch 14: val_loss did not improve from 1.35329\n",
      "296/296 [==============================] - 139s 471ms/step - loss: 1.0895 - mae: 1.0895 - mse: 2.0544 - val_loss: 2.8856 - val_mae: 2.8856 - val_mse: 12.4227\n",
      "Epoch 15/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 1.0036 - mae: 1.0036 - mse: 1.6840\n",
      "Epoch 15: val_loss did not improve from 1.35329\n",
      "296/296 [==============================] - 139s 471ms/step - loss: 1.0036 - mae: 1.0036 - mse: 1.6840 - val_loss: 3.3357 - val_mae: 3.3357 - val_mse: 13.9329\n",
      "Epoch 16/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 1.0466 - mae: 1.0466 - mse: 1.8349\n",
      "Epoch 16: val_loss improved from 1.35329 to 1.04827, saving model to checkpoint-50-epochs-8-batchs.h5\n",
      "296/296 [==============================] - 143s 483ms/step - loss: 1.0466 - mae: 1.0466 - mse: 1.8349 - val_loss: 1.0483 - val_mae: 1.0483 - val_mse: 1.9771\n",
      "Epoch 17/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.9775 - mae: 0.9775 - mse: 1.5817\n",
      "Epoch 17: val_loss did not improve from 1.04827\n",
      "296/296 [==============================] - 140s 472ms/step - loss: 0.9775 - mae: 0.9775 - mse: 1.5817 - val_loss: 1.1317 - val_mae: 1.1317 - val_mse: 2.1561\n",
      "Epoch 18/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.9636 - mae: 0.9636 - mse: 1.5669\n",
      "Epoch 18: val_loss did not improve from 1.04827\n",
      "296/296 [==============================] - 140s 473ms/step - loss: 0.9636 - mae: 0.9636 - mse: 1.5669 - val_loss: 1.1841 - val_mae: 1.1841 - val_mse: 2.3530\n",
      "Epoch 19/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.9621 - mae: 0.9621 - mse: 1.5888\n",
      "Epoch 19: val_loss improved from 1.04827 to 1.01416, saving model to checkpoint-50-epochs-8-batchs.h5\n",
      "296/296 [==============================] - 143s 484ms/step - loss: 0.9621 - mae: 0.9621 - mse: 1.5888 - val_loss: 1.0142 - val_mae: 1.0142 - val_mse: 1.7942\n",
      "Epoch 20/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.9039 - mae: 0.9039 - mse: 1.3939\n",
      "Epoch 20: val_loss did not improve from 1.01416\n",
      "296/296 [==============================] - 141s 476ms/step - loss: 0.9039 - mae: 0.9039 - mse: 1.3939 - val_loss: 1.6878 - val_mae: 1.6878 - val_mse: 4.1857\n",
      "Epoch 21/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.8640 - mae: 0.8640 - mse: 1.2676\n",
      "Epoch 21: val_loss did not improve from 1.01416\n",
      "296/296 [==============================] - 140s 473ms/step - loss: 0.8640 - mae: 0.8640 - mse: 1.2676 - val_loss: 1.2313 - val_mae: 1.2313 - val_mse: 2.5936\n",
      "Epoch 22/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.8862 - mae: 0.8862 - mse: 1.3229\n",
      "Epoch 22: val_loss did not improve from 1.01416\n",
      "296/296 [==============================] - 140s 474ms/step - loss: 0.8862 - mae: 0.8862 - mse: 1.3229 - val_loss: 1.3468 - val_mae: 1.3468 - val_mse: 2.8018\n",
      "Epoch 23/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.8604 - mae: 0.8604 - mse: 1.2353\n",
      "Epoch 23: val_loss did not improve from 1.01416\n",
      "296/296 [==============================] - 140s 472ms/step - loss: 0.8604 - mae: 0.8604 - mse: 1.2353 - val_loss: 1.4697 - val_mae: 1.4697 - val_mse: 3.5277\n",
      "Epoch 24/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.8563 - mae: 0.8563 - mse: 1.2712\n",
      "Epoch 24: val_loss did not improve from 1.01416\n",
      "296/296 [==============================] - 140s 472ms/step - loss: 0.8563 - mae: 0.8563 - mse: 1.2712 - val_loss: 2.0482 - val_mae: 2.0482 - val_mse: 5.8259\n",
      "Epoch 25/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.8530 - mae: 0.8530 - mse: 1.2808\n",
      "Epoch 25: val_loss did not improve from 1.01416\n",
      "296/296 [==============================] - 139s 471ms/step - loss: 0.8530 - mae: 0.8530 - mse: 1.2808 - val_loss: 1.0639 - val_mae: 1.0639 - val_mse: 1.9564\n",
      "Epoch 26/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.7788 - mae: 0.7788 - mse: 1.0552\n",
      "Epoch 26: val_loss did not improve from 1.01416\n",
      "296/296 [==============================] - 139s 470ms/step - loss: 0.7788 - mae: 0.7788 - mse: 1.0552 - val_loss: 1.7435 - val_mae: 1.7435 - val_mse: 4.7013\n",
      "Epoch 27/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.8113 - mae: 0.8113 - mse: 1.1225\n",
      "Epoch 27: val_loss did not improve from 1.01416\n",
      "296/296 [==============================] - 139s 471ms/step - loss: 0.8113 - mae: 0.8113 - mse: 1.1225 - val_loss: 1.0230 - val_mae: 1.0230 - val_mse: 1.9061\n",
      "Epoch 28/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.7419 - mae: 0.7419 - mse: 0.9376\n",
      "Epoch 28: val_loss did not improve from 1.01416\n",
      "296/296 [==============================] - 138s 466ms/step - loss: 0.7419 - mae: 0.7419 - mse: 0.9376 - val_loss: 1.3970 - val_mae: 1.3970 - val_mse: 3.2069\n",
      "Epoch 29/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.7773 - mae: 0.7773 - mse: 1.0435\n",
      "Epoch 29: val_loss improved from 1.01416 to 0.92710, saving model to checkpoint-50-epochs-8-batchs.h5\n",
      "296/296 [==============================] - 141s 478ms/step - loss: 0.7773 - mae: 0.7773 - mse: 1.0435 - val_loss: 0.9271 - val_mae: 0.9271 - val_mse: 1.6339\n",
      "Epoch 30/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.7395 - mae: 0.7395 - mse: 0.9612\n",
      "Epoch 30: val_loss did not improve from 0.92710\n",
      "296/296 [==============================] - 142s 479ms/step - loss: 0.7395 - mae: 0.7395 - mse: 0.9612 - val_loss: 2.0131 - val_mae: 2.0131 - val_mse: 5.4873\n",
      "Epoch 31/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.7082 - mae: 0.7082 - mse: 0.8620\n",
      "Epoch 31: val_loss improved from 0.92710 to 0.85190, saving model to checkpoint-50-epochs-8-batchs.h5\n",
      "296/296 [==============================] - 144s 488ms/step - loss: 0.7082 - mae: 0.7082 - mse: 0.8620 - val_loss: 0.8519 - val_mae: 0.8519 - val_mse: 1.4601\n",
      "Epoch 32/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.7287 - mae: 0.7287 - mse: 0.9103\n",
      "Epoch 32: val_loss did not improve from 0.85190\n",
      "296/296 [==============================] - 141s 477ms/step - loss: 0.7287 - mae: 0.7287 - mse: 0.9103 - val_loss: 1.0101 - val_mae: 1.0101 - val_mse: 1.8313\n",
      "Epoch 33/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.7515 - mae: 0.7515 - mse: 0.9552\n",
      "Epoch 33: val_loss did not improve from 0.85190\n",
      "296/296 [==============================] - 140s 474ms/step - loss: 0.7515 - mae: 0.7515 - mse: 0.9552 - val_loss: 1.3668 - val_mae: 1.3668 - val_mse: 2.7599\n",
      "Epoch 34/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.6683 - mae: 0.6683 - mse: 0.7723\n",
      "Epoch 34: val_loss did not improve from 0.85190\n",
      "296/296 [==============================] - 142s 479ms/step - loss: 0.6683 - mae: 0.6683 - mse: 0.7723 - val_loss: 1.2204 - val_mae: 1.2204 - val_mse: 2.3966\n",
      "Epoch 35/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.6665 - mae: 0.6665 - mse: 0.7683\n",
      "Epoch 35: val_loss did not improve from 0.85190\n",
      "296/296 [==============================] - 141s 477ms/step - loss: 0.6665 - mae: 0.6665 - mse: 0.7683 - val_loss: 2.7059 - val_mae: 2.7059 - val_mse: 9.0322\n",
      "Epoch 36/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.6645 - mae: 0.6645 - mse: 0.7447\n",
      "Epoch 36: val_loss did not improve from 0.85190\n",
      "296/296 [==============================] - 142s 479ms/step - loss: 0.6645 - mae: 0.6645 - mse: 0.7447 - val_loss: 1.0053 - val_mae: 1.0053 - val_mse: 1.7009\n",
      "Epoch 37/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.6857 - mae: 0.6857 - mse: 0.8515\n",
      "Epoch 37: val_loss did not improve from 0.85190\n",
      "296/296 [==============================] - 140s 473ms/step - loss: 0.6857 - mae: 0.6857 - mse: 0.8515 - val_loss: 2.3020 - val_mae: 2.3020 - val_mse: 7.1186\n",
      "Epoch 38/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.6341 - mae: 0.6341 - mse: 0.7079\n",
      "Epoch 38: val_loss did not improve from 0.85190\n",
      "296/296 [==============================] - 140s 473ms/step - loss: 0.6341 - mae: 0.6341 - mse: 0.7079 - val_loss: 0.8543 - val_mae: 0.8543 - val_mse: 1.3486\n",
      "Epoch 39/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.6495 - mae: 0.6495 - mse: 0.7262\n",
      "Epoch 39: val_loss did not improve from 0.85190\n",
      "296/296 [==============================] - 140s 474ms/step - loss: 0.6495 - mae: 0.6495 - mse: 0.7262 - val_loss: 1.0919 - val_mae: 1.0919 - val_mse: 1.9914\n",
      "Epoch 40/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.5895 - mae: 0.5895 - mse: 0.6094\n",
      "Epoch 40: val_loss did not improve from 0.85190\n",
      "296/296 [==============================] - 140s 473ms/step - loss: 0.5895 - mae: 0.5895 - mse: 0.6094 - val_loss: 1.8587 - val_mae: 1.8587 - val_mse: 4.7614\n",
      "Epoch 41/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.6120 - mae: 0.6120 - mse: 0.6510\n",
      "Epoch 41: val_loss did not improve from 0.85190\n",
      "296/296 [==============================] - 139s 470ms/step - loss: 0.6120 - mae: 0.6120 - mse: 0.6510 - val_loss: 1.3827 - val_mae: 1.3827 - val_mse: 2.9195\n",
      "Epoch 42/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.6178 - mae: 0.6178 - mse: 0.6629\n",
      "Epoch 42: val_loss did not improve from 0.85190\n",
      "296/296 [==============================] - 138s 465ms/step - loss: 0.6178 - mae: 0.6178 - mse: 0.6629 - val_loss: 1.1137 - val_mae: 1.1137 - val_mse: 2.0450\n",
      "Epoch 43/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.5896 - mae: 0.5896 - mse: 0.5961\n",
      "Epoch 43: val_loss did not improve from 0.85190\n",
      "296/296 [==============================] - 138s 467ms/step - loss: 0.5896 - mae: 0.5896 - mse: 0.5961 - val_loss: 2.0570 - val_mae: 2.0570 - val_mse: 5.9821\n",
      "Epoch 44/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.6002 - mae: 0.6002 - mse: 0.6463\n",
      "Epoch 44: val_loss improved from 0.85190 to 0.79251, saving model to checkpoint-50-epochs-8-batchs.h5\n",
      "296/296 [==============================] - 141s 476ms/step - loss: 0.6002 - mae: 0.6002 - mse: 0.6463 - val_loss: 0.7925 - val_mae: 0.7925 - val_mse: 1.1958\n",
      "Epoch 45/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.5838 - mae: 0.5838 - mse: 0.5756\n",
      "Epoch 45: val_loss did not improve from 0.79251\n",
      "296/296 [==============================] - 140s 474ms/step - loss: 0.5838 - mae: 0.5838 - mse: 0.5756 - val_loss: 0.8176 - val_mae: 0.8176 - val_mse: 1.3866\n",
      "Epoch 46/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.5799 - mae: 0.5799 - mse: 0.5623\n",
      "Epoch 46: val_loss did not improve from 0.79251\n",
      "296/296 [==============================] - 141s 477ms/step - loss: 0.5799 - mae: 0.5799 - mse: 0.5623 - val_loss: 0.8998 - val_mae: 0.8998 - val_mse: 1.4501\n",
      "Epoch 47/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.5506 - mae: 0.5506 - mse: 0.5188\n",
      "Epoch 47: val_loss did not improve from 0.79251\n",
      "296/296 [==============================] - 140s 472ms/step - loss: 0.5506 - mae: 0.5506 - mse: 0.5188 - val_loss: 1.7214 - val_mae: 1.7214 - val_mse: 4.0220\n",
      "Epoch 48/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.5468 - mae: 0.5468 - mse: 0.5280\n",
      "Epoch 48: val_loss did not improve from 0.79251\n",
      "296/296 [==============================] - 140s 472ms/step - loss: 0.5468 - mae: 0.5468 - mse: 0.5280 - val_loss: 0.9337 - val_mae: 0.9337 - val_mse: 1.4856\n",
      "Epoch 49/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.5592 - mae: 0.5592 - mse: 0.5397\n",
      "Epoch 49: val_loss did not improve from 0.79251\n",
      "296/296 [==============================] - 140s 473ms/step - loss: 0.5592 - mae: 0.5592 - mse: 0.5397 - val_loss: 1.2763 - val_mae: 1.2763 - val_mse: 2.5027\n",
      "Epoch 50/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.5778 - mae: 0.5778 - mse: 0.5698\n",
      "Epoch 50: val_loss did not improve from 0.79251\n",
      "296/296 [==============================] - 139s 471ms/step - loss: 0.5778 - mae: 0.5778 - mse: 0.5698 - val_loss: 0.8361 - val_mae: 0.8361 - val_mse: 1.3500\n",
      "Epoch 51/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.5037 - mae: 0.5037 - mse: 0.4361\n",
      "Epoch 51: val_loss did not improve from 0.79251\n",
      "296/296 [==============================] - 140s 473ms/step - loss: 0.5037 - mae: 0.5037 - mse: 0.4361 - val_loss: 0.8053 - val_mae: 0.8053 - val_mse: 1.2830\n",
      "Epoch 52/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.5258 - mae: 0.5258 - mse: 0.4781\n",
      "Epoch 52: val_loss did not improve from 0.79251\n",
      "296/296 [==============================] - 141s 477ms/step - loss: 0.5258 - mae: 0.5258 - mse: 0.4781 - val_loss: 0.9546 - val_mae: 0.9546 - val_mse: 1.5177\n",
      "Epoch 53/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.5023 - mae: 0.5023 - mse: 0.4320\n",
      "Epoch 53: val_loss did not improve from 0.79251\n",
      "296/296 [==============================] - 141s 478ms/step - loss: 0.5023 - mae: 0.5023 - mse: 0.4320 - val_loss: 0.8265 - val_mae: 0.8265 - val_mse: 1.2991\n",
      "Epoch 54/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.5257 - mae: 0.5257 - mse: 0.4768\n",
      "Epoch 54: val_loss did not improve from 0.79251\n",
      "296/296 [==============================] - 138s 467ms/step - loss: 0.5257 - mae: 0.5257 - mse: 0.4768 - val_loss: 1.6054 - val_mae: 1.6054 - val_mse: 3.5004\n",
      "Epoch 55/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.5216 - mae: 0.5216 - mse: 0.4713\n",
      "Epoch 55: val_loss improved from 0.79251 to 0.77876, saving model to checkpoint-50-epochs-8-batchs.h5\n",
      "296/296 [==============================] - 142s 480ms/step - loss: 0.5216 - mae: 0.5216 - mse: 0.4713 - val_loss: 0.7788 - val_mae: 0.7788 - val_mse: 1.1511\n",
      "Epoch 56/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.5086 - mae: 0.5086 - mse: 0.4498\n",
      "Epoch 56: val_loss did not improve from 0.77876\n",
      "296/296 [==============================] - 141s 475ms/step - loss: 0.5086 - mae: 0.5086 - mse: 0.4498 - val_loss: 1.0496 - val_mae: 1.0496 - val_mse: 1.8591\n",
      "Epoch 57/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.5295 - mae: 0.5295 - mse: 0.4610\n",
      "Epoch 57: val_loss improved from 0.77876 to 0.73994, saving model to checkpoint-50-epochs-8-batchs.h5\n",
      "296/296 [==============================] - 143s 484ms/step - loss: 0.5295 - mae: 0.5295 - mse: 0.4610 - val_loss: 0.7399 - val_mae: 0.7399 - val_mse: 1.1147\n",
      "Epoch 58/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.5105 - mae: 0.5105 - mse: 0.4304\n",
      "Epoch 58: val_loss did not improve from 0.73994\n",
      "296/296 [==============================] - 139s 469ms/step - loss: 0.5105 - mae: 0.5105 - mse: 0.4304 - val_loss: 0.8023 - val_mae: 0.8023 - val_mse: 1.1717\n",
      "Epoch 59/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.4917 - mae: 0.4917 - mse: 0.4027\n",
      "Epoch 59: val_loss did not improve from 0.73994\n",
      "296/296 [==============================] - 139s 470ms/step - loss: 0.4917 - mae: 0.4917 - mse: 0.4027 - val_loss: 1.2943 - val_mae: 1.2943 - val_mse: 2.6396\n",
      "Epoch 60/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.5072 - mae: 0.5072 - mse: 0.4417\n",
      "Epoch 60: val_loss did not improve from 0.73994\n",
      "296/296 [==============================] - 139s 468ms/step - loss: 0.5072 - mae: 0.5072 - mse: 0.4417 - val_loss: 1.2712 - val_mae: 1.2712 - val_mse: 2.3544\n",
      "Epoch 61/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.4725 - mae: 0.4725 - mse: 0.3814\n",
      "Epoch 61: val_loss did not improve from 0.73994\n",
      "296/296 [==============================] - 138s 468ms/step - loss: 0.4725 - mae: 0.4725 - mse: 0.3814 - val_loss: 1.2075 - val_mae: 1.2075 - val_mse: 2.2334\n",
      "Epoch 62/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.4666 - mae: 0.4666 - mse: 0.3901\n",
      "Epoch 62: val_loss did not improve from 0.73994\n",
      "296/296 [==============================] - 140s 472ms/step - loss: 0.4666 - mae: 0.4666 - mse: 0.3901 - val_loss: 0.9079 - val_mae: 0.9079 - val_mse: 1.3825\n",
      "Epoch 63/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.4583 - mae: 0.4583 - mse: 0.3476\n",
      "Epoch 63: val_loss did not improve from 0.73994\n",
      "296/296 [==============================] - 138s 467ms/step - loss: 0.4583 - mae: 0.4583 - mse: 0.3476 - val_loss: 0.7506 - val_mae: 0.7506 - val_mse: 1.2012\n",
      "Epoch 64/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.4523 - mae: 0.4523 - mse: 0.3464\n",
      "Epoch 64: val_loss did not improve from 0.73994\n",
      "296/296 [==============================] - 138s 467ms/step - loss: 0.4523 - mae: 0.4523 - mse: 0.3464 - val_loss: 0.9297 - val_mae: 0.9297 - val_mse: 1.6455\n",
      "Epoch 65/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.4796 - mae: 0.4796 - mse: 0.3932\n",
      "Epoch 65: val_loss did not improve from 0.73994\n",
      "296/296 [==============================] - 139s 469ms/step - loss: 0.4796 - mae: 0.4796 - mse: 0.3932 - val_loss: 0.8739 - val_mae: 0.8739 - val_mse: 1.4306\n",
      "Epoch 66/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.4534 - mae: 0.4534 - mse: 0.3426\n",
      "Epoch 66: val_loss did not improve from 0.73994\n",
      "296/296 [==============================] - 139s 469ms/step - loss: 0.4534 - mae: 0.4534 - mse: 0.3426 - val_loss: 0.8188 - val_mae: 0.8188 - val_mse: 1.2395\n",
      "Epoch 67/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.4510 - mae: 0.4510 - mse: 0.3421\n",
      "Epoch 67: val_loss did not improve from 0.73994\n",
      "296/296 [==============================] - 139s 469ms/step - loss: 0.4510 - mae: 0.4510 - mse: 0.3421 - val_loss: 0.8331 - val_mae: 0.8331 - val_mse: 1.2701\n",
      "Epoch 68/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.4967 - mae: 0.4967 - mse: 0.4020\n",
      "Epoch 68: val_loss improved from 0.73994 to 0.73591, saving model to checkpoint-50-epochs-8-batchs.h5\n",
      "296/296 [==============================] - 143s 482ms/step - loss: 0.4967 - mae: 0.4967 - mse: 0.4020 - val_loss: 0.7359 - val_mae: 0.7359 - val_mse: 1.0780\n",
      "Epoch 69/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.4110 - mae: 0.4110 - mse: 0.2882\n",
      "Epoch 69: val_loss did not improve from 0.73591\n",
      "296/296 [==============================] - 140s 472ms/step - loss: 0.4110 - mae: 0.4110 - mse: 0.2882 - val_loss: 1.2836 - val_mae: 1.2836 - val_mse: 2.4051\n",
      "Epoch 70/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.4390 - mae: 0.4390 - mse: 0.3392\n",
      "Epoch 70: val_loss improved from 0.73591 to 0.71055, saving model to checkpoint-50-epochs-8-batchs.h5\n",
      "296/296 [==============================] - 142s 480ms/step - loss: 0.4390 - mae: 0.4390 - mse: 0.3392 - val_loss: 0.7105 - val_mae: 0.7105 - val_mse: 0.9975\n",
      "Epoch 71/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.4064 - mae: 0.4064 - mse: 0.2718\n",
      "Epoch 71: val_loss did not improve from 0.71055\n",
      "296/296 [==============================] - 140s 474ms/step - loss: 0.4064 - mae: 0.4064 - mse: 0.2718 - val_loss: 0.7496 - val_mae: 0.7496 - val_mse: 1.1097\n",
      "Epoch 72/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.4686 - mae: 0.4686 - mse: 0.3958\n",
      "Epoch 72: val_loss did not improve from 0.71055\n",
      "296/296 [==============================] - 140s 474ms/step - loss: 0.4686 - mae: 0.4686 - mse: 0.3958 - val_loss: 1.0114 - val_mae: 1.0114 - val_mse: 1.6409\n",
      "Epoch 73/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.4191 - mae: 0.4191 - mse: 0.3105\n",
      "Epoch 73: val_loss improved from 0.71055 to 0.66591, saving model to checkpoint-50-epochs-8-batchs.h5\n",
      "296/296 [==============================] - 140s 473ms/step - loss: 0.4191 - mae: 0.4191 - mse: 0.3105 - val_loss: 0.6659 - val_mae: 0.6659 - val_mse: 0.9424\n",
      "Epoch 74/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.4543 - mae: 0.4543 - mse: 0.3619\n",
      "Epoch 74: val_loss did not improve from 0.66591\n",
      "296/296 [==============================] - 141s 476ms/step - loss: 0.4543 - mae: 0.4543 - mse: 0.3619 - val_loss: 0.6872 - val_mae: 0.6872 - val_mse: 1.0947\n",
      "Epoch 75/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.4372 - mae: 0.4372 - mse: 0.3405\n",
      "Epoch 75: val_loss did not improve from 0.66591\n",
      "296/296 [==============================] - 142s 479ms/step - loss: 0.4372 - mae: 0.4372 - mse: 0.3405 - val_loss: 1.0217 - val_mae: 1.0217 - val_mse: 1.7082\n",
      "Epoch 76/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.4146 - mae: 0.4146 - mse: 0.2950\n",
      "Epoch 76: val_loss did not improve from 0.66591\n",
      "296/296 [==============================] - 142s 478ms/step - loss: 0.4146 - mae: 0.4146 - mse: 0.2950 - val_loss: 0.7102 - val_mae: 0.7102 - val_mse: 1.0183\n",
      "Epoch 77/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.4072 - mae: 0.4072 - mse: 0.2783\n",
      "Epoch 77: val_loss did not improve from 0.66591\n",
      "296/296 [==============================] - 142s 481ms/step - loss: 0.4072 - mae: 0.4072 - mse: 0.2783 - val_loss: 0.7236 - val_mae: 0.7236 - val_mse: 1.0900\n",
      "Epoch 78/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.4084 - mae: 0.4084 - mse: 0.2850\n",
      "Epoch 78: val_loss did not improve from 0.66591\n",
      "296/296 [==============================] - 141s 475ms/step - loss: 0.4084 - mae: 0.4084 - mse: 0.2850 - val_loss: 1.8966 - val_mae: 1.8966 - val_mse: 4.6568\n",
      "Epoch 79/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.4233 - mae: 0.4233 - mse: 0.3085\n",
      "Epoch 79: val_loss did not improve from 0.66591\n",
      "296/296 [==============================] - 141s 476ms/step - loss: 0.4233 - mae: 0.4233 - mse: 0.3085 - val_loss: 0.7156 - val_mae: 0.7156 - val_mse: 1.0774\n",
      "Epoch 80/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.4235 - mae: 0.4235 - mse: 0.3098\n",
      "Epoch 80: val_loss did not improve from 0.66591\n",
      "296/296 [==============================] - 139s 471ms/step - loss: 0.4235 - mae: 0.4235 - mse: 0.3098 - val_loss: 0.8404 - val_mae: 0.8404 - val_mse: 1.3681\n",
      "Epoch 81/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3803 - mae: 0.3803 - mse: 0.2457\n",
      "Epoch 81: val_loss did not improve from 0.66591\n",
      "296/296 [==============================] - 140s 472ms/step - loss: 0.3803 - mae: 0.3803 - mse: 0.2457 - val_loss: 0.7380 - val_mae: 0.7380 - val_mse: 1.0811\n",
      "Epoch 82/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.4075 - mae: 0.4075 - mse: 0.3134\n",
      "Epoch 82: val_loss did not improve from 0.66591\n",
      "296/296 [==============================] - 141s 477ms/step - loss: 0.4075 - mae: 0.4075 - mse: 0.3134 - val_loss: 0.7206 - val_mae: 0.7206 - val_mse: 1.1281\n",
      "Epoch 83/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3763 - mae: 0.3763 - mse: 0.2412\n",
      "Epoch 83: val_loss did not improve from 0.66591\n",
      "296/296 [==============================] - 142s 480ms/step - loss: 0.3763 - mae: 0.3763 - mse: 0.2412 - val_loss: 0.7558 - val_mae: 0.7558 - val_mse: 1.1471\n",
      "Epoch 84/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3785 - mae: 0.3785 - mse: 0.2375\n",
      "Epoch 84: val_loss did not improve from 0.66591\n",
      "296/296 [==============================] - 142s 480ms/step - loss: 0.3785 - mae: 0.3785 - mse: 0.2375 - val_loss: 0.6952 - val_mae: 0.6952 - val_mse: 0.9670\n",
      "Epoch 85/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3748 - mae: 0.3748 - mse: 0.2428\n",
      "Epoch 85: val_loss did not improve from 0.66591\n",
      "296/296 [==============================] - 139s 470ms/step - loss: 0.3748 - mae: 0.3748 - mse: 0.2428 - val_loss: 0.8403 - val_mae: 0.8403 - val_mse: 1.3272\n",
      "Epoch 86/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3824 - mae: 0.3824 - mse: 0.2519\n",
      "Epoch 86: val_loss improved from 0.66591 to 0.64004, saving model to checkpoint-50-epochs-8-batchs.h5\n",
      "296/296 [==============================] - 144s 488ms/step - loss: 0.3824 - mae: 0.3824 - mse: 0.2519 - val_loss: 0.6400 - val_mae: 0.6400 - val_mse: 0.9479\n",
      "Epoch 87/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3639 - mae: 0.3639 - mse: 0.2286\n",
      "Epoch 87: val_loss did not improve from 0.64004\n",
      "296/296 [==============================] - 141s 476ms/step - loss: 0.3639 - mae: 0.3639 - mse: 0.2286 - val_loss: 0.7454 - val_mae: 0.7454 - val_mse: 1.0804\n",
      "Epoch 88/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3785 - mae: 0.3785 - mse: 0.2407\n",
      "Epoch 88: val_loss did not improve from 0.64004\n",
      "296/296 [==============================] - 141s 478ms/step - loss: 0.3785 - mae: 0.3785 - mse: 0.2407 - val_loss: 0.6580 - val_mae: 0.6580 - val_mse: 0.9339\n",
      "Epoch 89/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3765 - mae: 0.3765 - mse: 0.2777\n",
      "Epoch 89: val_loss did not improve from 0.64004\n",
      "296/296 [==============================] - 141s 478ms/step - loss: 0.3765 - mae: 0.3765 - mse: 0.2777 - val_loss: 0.7419 - val_mae: 0.7419 - val_mse: 1.0548\n",
      "Epoch 90/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3802 - mae: 0.3802 - mse: 0.2486\n",
      "Epoch 90: val_loss did not improve from 0.64004\n",
      "296/296 [==============================] - 140s 471ms/step - loss: 0.3802 - mae: 0.3802 - mse: 0.2486 - val_loss: 0.6995 - val_mae: 0.6995 - val_mse: 1.0342\n",
      "Epoch 91/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3598 - mae: 0.3598 - mse: 0.2223\n",
      "Epoch 91: val_loss improved from 0.64004 to 0.62848, saving model to checkpoint-50-epochs-8-batchs.h5\n",
      "296/296 [==============================] - 141s 476ms/step - loss: 0.3598 - mae: 0.3598 - mse: 0.2223 - val_loss: 0.6285 - val_mae: 0.6285 - val_mse: 0.8826\n",
      "Epoch 92/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3842 - mae: 0.3842 - mse: 0.2701\n",
      "Epoch 92: val_loss did not improve from 0.62848\n",
      "296/296 [==============================] - 142s 478ms/step - loss: 0.3842 - mae: 0.3842 - mse: 0.2701 - val_loss: 0.6756 - val_mae: 0.6756 - val_mse: 0.9864\n",
      "Epoch 93/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3637 - mae: 0.3637 - mse: 0.2194\n",
      "Epoch 93: val_loss did not improve from 0.62848\n",
      "296/296 [==============================] - 141s 477ms/step - loss: 0.3637 - mae: 0.3637 - mse: 0.2194 - val_loss: 0.6982 - val_mae: 0.6982 - val_mse: 1.0252\n",
      "Epoch 94/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3662 - mae: 0.3662 - mse: 0.2222\n",
      "Epoch 94: val_loss did not improve from 0.62848\n",
      "296/296 [==============================] - 141s 475ms/step - loss: 0.3662 - mae: 0.3662 - mse: 0.2222 - val_loss: 1.1795 - val_mae: 1.1795 - val_mse: 2.2126\n",
      "Epoch 95/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3675 - mae: 0.3675 - mse: 0.2217\n",
      "Epoch 95: val_loss did not improve from 0.62848\n",
      "296/296 [==============================] - 142s 479ms/step - loss: 0.3675 - mae: 0.3675 - mse: 0.2217 - val_loss: 1.0158 - val_mae: 1.0158 - val_mse: 1.7324\n",
      "Epoch 96/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3338 - mae: 0.3338 - mse: 0.1995\n",
      "Epoch 96: val_loss improved from 0.62848 to 0.62711, saving model to checkpoint-50-epochs-8-batchs.h5\n",
      "296/296 [==============================] - 144s 488ms/step - loss: 0.3338 - mae: 0.3338 - mse: 0.1995 - val_loss: 0.6271 - val_mae: 0.6271 - val_mse: 0.8526\n",
      "Epoch 97/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3911 - mae: 0.3911 - mse: 0.2641\n",
      "Epoch 97: val_loss did not improve from 0.62711\n",
      "296/296 [==============================] - 141s 477ms/step - loss: 0.3911 - mae: 0.3911 - mse: 0.2641 - val_loss: 0.6483 - val_mae: 0.6483 - val_mse: 0.9260\n",
      "Epoch 98/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3595 - mae: 0.3595 - mse: 0.2155\n",
      "Epoch 98: val_loss did not improve from 0.62711\n",
      "296/296 [==============================] - 142s 482ms/step - loss: 0.3595 - mae: 0.3595 - mse: 0.2155 - val_loss: 0.7200 - val_mae: 0.7200 - val_mse: 1.0310\n",
      "Epoch 99/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3561 - mae: 0.3561 - mse: 0.2127\n",
      "Epoch 99: val_loss did not improve from 0.62711\n",
      "296/296 [==============================] - 142s 481ms/step - loss: 0.3561 - mae: 0.3561 - mse: 0.2127 - val_loss: 1.1279 - val_mae: 1.1279 - val_mse: 1.9294\n",
      "Epoch 100/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3484 - mae: 0.3484 - mse: 0.2032\n",
      "Epoch 100: val_loss did not improve from 0.62711\n",
      "296/296 [==============================] - 142s 479ms/step - loss: 0.3484 - mae: 0.3484 - mse: 0.2032 - val_loss: 0.7148 - val_mae: 0.7148 - val_mse: 1.0612\n",
      "Epoch 101/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3544 - mae: 0.3544 - mse: 0.2086\n",
      "Epoch 101: val_loss did not improve from 0.62711\n",
      "296/296 [==============================] - 142s 481ms/step - loss: 0.3544 - mae: 0.3544 - mse: 0.2086 - val_loss: 0.6560 - val_mae: 0.6560 - val_mse: 0.9613\n",
      "Epoch 102/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3515 - mae: 0.3515 - mse: 0.2135\n",
      "Epoch 102: val_loss did not improve from 0.62711\n",
      "296/296 [==============================] - 142s 481ms/step - loss: 0.3515 - mae: 0.3515 - mse: 0.2135 - val_loss: 0.7007 - val_mae: 0.7007 - val_mse: 1.0436\n",
      "Epoch 103/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3371 - mae: 0.3371 - mse: 0.1932\n",
      "Epoch 103: val_loss did not improve from 0.62711\n",
      "296/296 [==============================] - 142s 481ms/step - loss: 0.3371 - mae: 0.3371 - mse: 0.1932 - val_loss: 0.6402 - val_mae: 0.6402 - val_mse: 0.9453\n",
      "Epoch 104/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3240 - mae: 0.3240 - mse: 0.1800\n",
      "Epoch 104: val_loss did not improve from 0.62711\n",
      "296/296 [==============================] - 140s 472ms/step - loss: 0.3240 - mae: 0.3240 - mse: 0.1800 - val_loss: 0.6515 - val_mae: 0.6515 - val_mse: 0.9652\n",
      "Epoch 105/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3195 - mae: 0.3195 - mse: 0.1711\n",
      "Epoch 105: val_loss did not improve from 0.62711\n",
      "296/296 [==============================] - 141s 476ms/step - loss: 0.3195 - mae: 0.3195 - mse: 0.1711 - val_loss: 0.9612 - val_mae: 0.9612 - val_mse: 1.6543\n",
      "Epoch 106/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3423 - mae: 0.3423 - mse: 0.1960\n",
      "Epoch 106: val_loss did not improve from 0.62711\n",
      "296/296 [==============================] - 142s 480ms/step - loss: 0.3423 - mae: 0.3423 - mse: 0.1960 - val_loss: 0.9652 - val_mae: 0.9652 - val_mse: 1.5503\n",
      "Epoch 107/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3511 - mae: 0.3511 - mse: 0.2095\n",
      "Epoch 107: val_loss did not improve from 0.62711\n",
      "296/296 [==============================] - 140s 471ms/step - loss: 0.3511 - mae: 0.3511 - mse: 0.2095 - val_loss: 0.6731 - val_mae: 0.6731 - val_mse: 1.0082\n",
      "Epoch 108/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3103 - mae: 0.3103 - mse: 0.1628\n",
      "Epoch 108: val_loss did not improve from 0.62711\n",
      "296/296 [==============================] - 142s 480ms/step - loss: 0.3103 - mae: 0.3103 - mse: 0.1628 - val_loss: 0.9986 - val_mae: 0.9986 - val_mse: 1.6447\n",
      "Epoch 109/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3237 - mae: 0.3237 - mse: 0.1806\n",
      "Epoch 109: val_loss did not improve from 0.62711\n",
      "296/296 [==============================] - 142s 480ms/step - loss: 0.3237 - mae: 0.3237 - mse: 0.1806 - val_loss: 0.6382 - val_mae: 0.6382 - val_mse: 0.9743\n",
      "Epoch 110/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3199 - mae: 0.3199 - mse: 0.1716\n",
      "Epoch 110: val_loss did not improve from 0.62711\n",
      "296/296 [==============================] - 142s 480ms/step - loss: 0.3199 - mae: 0.3199 - mse: 0.1716 - val_loss: 1.3142 - val_mae: 1.3142 - val_mse: 2.6518\n",
      "Epoch 111/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3163 - mae: 0.3163 - mse: 0.1670\n",
      "Epoch 111: val_loss did not improve from 0.62711\n",
      "296/296 [==============================] - 141s 477ms/step - loss: 0.3163 - mae: 0.3163 - mse: 0.1670 - val_loss: 0.7402 - val_mae: 0.7402 - val_mse: 1.1374\n",
      "Epoch 112/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3283 - mae: 0.3283 - mse: 0.1773\n",
      "Epoch 112: val_loss did not improve from 0.62711\n",
      "296/296 [==============================] - 140s 474ms/step - loss: 0.3283 - mae: 0.3283 - mse: 0.1773 - val_loss: 0.6640 - val_mae: 0.6640 - val_mse: 0.9714\n",
      "Epoch 113/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3205 - mae: 0.3205 - mse: 0.1693\n",
      "Epoch 113: val_loss improved from 0.62711 to 0.61830, saving model to checkpoint-50-epochs-8-batchs.h5\n",
      "296/296 [==============================] - 143s 482ms/step - loss: 0.3205 - mae: 0.3205 - mse: 0.1693 - val_loss: 0.6183 - val_mae: 0.6183 - val_mse: 0.9097\n",
      "Epoch 114/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3109 - mae: 0.3109 - mse: 0.1612\n",
      "Epoch 114: val_loss did not improve from 0.61830\n",
      "296/296 [==============================] - 139s 470ms/step - loss: 0.3109 - mae: 0.3109 - mse: 0.1612 - val_loss: 0.9899 - val_mae: 0.9899 - val_mse: 1.5721\n",
      "Epoch 115/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3182 - mae: 0.3182 - mse: 0.1685\n",
      "Epoch 115: val_loss did not improve from 0.61830\n",
      "296/296 [==============================] - 141s 478ms/step - loss: 0.3182 - mae: 0.3182 - mse: 0.1685 - val_loss: 1.1556 - val_mae: 1.1556 - val_mse: 1.9568\n",
      "Epoch 116/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.2963 - mae: 0.2963 - mse: 0.1455\n",
      "Epoch 116: val_loss did not improve from 0.61830\n",
      "296/296 [==============================] - 140s 472ms/step - loss: 0.2963 - mae: 0.2963 - mse: 0.1455 - val_loss: 0.8748 - val_mae: 0.8748 - val_mse: 1.3502\n",
      "Epoch 117/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3250 - mae: 0.3250 - mse: 0.1780\n",
      "Epoch 117: val_loss did not improve from 0.61830\n",
      "296/296 [==============================] - 141s 478ms/step - loss: 0.3250 - mae: 0.3250 - mse: 0.1780 - val_loss: 0.6239 - val_mae: 0.6239 - val_mse: 0.9044\n",
      "Epoch 118/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3136 - mae: 0.3136 - mse: 0.1629\n",
      "Epoch 118: val_loss did not improve from 0.61830\n",
      "296/296 [==============================] - 141s 476ms/step - loss: 0.3136 - mae: 0.3136 - mse: 0.1629 - val_loss: 0.6802 - val_mae: 0.6802 - val_mse: 0.9467\n",
      "Epoch 119/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.2997 - mae: 0.2997 - mse: 0.1507\n",
      "Epoch 119: val_loss improved from 0.61830 to 0.59425, saving model to checkpoint-50-epochs-8-batchs.h5\n",
      "296/296 [==============================] - 143s 482ms/step - loss: 0.2997 - mae: 0.2997 - mse: 0.1507 - val_loss: 0.5942 - val_mae: 0.5942 - val_mse: 0.8211\n",
      "Epoch 120/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3104 - mae: 0.3104 - mse: 0.1633\n",
      "Epoch 120: val_loss did not improve from 0.59425\n",
      "296/296 [==============================] - 140s 474ms/step - loss: 0.3104 - mae: 0.3104 - mse: 0.1633 - val_loss: 0.6098 - val_mae: 0.6098 - val_mse: 0.8512\n",
      "Epoch 121/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3083 - mae: 0.3083 - mse: 0.1601\n",
      "Epoch 121: val_loss did not improve from 0.59425\n",
      "296/296 [==============================] - 142s 478ms/step - loss: 0.3083 - mae: 0.3083 - mse: 0.1601 - val_loss: 1.0588 - val_mae: 1.0588 - val_mse: 1.7301\n",
      "Epoch 122/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3236 - mae: 0.3236 - mse: 0.1745\n",
      "Epoch 122: val_loss did not improve from 0.59425\n",
      "296/296 [==============================] - 140s 474ms/step - loss: 0.3236 - mae: 0.3236 - mse: 0.1745 - val_loss: 0.5996 - val_mae: 0.5996 - val_mse: 0.8450\n",
      "Epoch 123/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3288 - mae: 0.3288 - mse: 0.1902\n",
      "Epoch 123: val_loss did not improve from 0.59425\n",
      "296/296 [==============================] - 142s 479ms/step - loss: 0.3288 - mae: 0.3288 - mse: 0.1902 - val_loss: 0.6568 - val_mae: 0.6568 - val_mse: 0.9473\n",
      "Epoch 124/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3043 - mae: 0.3043 - mse: 0.1627\n",
      "Epoch 124: val_loss did not improve from 0.59425\n",
      "296/296 [==============================] - 141s 478ms/step - loss: 0.3043 - mae: 0.3043 - mse: 0.1627 - val_loss: 0.7797 - val_mae: 0.7797 - val_mse: 1.1876\n",
      "Epoch 125/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3004 - mae: 0.3004 - mse: 0.1528\n",
      "Epoch 125: val_loss did not improve from 0.59425\n",
      "296/296 [==============================] - 139s 470ms/step - loss: 0.3004 - mae: 0.3004 - mse: 0.1528 - val_loss: 0.8514 - val_mae: 0.8514 - val_mse: 1.3346\n",
      "Epoch 126/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.2872 - mae: 0.2872 - mse: 0.1392\n",
      "Epoch 126: val_loss did not improve from 0.59425\n",
      "296/296 [==============================] - 141s 478ms/step - loss: 0.2872 - mae: 0.2872 - mse: 0.1392 - val_loss: 0.9712 - val_mae: 0.9712 - val_mse: 1.5517\n",
      "Epoch 127/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3253 - mae: 0.3253 - mse: 0.1732\n",
      "Epoch 127: val_loss did not improve from 0.59425\n",
      "296/296 [==============================] - 142s 480ms/step - loss: 0.3253 - mae: 0.3253 - mse: 0.1732 - val_loss: 0.8019 - val_mae: 0.8019 - val_mse: 1.1545\n",
      "Epoch 128/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3030 - mae: 0.3030 - mse: 0.1588\n",
      "Epoch 128: val_loss did not improve from 0.59425\n",
      "296/296 [==============================] - 141s 478ms/step - loss: 0.3030 - mae: 0.3030 - mse: 0.1588 - val_loss: 0.6729 - val_mae: 0.6729 - val_mse: 0.9899\n",
      "Epoch 129/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3134 - mae: 0.3134 - mse: 0.1654\n",
      "Epoch 129: val_loss did not improve from 0.59425\n",
      "296/296 [==============================] - 140s 472ms/step - loss: 0.3134 - mae: 0.3134 - mse: 0.1654 - val_loss: 0.6302 - val_mae: 0.6302 - val_mse: 0.9211\n",
      "Epoch 130/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3113 - mae: 0.3113 - mse: 0.1812\n",
      "Epoch 130: val_loss did not improve from 0.59425\n",
      "296/296 [==============================] - 140s 472ms/step - loss: 0.3113 - mae: 0.3113 - mse: 0.1812 - val_loss: 0.6009 - val_mae: 0.6009 - val_mse: 0.8514\n",
      "Epoch 131/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3056 - mae: 0.3056 - mse: 0.1573\n",
      "Epoch 131: val_loss did not improve from 0.59425\n",
      "296/296 [==============================] - 139s 471ms/step - loss: 0.3056 - mae: 0.3056 - mse: 0.1573 - val_loss: 0.8179 - val_mae: 0.8179 - val_mse: 1.1914\n",
      "Epoch 132/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.2958 - mae: 0.2958 - mse: 0.1450\n",
      "Epoch 132: val_loss did not improve from 0.59425\n",
      "296/296 [==============================] - 140s 473ms/step - loss: 0.2958 - mae: 0.2958 - mse: 0.1450 - val_loss: 0.7497 - val_mae: 0.7497 - val_mse: 1.1208\n",
      "Epoch 133/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.2979 - mae: 0.2979 - mse: 0.1495\n",
      "Epoch 133: val_loss did not improve from 0.59425\n",
      "296/296 [==============================] - 139s 469ms/step - loss: 0.2979 - mae: 0.2979 - mse: 0.1495 - val_loss: 0.9946 - val_mae: 0.9946 - val_mse: 1.5632\n",
      "Epoch 134/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.2979 - mae: 0.2979 - mse: 0.1641\n",
      "Epoch 134: val_loss did not improve from 0.59425\n",
      "296/296 [==============================] - 139s 471ms/step - loss: 0.2979 - mae: 0.2979 - mse: 0.1641 - val_loss: 0.6407 - val_mae: 0.6407 - val_mse: 0.9489\n",
      "Epoch 135/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3033 - mae: 0.3033 - mse: 0.1565\n",
      "Epoch 135: val_loss did not improve from 0.59425\n",
      "296/296 [==============================] - 140s 472ms/step - loss: 0.3033 - mae: 0.3033 - mse: 0.1565 - val_loss: 0.7062 - val_mae: 0.7062 - val_mse: 1.0930\n",
      "Epoch 136/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.2793 - mae: 0.2793 - mse: 0.1283\n",
      "Epoch 136: val_loss did not improve from 0.59425\n",
      "296/296 [==============================] - 140s 473ms/step - loss: 0.2793 - mae: 0.2793 - mse: 0.1283 - val_loss: 0.6905 - val_mae: 0.6905 - val_mse: 0.9984\n",
      "Epoch 137/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.2852 - mae: 0.2852 - mse: 0.1339\n",
      "Epoch 137: val_loss did not improve from 0.59425\n",
      "296/296 [==============================] - 140s 473ms/step - loss: 0.2852 - mae: 0.2852 - mse: 0.1339 - val_loss: 0.6178 - val_mae: 0.6178 - val_mse: 0.9178\n",
      "Epoch 138/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.2931 - mae: 0.2931 - mse: 0.1627\n",
      "Epoch 138: val_loss did not improve from 0.59425\n",
      "296/296 [==============================] - 141s 476ms/step - loss: 0.2931 - mae: 0.2931 - mse: 0.1627 - val_loss: 0.7921 - val_mae: 0.7921 - val_mse: 1.1658\n",
      "Epoch 139/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3153 - mae: 0.3153 - mse: 0.1681\n",
      "Epoch 139: val_loss did not improve from 0.59425\n",
      "296/296 [==============================] - 138s 467ms/step - loss: 0.3153 - mae: 0.3153 - mse: 0.1681 - val_loss: 0.9051 - val_mae: 0.9051 - val_mse: 1.4062\n",
      "Epoch 140/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3174 - mae: 0.3174 - mse: 0.1917\n",
      "Epoch 140: val_loss did not improve from 0.59425\n",
      "296/296 [==============================] - 138s 466ms/step - loss: 0.3174 - mae: 0.3174 - mse: 0.1917 - val_loss: 0.6067 - val_mae: 0.6067 - val_mse: 0.8657\n",
      "Epoch 141/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.2928 - mae: 0.2928 - mse: 0.1436\n",
      "Epoch 141: val_loss did not improve from 0.59425\n",
      "296/296 [==============================] - 139s 469ms/step - loss: 0.2928 - mae: 0.2928 - mse: 0.1436 - val_loss: 0.7028 - val_mae: 0.7028 - val_mse: 1.0356\n",
      "Epoch 142/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.2847 - mae: 0.2847 - mse: 0.1407\n",
      "Epoch 142: val_loss did not improve from 0.59425\n",
      "296/296 [==============================] - 141s 477ms/step - loss: 0.2847 - mae: 0.2847 - mse: 0.1407 - val_loss: 0.7308 - val_mae: 0.7308 - val_mse: 1.0787\n",
      "Epoch 143/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.2928 - mae: 0.2928 - mse: 0.1468\n",
      "Epoch 143: val_loss did not improve from 0.59425\n",
      "296/296 [==============================] - 141s 478ms/step - loss: 0.2928 - mae: 0.2928 - mse: 0.1468 - val_loss: 0.7072 - val_mae: 0.7072 - val_mse: 1.0836\n",
      "Epoch 144/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.2966 - mae: 0.2966 - mse: 0.1495\n",
      "Epoch 144: val_loss did not improve from 0.59425\n",
      "296/296 [==============================] - 141s 477ms/step - loss: 0.2966 - mae: 0.2966 - mse: 0.1495 - val_loss: 0.8608 - val_mae: 0.8608 - val_mse: 1.2593\n",
      "Epoch 145/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.2713 - mae: 0.2713 - mse: 0.1246\n",
      "Epoch 145: val_loss did not improve from 0.59425\n",
      "296/296 [==============================] - 141s 476ms/step - loss: 0.2713 - mae: 0.2713 - mse: 0.1246 - val_loss: 0.6321 - val_mae: 0.6321 - val_mse: 0.8676\n",
      "Epoch 146/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3042 - mae: 0.3042 - mse: 0.1506\n",
      "Epoch 146: val_loss did not improve from 0.59425\n",
      "296/296 [==============================] - 139s 469ms/step - loss: 0.3042 - mae: 0.3042 - mse: 0.1506 - val_loss: 0.6179 - val_mae: 0.6179 - val_mse: 0.8968\n",
      "Epoch 147/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.3065 - mae: 0.3065 - mse: 0.1608\n",
      "Epoch 147: val_loss did not improve from 0.59425\n",
      "296/296 [==============================] - 139s 468ms/step - loss: 0.3065 - mae: 0.3065 - mse: 0.1608 - val_loss: 0.8454 - val_mae: 0.8454 - val_mse: 1.3296\n",
      "Epoch 148/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.2589 - mae: 0.2589 - mse: 0.1096\n",
      "Epoch 148: val_loss did not improve from 0.59425\n",
      "296/296 [==============================] - 139s 468ms/step - loss: 0.2589 - mae: 0.2589 - mse: 0.1096 - val_loss: 0.5958 - val_mae: 0.5958 - val_mse: 0.8047\n",
      "Epoch 149/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.2743 - mae: 0.2743 - mse: 0.1237\n",
      "Epoch 149: val_loss did not improve from 0.59425\n",
      "296/296 [==============================] - 139s 468ms/step - loss: 0.2743 - mae: 0.2743 - mse: 0.1237 - val_loss: 0.7746 - val_mae: 0.7746 - val_mse: 1.1287\n",
      "Epoch 150/150\n",
      "296/296 [==============================] - ETA: 0s - loss: 0.2712 - mae: 0.2712 - mse: 0.1257\n",
      "Epoch 150: val_loss did not improve from 0.59425\n",
      "296/296 [==============================] - 138s 467ms/step - loss: 0.2712 - mae: 0.2712 - mse: 0.1257 - val_loss: 0.6817 - val_mae: 0.6817 - val_mse: 0.9633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f29a9043100>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_aug_train, y_aug_train, batch_size=8, epochs=150, callbacks=checkpoint, validation_data=(X_aug_val,y_aug_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 17s 1s/step\n"
     ]
    }
   ],
   "source": [
    "test_prediction = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae : 0.836\n",
      "rmse : 1.068\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "print('mae : {:.3f}'.format(mean_absolute_error(test_prediction, y_test)))\n",
    "print('rmse : {:.3f}'.format(np.sqrt(mean_squared_error(test_prediction, y_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__원본 데이터__ <br/>\n",
    "batchsize : 16\n",
    "+ crop 이미지 전처리 안했을 때 mae : 1.139, rmse : 1.422\n",
    "+ crop 이미지 전처리 했을 때 mae : 1.880, rmse : 2.354\n",
    "+ 앞으로 전처리 안하고 진행\n",
    "\n",
    "batchsize : 8\n",
    "+ mae : 1.111, rmse : 1.400 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__data augmentation - 원본:회전:노이즈 = 1:1:1__ <br/>\n",
    "batchsize : 8\n",
    "+ mae : 0.836\n",
    "+ rmse : 1.068"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "junoflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
