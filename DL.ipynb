{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import zipfile\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cuda\n",
      "Current : 0\n",
      "Count : 1\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "print('device :', device)\n",
    "print('Current :', torch.cuda.current_device())\n",
    "print('Count :', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>등록번호</th>\n",
       "      <th>생년월일</th>\n",
       "      <th>성별</th>\n",
       "      <th>진료의</th>\n",
       "      <th>검사 시 나이</th>\n",
       "      <th>신장</th>\n",
       "      <th>체중</th>\n",
       "      <th>BMI</th>\n",
       "      <th>처방일자</th>\n",
       "      <th>시행일자</th>\n",
       "      <th>BA 1</th>\n",
       "      <th>BA 2</th>\n",
       "      <th>Unnamed: 14</th>\n",
       "      <th>No</th>\n",
       "      <th>boneage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1673</td>\n",
       "      <td>8225553</td>\n",
       "      <td>2004-09-30</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>11.654795</td>\n",
       "      <td>144.7</td>\n",
       "      <td>33.0</td>\n",
       "      <td>15.8</td>\n",
       "      <td>2016-05-24</td>\n",
       "      <td>2016-05-24</td>\n",
       "      <td>12.00</td>\n",
       "      <td>12.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.jpg</td>\n",
       "      <td>12.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>228</td>\n",
       "      <td>5889504</td>\n",
       "      <td>2006-09-15</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>8.389041</td>\n",
       "      <td>123.3</td>\n",
       "      <td>25.1</td>\n",
       "      <td>16.6</td>\n",
       "      <td>2014-08-04</td>\n",
       "      <td>2015-02-02</td>\n",
       "      <td>7.00</td>\n",
       "      <td>7.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1162.jpg</td>\n",
       "      <td>7.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>311</td>\n",
       "      <td>5931393</td>\n",
       "      <td>2000-02-02</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>11.161644</td>\n",
       "      <td>138.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>19.5</td>\n",
       "      <td>2011-03-30</td>\n",
       "      <td>2011-03-30</td>\n",
       "      <td>9.75</td>\n",
       "      <td>11.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>636.jpg</td>\n",
       "      <td>10.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2053</td>\n",
       "      <td>8807714</td>\n",
       "      <td>2014-08-20</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>4.967123</td>\n",
       "      <td>102.5</td>\n",
       "      <td>15.2</td>\n",
       "      <td>14.5</td>\n",
       "      <td>2019-08-07</td>\n",
       "      <td>2019-08-07</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>990.jpg</td>\n",
       "      <td>4.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>3726179</td>\n",
       "      <td>2000-05-24</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>10.860274</td>\n",
       "      <td>135.5</td>\n",
       "      <td>32.6</td>\n",
       "      <td>17.8</td>\n",
       "      <td>2011-04-01</td>\n",
       "      <td>2011-04-01</td>\n",
       "      <td>9.50</td>\n",
       "      <td>9.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>618.jpg</td>\n",
       "      <td>9.625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Group     등록번호        생년월일 성별 진료의    검사 시 나이     신장    체중   BMI  \\\n",
       "0   1673  8225553  2004-09-30  F   1  11.654795  144.7  33.0  15.8   \n",
       "1    228  5889504  2006-09-15  M   1   8.389041  123.3  25.1  16.6   \n",
       "2    311  5931393  2000-02-02  M   1  11.161644  138.0  37.0  19.5   \n",
       "3   2053  8807714  2014-08-20  M   1   4.967123  102.5  15.2  14.5   \n",
       "4     32  3726179  2000-05-24  M   1  10.860274  135.5  32.6  17.8   \n",
       "\n",
       "         처방일자        시행일자   BA 1   BA 2 Unnamed: 14        No  boneage  \n",
       "0  2016-05-24  2016-05-24  12.00  12.25         NaN    98.jpg   12.125  \n",
       "1  2014-08-04  2015-02-02   7.00   7.25         NaN  1162.jpg    7.125  \n",
       "2  2011-03-30  2011-03-30   9.75  11.25         NaN   636.jpg   10.500  \n",
       "3  2019-08-07  2019-08-07   4.25   4.00         NaN   990.jpg    4.125  \n",
       "4  2011-04-01  2011-04-01   9.50   9.75         NaN   618.jpg    9.625  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('골밀도 데이터/train_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_img(r1,r2,r3):\n",
    "    tmp_binary_img = []\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8)) #CLAHE 생성\n",
    "    for _,img in enumerate([r1,r2,r3]):\n",
    "        if img is not None:\n",
    "            # resized_img = cv2.resize(img,(1000,400)) # (400, 500)\n",
    "            blured_img = cv2.GaussianBlur(img,(5,5),0)            \n",
    "            clahed_img = clahe.apply(blured_img)          #CLAHE 적용\n",
    "            _,binary_img = cv2.threshold(clahed_img,clahed_img.mean()*1.25,255,cv2.THRESH_BINARY)\n",
    "            \n",
    "            # 최종 이미지 크기\n",
    "            target_size = 600\n",
    "            old_size = binary_img.shape\n",
    "\n",
    "            # 새로운 이미지 생성 (검은색 배경)\n",
    "            new_image = np.zeros((target_size, target_size), dtype=np.uint8)\n",
    "\n",
    "            # 이미지 중앙에 배치하기 위한 좌표 계산\n",
    "            start_x = (target_size - old_size[1]) // 2\n",
    "            start_y = (target_size - old_size[0]) // 2\n",
    "\n",
    "            # 원본 이미지를 중앙에 배치\n",
    "            new_image[start_y:start_y+old_size[0], start_x:start_x+old_size[1]] = binary_img\n",
    "            \n",
    "            tmp_binary_img.append(new_image)\n",
    "                \n",
    "    return np.array(tmp_binary_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_img(img, roi_1, roi_2, roi_3):\n",
    "    \n",
    "    cropped_roi_1_img = img[roi_1[0][1]:roi_1[1][1],roi_1[0][0]:roi_1[1][0]]\n",
    "    cropped_roi_2_img = img[roi_2[0][1]:roi_2[1][1],roi_2[0][0]:roi_2[1][0]]\n",
    "    cropped_roi_3_img = img[roi_3[0][1]:roi_3[1][1],roi_3[0][0]:roi_3[1][0]]\n",
    "    \n",
    "    optimzed_imgs = optimize_img(cropped_roi_1_img,cropped_roi_2_img,cropped_roi_3_img)\n",
    "    return optimzed_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = []\n",
    "y_data = []\n",
    "for k in range(len(data)):\n",
    "    img0 = cv2.imread('골밀도 데이터/rotate_image/' + data.No[k], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # 이미지 thresholding\n",
    "    r_img = np.copy(img0)\n",
    "    height, width = img0.shape\n",
    "    img = img0[0:(int)(height*0.9),0:(int)(width*0.95)]\n",
    "    ret, img = cv2.threshold(img, img0.mean(), 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # 이미지 contouring\n",
    "    contours, hierarchy = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    max_cnt = max(contours, key = cv2.contourArea)\n",
    "    mask = np.zeros(img.shape, dtype= np.uint8)\n",
    "    cv2.drawContours(mask, [max_cnt], -1, (255, 255, 255), -1)\n",
    "\n",
    "    # 볼록한 점 구하기\n",
    "    hull = cv2.convexHull(max_cnt, returnPoints= False)\n",
    "    hull1 = cv2.convexHull(max_cnt)\n",
    "\n",
    "    # 오목한 지점 구하기\n",
    "    defects = cv2.convexityDefects(max_cnt, hull) # 인덱스로 반환\n",
    "\n",
    "    # 거리를 저장할 수 있는 공간 생성\n",
    "    di = []\n",
    "\n",
    "    for index in range(defects.shape[0]):\n",
    "        # 시작점, 끝점, far 점, 거리 할당\n",
    "        sp, ep, fp, distance = defects[index, 0]\n",
    "        \n",
    "        # 거리 저장\n",
    "        di.append(distance)\n",
    "\n",
    "    far_xrange = []\n",
    "    far_yrange = []\n",
    "    start_xrange = []\n",
    "    start_yrange = []\n",
    "\n",
    "    # 가장 오목하게 들어가 있는 부분을 찾기 위해 sorting(내림차순)\n",
    "    di = np.array(di)\n",
    "    s_di = np.sort(di)[::-1]\n",
    "\n",
    "    # 내림차순된 거리들을 6개만 뽑아내기 위해 slice\n",
    "    for i in list(s_di[:6]):\n",
    "        index = np.where(di == i)[0][0]\n",
    "        \n",
    "        sp, ep, fp, _ = defects[index, 0]\n",
    "        \n",
    "        far_xrange.append(max_cnt[fp][0][0])\n",
    "        far_yrange.append(max_cnt[fp][0][1])\n",
    "        \n",
    "        start_xrange.append(max_cnt[sp][0][0])\n",
    "        start_yrange.append(max_cnt[sp][0][1])\n",
    "        \n",
    "\n",
    "    #손목뼈 ROI\n",
    "    carpus_start_point = ((int)(min(far_xrange[4:6])*0.90),(int)(max(far_yrange[4:6])*0.90))\n",
    "    carpus_end_point = (int(max(far_xrange[4:6])*1.05),(int)(max(far_yrange[4:6])*1.15))\n",
    "    \n",
    "    #손목뼈 위쪽에 있는 관절 4개를 추출\n",
    "    four_start_point = ((int)(min(far_xrange[0:4])*0.70),int(min(far_yrange[0:4])*0.95))\n",
    "    four_end_point = (int(max(far_xrange[0:4])*1.05),(int)(max(far_yrange[0:4])*1.05))\n",
    "\n",
    "    #중지 ROI 추출\n",
    "    ## 중지 끝 좌표 구하기\n",
    "    for y,x_r in enumerate(mask) :\n",
    "        if 255 in x_r:\n",
    "            #y에 따른 x rows 중 255인 x값 추출\n",
    "            x_255_indexs = np.where(x_r == 255)[0]\n",
    "\n",
    "            #255인 x값들 중 median 추출\n",
    "            x_255_mid_index = x_255_indexs[(int)(len(x_255_indexs)/2)]\n",
    "            first_255_x_point = x_255_mid_index\n",
    "\n",
    "            first_255_y_point = y\n",
    "            break\n",
    "        \n",
    "    sub = abs(first_255_x_point - far_xrange[0])\n",
    "        \n",
    "    middle_finger_start_point = (int((first_255_x_point - sub)), int(first_255_y_point*0.85))\n",
    "    middle_finger_end_point = (int((first_255_x_point + sub)), int(far_yrange[0]*1.05))\n",
    "\n",
    "    optimized_imgs = crop_img(img0,(carpus_start_point,carpus_end_point),\n",
    "                            (four_start_point, four_end_point),(middle_finger_start_point,middle_finger_end_point))\n",
    "    \n",
    "    optimized_imgs = cv2.resize(optimized_imgs.reshape(600, 600, -1), (256, 256))\n",
    "    \n",
    "    X_data.append(optimized_imgs.reshape(256, 256, -1))\n",
    "    y_data.append(data.boneage[k])\n",
    "\n",
    "X_data = np.array(X_data)\n",
    "y_data = np.array(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(989, 256, 256, 3)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2e204d50f0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_data[0], 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 모델 생성 - Attention-Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separable 합성곱 함수\n",
    "def separable_conv(x, inchannel,outchannel):\n",
    "  x = keras.layers.Conv2D(inchannel, (3,3), strides=1, padding=\"same\")(x)\n",
    "  x = keras.layers.BatchNormalization()(x)\n",
    "  x = keras.layers.Conv2D(outchannel, (1,1), strides=1, padding=\"same\")(x)\n",
    "  x = keras.layers.BatchNormalization()(x)\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resiual_units 함수 \n",
    "def resiual_units(input_x):\n",
    "  x = keras.layers.ReLU()(input_x)\n",
    "  x = separable_conv(x,x.shape[-1],128)\n",
    "  x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x = keras.layers.ReLU()(x)\n",
    "  x = separable_conv(x,x.shape[-1],256)\n",
    "  x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x = keras.layers.ReLU()(x)\n",
    "  x = separable_conv(x,x.shape[-1],512)\n",
    "  x = keras.layers.BatchNormalization()(x)\n",
    "  \n",
    "  input_x = keras.layers.Add()([x,input_x])\n",
    "\n",
    "  return input_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model middle_flow 함수\n",
    "def middle_flow(input_x):\n",
    "  #encoder\n",
    "  x = keras.layers.MaxPool2D((2,2),padding=\"same\")(input_x)\n",
    "  x = resiual_units(x)\n",
    "  x = keras.layers.MaxPool2D((2,2),padding=\"same\")(x)\n",
    "  x = resiual_units(x)\n",
    "  x = keras.layers.MaxPool2D((2,2),padding=\"same\")(x)\n",
    "  x = resiual_units(x)\n",
    "  \n",
    "  #decoder\n",
    "  x = resiual_units(x)\n",
    "  x = keras.layers.UpSampling2D((2,2),interpolation='bilinear')(x)\n",
    "  x = resiual_units(x)\n",
    "  x = keras.layers.UpSampling2D((2,2),interpolation='bilinear')(x)\n",
    "  x = resiual_units(x)\n",
    "  x = keras.layers.UpSampling2D((2,2),interpolation='bilinear')(x)\n",
    "  \n",
    "  x = separable_conv(x,x.shape[-1],512)\n",
    "  x = separable_conv(x,x.shape[-1],512) \n",
    "  \n",
    "  #sigmoid \n",
    "  x = keras.activations.sigmoid(x)\n",
    "  x = keras.layers.Multiply()([input_x,x])\n",
    "  x = keras.layers.Add()([input_x,x])\n",
    "\n",
    "  x = resiual_units(x)\n",
    "\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#골연령 측정 모델\n",
    "# entry flow model\n",
    "input = keras.Input(shape=(256,256,1))\n",
    "x = keras.layers.Conv2D(32, (3,3), strides = 2)(input)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.ReLU()(x)\n",
    "\n",
    "x = keras.layers.Conv2D(64, (3,3), strides=1)(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.ReLU()(x)\n",
    "\n",
    "#첫번째\n",
    "x1 = keras.layers.Conv2D(128,(1,1),strides=2)(x) \n",
    "x1 = keras.layers.BatchNormalization()(x1)\n",
    "\n",
    "x = separable_conv(x,x.shape[-1],128)\n",
    "x = keras.layers.ReLU()(x)\n",
    "\n",
    "x = separable_conv(x,x.shape[-1],128)\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = keras.layers.MaxPool2D((2,2),2,padding=\"same\")(x)\n",
    "\n",
    "x = keras.layers.Add()([x,x1])\n",
    "\n",
    "#2번째\n",
    "x1 = keras.layers.Conv2D(512,(1,1),strides=2)(x)\n",
    "x1 = keras.layers.BatchNormalization()(x1)\n",
    "\n",
    "x = separable_conv(x,x.shape[-1],512)\n",
    "x = keras.layers.ReLU()(x)\n",
    "\n",
    "x = separable_conv(x,x.shape[-1],512)\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = keras.layers.MaxPool2D((2,2),2,padding=\"same\")(x)\n",
    "\n",
    "x = keras.layers.Add()([x,x1])\n",
    "\n",
    "\n",
    "#middle flow model\n",
    "x = middle_flow(x)\n",
    "\n",
    "\n",
    "#exit flow model\n",
    "x1 = keras.layers.Conv2D(1024,(1,1),strides=2)(x)\n",
    "\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = separable_conv(x,x.shape[-1],728)\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = separable_conv(x,x.shape[-1],1024)\n",
    "x = keras.layers.MaxPool2D((2,2),2,padding=\"same\")(x)\n",
    "\n",
    "x = keras.layers.Add()([x,x1])\n",
    "\n",
    "\n",
    "x = separable_conv(x,x.shape[-1],1536)\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = separable_conv(x,x.shape[-1],2048)\n",
    "x = keras.layers.ReLU()(x)\n",
    "\n",
    "x = keras.layers.GlobalAvgPool2D()(x)\n",
    "\n",
    "x = keras.layers.Dense(1000,activation='relu')(x)\n",
    "x = keras.layers.Dense(256,activation='relu')(x)\n",
    "x = keras.layers.Dense(1)(x)\n",
    "\n",
    "model = keras.models.Model(input,x)\n",
    "model.compile(optimizer='adam', loss='mae', metrics=['mae','mse'], run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiuser/.conda/envs/junoflow/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('./tjnet_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 256, 256, 1)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d_67 (Conv2D)          (None, 127, 127, 32)         320       ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_87 (Ba  (None, 127, 127, 32)         128       ['conv2d_67[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_31 (ReLU)             (None, 127, 127, 32)         0         ['batch_normalization_87[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_68 (Conv2D)          (None, 125, 125, 64)         18496     ['re_lu_31[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_88 (Ba  (None, 125, 125, 64)         256       ['conv2d_68[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_32 (ReLU)             (None, 125, 125, 64)         0         ['batch_normalization_88[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_70 (Conv2D)          (None, 125, 125, 64)         36928     ['re_lu_32[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_90 (Ba  (None, 125, 125, 64)         256       ['conv2d_70[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_71 (Conv2D)          (None, 125, 125, 128)        8320      ['batch_normalization_90[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_91 (Ba  (None, 125, 125, 128)        512       ['conv2d_71[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_33 (ReLU)             (None, 125, 125, 128)        0         ['batch_normalization_91[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_72 (Conv2D)          (None, 125, 125, 128)        147584    ['re_lu_33[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_92 (Ba  (None, 125, 125, 128)        512       ['conv2d_72[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_73 (Conv2D)          (None, 125, 125, 128)        16512     ['batch_normalization_92[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_93 (Ba  (None, 125, 125, 128)        512       ['conv2d_73[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_34 (ReLU)             (None, 125, 125, 128)        0         ['batch_normalization_93[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_69 (Conv2D)          (None, 63, 63, 128)          8320      ['re_lu_32[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPoolin  (None, 63, 63, 128)          0         ['re_lu_34[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_89 (Ba  (None, 63, 63, 128)          512       ['conv2d_69[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_11 (Add)                (None, 63, 63, 128)          0         ['max_pooling2d_6[0][0]',     \n",
      "                                                                     'batch_normalization_89[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_75 (Conv2D)          (None, 63, 63, 128)          147584    ['add_11[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_95 (Ba  (None, 63, 63, 128)          512       ['conv2d_75[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_76 (Conv2D)          (None, 63, 63, 512)          66048     ['batch_normalization_95[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_96 (Ba  (None, 63, 63, 512)          2048      ['conv2d_76[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_35 (ReLU)             (None, 63, 63, 512)          0         ['batch_normalization_96[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_77 (Conv2D)          (None, 63, 63, 512)          2359808   ['re_lu_35[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_97 (Ba  (None, 63, 63, 512)          2048      ['conv2d_77[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_78 (Conv2D)          (None, 63, 63, 512)          262656    ['batch_normalization_97[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_98 (Ba  (None, 63, 63, 512)          2048      ['conv2d_78[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " re_lu_36 (ReLU)             (None, 63, 63, 512)          0         ['batch_normalization_98[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_74 (Conv2D)          (None, 32, 32, 512)          66048     ['add_11[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPoolin  (None, 32, 32, 512)          0         ['re_lu_36[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_94 (Ba  (None, 32, 32, 512)          2048      ['conv2d_74[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_12 (Add)                (None, 32, 32, 512)          0         ['max_pooling2d_7[0][0]',     \n",
      "                                                                     'batch_normalization_94[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " max_pooling2d_8 (MaxPoolin  (None, 16, 16, 512)          0         ['add_12[0][0]']              \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " re_lu_37 (ReLU)             (None, 16, 16, 512)          0         ['max_pooling2d_8[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_79 (Conv2D)          (None, 16, 16, 512)          2359808   ['re_lu_37[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_99 (Ba  (None, 16, 16, 512)          2048      ['conv2d_79[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_80 (Conv2D)          (None, 16, 16, 128)          65664     ['batch_normalization_99[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_100 (B  (None, 16, 16, 128)          512       ['conv2d_80[0][0]']           \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_101 (B  (None, 16, 16, 128)          512       ['batch_normalization_100[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " re_lu_38 (ReLU)             (None, 16, 16, 128)          0         ['batch_normalization_101[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_81 (Conv2D)          (None, 16, 16, 128)          147584    ['re_lu_38[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_102 (B  (None, 16, 16, 128)          512       ['conv2d_81[0][0]']           \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_82 (Conv2D)          (None, 16, 16, 256)          33024     ['batch_normalization_102[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_103 (B  (None, 16, 16, 256)          1024      ['conv2d_82[0][0]']           \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_104 (B  (None, 16, 16, 256)          1024      ['batch_normalization_103[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " re_lu_39 (ReLU)             (None, 16, 16, 256)          0         ['batch_normalization_104[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_83 (Conv2D)          (None, 16, 16, 256)          590080    ['re_lu_39[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_105 (B  (None, 16, 16, 256)          1024      ['conv2d_83[0][0]']           \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_84 (Conv2D)          (None, 16, 16, 512)          131584    ['batch_normalization_105[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_106 (B  (None, 16, 16, 512)          2048      ['conv2d_84[0][0]']           \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_107 (B  (None, 16, 16, 512)          2048      ['batch_normalization_106[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " add_13 (Add)                (None, 16, 16, 512)          0         ['batch_normalization_107[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'max_pooling2d_8[0][0]']     \n",
      "                                                                                                  \n",
      " max_pooling2d_9 (MaxPoolin  (None, 8, 8, 512)            0         ['add_13[0][0]']              \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " re_lu_40 (ReLU)             (None, 8, 8, 512)            0         ['max_pooling2d_9[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_85 (Conv2D)          (None, 8, 8, 512)            2359808   ['re_lu_40[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_108 (B  (None, 8, 8, 512)            2048      ['conv2d_85[0][0]']           \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_86 (Conv2D)          (None, 8, 8, 128)            65664     ['batch_normalization_108[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_109 (B  (None, 8, 8, 128)            512       ['conv2d_86[0][0]']           \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_110 (B  (None, 8, 8, 128)            512       ['batch_normalization_109[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " re_lu_41 (ReLU)             (None, 8, 8, 128)            0         ['batch_normalization_110[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_87 (Conv2D)          (None, 8, 8, 128)            147584    ['re_lu_41[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_111 (B  (None, 8, 8, 128)            512       ['conv2d_87[0][0]']           \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_88 (Conv2D)          (None, 8, 8, 256)            33024     ['batch_normalization_111[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_112 (B  (None, 8, 8, 256)            1024      ['conv2d_88[0][0]']           \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_113 (B  (None, 8, 8, 256)            1024      ['batch_normalization_112[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " re_lu_42 (ReLU)             (None, 8, 8, 256)            0         ['batch_normalization_113[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_89 (Conv2D)          (None, 8, 8, 256)            590080    ['re_lu_42[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_114 (B  (None, 8, 8, 256)            1024      ['conv2d_89[0][0]']           \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_90 (Conv2D)          (None, 8, 8, 512)            131584    ['batch_normalization_114[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_115 (B  (None, 8, 8, 512)            2048      ['conv2d_90[0][0]']           \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_116 (B  (None, 8, 8, 512)            2048      ['batch_normalization_115[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " add_14 (Add)                (None, 8, 8, 512)            0         ['batch_normalization_116[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'max_pooling2d_9[0][0]']     \n",
      "                                                                                                  \n",
      " max_pooling2d_10 (MaxPooli  (None, 4, 4, 512)            0         ['add_14[0][0]']              \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " re_lu_43 (ReLU)             (None, 4, 4, 512)            0         ['max_pooling2d_10[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_91 (Conv2D)          (None, 4, 4, 512)            2359808   ['re_lu_43[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_117 (B  (None, 4, 4, 512)            2048      ['conv2d_91[0][0]']           \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_92 (Conv2D)          (None, 4, 4, 128)            65664     ['batch_normalization_117[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_118 (B  (None, 4, 4, 128)            512       ['conv2d_92[0][0]']           \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_119 (B  (None, 4, 4, 128)            512       ['batch_normalization_118[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " re_lu_44 (ReLU)             (None, 4, 4, 128)            0         ['batch_normalization_119[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_93 (Conv2D)          (None, 4, 4, 128)            147584    ['re_lu_44[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_120 (B  (None, 4, 4, 128)            512       ['conv2d_93[0][0]']           \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_94 (Conv2D)          (None, 4, 4, 256)            33024     ['batch_normalization_120[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_121 (B  (None, 4, 4, 256)            1024      ['conv2d_94[0][0]']           \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_122 (B  (None, 4, 4, 256)            1024      ['batch_normalization_121[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " re_lu_45 (ReLU)             (None, 4, 4, 256)            0         ['batch_normalization_122[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_95 (Conv2D)          (None, 4, 4, 256)            590080    ['re_lu_45[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_123 (B  (None, 4, 4, 256)            1024      ['conv2d_95[0][0]']           \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_96 (Conv2D)          (None, 4, 4, 512)            131584    ['batch_normalization_123[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_124 (B  (None, 4, 4, 512)            2048      ['conv2d_96[0][0]']           \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_125 (B  (None, 4, 4, 512)            2048      ['batch_normalization_124[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " add_15 (Add)                (None, 4, 4, 512)            0         ['batch_normalization_125[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'max_pooling2d_10[0][0]']    \n",
      "                                                                                                  \n",
      " re_lu_46 (ReLU)             (None, 4, 4, 512)            0         ['add_15[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_97 (Conv2D)          (None, 4, 4, 512)            2359808   ['re_lu_46[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_126 (B  (None, 4, 4, 512)            2048      ['conv2d_97[0][0]']           \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_98 (Conv2D)          (None, 4, 4, 128)            65664     ['batch_normalization_126[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_127 (B  (None, 4, 4, 128)            512       ['conv2d_98[0][0]']           \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_128 (B  (None, 4, 4, 128)            512       ['batch_normalization_127[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " re_lu_47 (ReLU)             (None, 4, 4, 128)            0         ['batch_normalization_128[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_99 (Conv2D)          (None, 4, 4, 128)            147584    ['re_lu_47[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_129 (B  (None, 4, 4, 128)            512       ['conv2d_99[0][0]']           \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_100 (Conv2D)         (None, 4, 4, 256)            33024     ['batch_normalization_129[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_130 (B  (None, 4, 4, 256)            1024      ['conv2d_100[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_131 (B  (None, 4, 4, 256)            1024      ['batch_normalization_130[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " re_lu_48 (ReLU)             (None, 4, 4, 256)            0         ['batch_normalization_131[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_101 (Conv2D)         (None, 4, 4, 256)            590080    ['re_lu_48[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_132 (B  (None, 4, 4, 256)            1024      ['conv2d_101[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_102 (Conv2D)         (None, 4, 4, 512)            131584    ['batch_normalization_132[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_133 (B  (None, 4, 4, 512)            2048      ['conv2d_102[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_134 (B  (None, 4, 4, 512)            2048      ['batch_normalization_133[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " add_16 (Add)                (None, 4, 4, 512)            0         ['batch_normalization_134[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'add_15[0][0]']              \n",
      "                                                                                                  \n",
      " up_sampling2d_3 (UpSamplin  (None, 8, 8, 512)            0         ['add_16[0][0]']              \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " re_lu_49 (ReLU)             (None, 8, 8, 512)            0         ['up_sampling2d_3[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_103 (Conv2D)         (None, 8, 8, 512)            2359808   ['re_lu_49[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_135 (B  (None, 8, 8, 512)            2048      ['conv2d_103[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_104 (Conv2D)         (None, 8, 8, 128)            65664     ['batch_normalization_135[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_136 (B  (None, 8, 8, 128)            512       ['conv2d_104[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_137 (B  (None, 8, 8, 128)            512       ['batch_normalization_136[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " re_lu_50 (ReLU)             (None, 8, 8, 128)            0         ['batch_normalization_137[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_105 (Conv2D)         (None, 8, 8, 128)            147584    ['re_lu_50[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_138 (B  (None, 8, 8, 128)            512       ['conv2d_105[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_106 (Conv2D)         (None, 8, 8, 256)            33024     ['batch_normalization_138[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_139 (B  (None, 8, 8, 256)            1024      ['conv2d_106[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_140 (B  (None, 8, 8, 256)            1024      ['batch_normalization_139[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " re_lu_51 (ReLU)             (None, 8, 8, 256)            0         ['batch_normalization_140[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_107 (Conv2D)         (None, 8, 8, 256)            590080    ['re_lu_51[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_141 (B  (None, 8, 8, 256)            1024      ['conv2d_107[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_108 (Conv2D)         (None, 8, 8, 512)            131584    ['batch_normalization_141[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_142 (B  (None, 8, 8, 512)            2048      ['conv2d_108[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_143 (B  (None, 8, 8, 512)            2048      ['batch_normalization_142[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " add_17 (Add)                (None, 8, 8, 512)            0         ['batch_normalization_143[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'up_sampling2d_3[0][0]']     \n",
      "                                                                                                  \n",
      " up_sampling2d_4 (UpSamplin  (None, 16, 16, 512)          0         ['add_17[0][0]']              \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " re_lu_52 (ReLU)             (None, 16, 16, 512)          0         ['up_sampling2d_4[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_109 (Conv2D)         (None, 16, 16, 512)          2359808   ['re_lu_52[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_144 (B  (None, 16, 16, 512)          2048      ['conv2d_109[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_110 (Conv2D)         (None, 16, 16, 128)          65664     ['batch_normalization_144[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_145 (B  (None, 16, 16, 128)          512       ['conv2d_110[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_146 (B  (None, 16, 16, 128)          512       ['batch_normalization_145[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " re_lu_53 (ReLU)             (None, 16, 16, 128)          0         ['batch_normalization_146[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_111 (Conv2D)         (None, 16, 16, 128)          147584    ['re_lu_53[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_147 (B  (None, 16, 16, 128)          512       ['conv2d_111[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_112 (Conv2D)         (None, 16, 16, 256)          33024     ['batch_normalization_147[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_148 (B  (None, 16, 16, 256)          1024      ['conv2d_112[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_149 (B  (None, 16, 16, 256)          1024      ['batch_normalization_148[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " re_lu_54 (ReLU)             (None, 16, 16, 256)          0         ['batch_normalization_149[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_113 (Conv2D)         (None, 16, 16, 256)          590080    ['re_lu_54[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_150 (B  (None, 16, 16, 256)          1024      ['conv2d_113[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_114 (Conv2D)         (None, 16, 16, 512)          131584    ['batch_normalization_150[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_151 (B  (None, 16, 16, 512)          2048      ['conv2d_114[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_152 (B  (None, 16, 16, 512)          2048      ['batch_normalization_151[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " add_18 (Add)                (None, 16, 16, 512)          0         ['batch_normalization_152[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'up_sampling2d_4[0][0]']     \n",
      "                                                                                                  \n",
      " up_sampling2d_5 (UpSamplin  (None, 32, 32, 512)          0         ['add_18[0][0]']              \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_115 (Conv2D)         (None, 32, 32, 512)          2359808   ['up_sampling2d_5[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_153 (B  (None, 32, 32, 512)          2048      ['conv2d_115[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_116 (Conv2D)         (None, 32, 32, 512)          262656    ['batch_normalization_153[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_154 (B  (None, 32, 32, 512)          2048      ['conv2d_116[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_117 (Conv2D)         (None, 32, 32, 512)          2359808   ['batch_normalization_154[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_155 (B  (None, 32, 32, 512)          2048      ['conv2d_117[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_118 (Conv2D)         (None, 32, 32, 512)          262656    ['batch_normalization_155[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_156 (B  (None, 32, 32, 512)          2048      ['conv2d_118[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_1 (TFOpLam  (None, 32, 32, 512)          0         ['batch_normalization_156[0][0\n",
      " bda)                                                               ]']                           \n",
      "                                                                                                  \n",
      " multiply_1 (Multiply)       (None, 32, 32, 512)          0         ['add_12[0][0]',              \n",
      "                                                                     'tf.math.sigmoid_1[0][0]']   \n",
      "                                                                                                  \n",
      " add_19 (Add)                (None, 32, 32, 512)          0         ['add_12[0][0]',              \n",
      "                                                                     'multiply_1[0][0]']          \n",
      "                                                                                                  \n",
      " re_lu_55 (ReLU)             (None, 32, 32, 512)          0         ['add_19[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_119 (Conv2D)         (None, 32, 32, 512)          2359808   ['re_lu_55[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_157 (B  (None, 32, 32, 512)          2048      ['conv2d_119[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_120 (Conv2D)         (None, 32, 32, 128)          65664     ['batch_normalization_157[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_158 (B  (None, 32, 32, 128)          512       ['conv2d_120[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_159 (B  (None, 32, 32, 128)          512       ['batch_normalization_158[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " re_lu_56 (ReLU)             (None, 32, 32, 128)          0         ['batch_normalization_159[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_121 (Conv2D)         (None, 32, 32, 128)          147584    ['re_lu_56[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_160 (B  (None, 32, 32, 128)          512       ['conv2d_121[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_122 (Conv2D)         (None, 32, 32, 256)          33024     ['batch_normalization_160[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_161 (B  (None, 32, 32, 256)          1024      ['conv2d_122[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_162 (B  (None, 32, 32, 256)          1024      ['batch_normalization_161[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " re_lu_57 (ReLU)             (None, 32, 32, 256)          0         ['batch_normalization_162[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_123 (Conv2D)         (None, 32, 32, 256)          590080    ['re_lu_57[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_163 (B  (None, 32, 32, 256)          1024      ['conv2d_123[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_124 (Conv2D)         (None, 32, 32, 512)          131584    ['batch_normalization_163[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_164 (B  (None, 32, 32, 512)          2048      ['conv2d_124[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_165 (B  (None, 32, 32, 512)          2048      ['batch_normalization_164[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " add_20 (Add)                (None, 32, 32, 512)          0         ['batch_normalization_165[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'add_19[0][0]']              \n",
      "                                                                                                  \n",
      " re_lu_58 (ReLU)             (None, 32, 32, 512)          0         ['add_20[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_126 (Conv2D)         (None, 32, 32, 512)          2359808   ['re_lu_58[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_166 (B  (None, 32, 32, 512)          2048      ['conv2d_126[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_127 (Conv2D)         (None, 32, 32, 728)          373464    ['batch_normalization_166[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_167 (B  (None, 32, 32, 728)          2912      ['conv2d_127[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " re_lu_59 (ReLU)             (None, 32, 32, 728)          0         ['batch_normalization_167[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_128 (Conv2D)         (None, 32, 32, 728)          4770584   ['re_lu_59[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_168 (B  (None, 32, 32, 728)          2912      ['conv2d_128[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_129 (Conv2D)         (None, 32, 32, 1024)         746496    ['batch_normalization_168[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_169 (B  (None, 32, 32, 1024)         4096      ['conv2d_129[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_11 (MaxPooli  (None, 16, 16, 1024)         0         ['batch_normalization_169[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " conv2d_125 (Conv2D)         (None, 16, 16, 1024)         525312    ['add_20[0][0]']              \n",
      "                                                                                                  \n",
      " add_21 (Add)                (None, 16, 16, 1024)         0         ['max_pooling2d_11[0][0]',    \n",
      "                                                                     'conv2d_125[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_130 (Conv2D)         (None, 16, 16, 1024)         9438208   ['add_21[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_170 (B  (None, 16, 16, 1024)         4096      ['conv2d_130[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_131 (Conv2D)         (None, 16, 16, 1536)         1574400   ['batch_normalization_170[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_171 (B  (None, 16, 16, 1536)         6144      ['conv2d_131[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " re_lu_60 (ReLU)             (None, 16, 16, 1536)         0         ['batch_normalization_171[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_132 (Conv2D)         (None, 16, 16, 1536)         2123520   ['re_lu_60[0][0]']            \n",
      "                                                          0                                       \n",
      "                                                                                                  \n",
      " batch_normalization_172 (B  (None, 16, 16, 1536)         6144      ['conv2d_132[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_133 (Conv2D)         (None, 16, 16, 2048)         3147776   ['batch_normalization_172[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_173 (B  (None, 16, 16, 2048)         8192      ['conv2d_133[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " re_lu_61 (ReLU)             (None, 16, 16, 2048)         0         ['batch_normalization_173[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1  (None, 2048)                 0         ['re_lu_61[0][0]']            \n",
      "  (GlobalAveragePooling2D)                                                                        \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 1000)                 2049000   ['global_average_pooling2d_1[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 256)                  256256    ['dense_3[0][0]']             \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 1)                    257       ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 78285913 (298.64 MB)\n",
      "Trainable params: 78220217 (298.39 MB)\n",
      "Non-trainable params: 65696 (256.62 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.zeros((256,256))\n",
    "for _, x in enumerate(X_data):\n",
    "  if _ == 0:\n",
    "    tmp = x[:, :, 0]\n",
    "    tmp = tmp.reshape(1,256,256)\n",
    "  else:\n",
    "    tmp = np.concatenate([tmp,x[:, :, 0].reshape(1, 256, 256)])\n",
    "\n",
    "X_data = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(989, 256, 256, 1)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data.reshape(989, 256, 256, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, val_x, train_y, val_y = train_test_split(X_data.reshape(989, 256, 256, -1), y_data, \n",
    "                                                  random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filename = 'checkpoint-50-epochs-16-batchs.h5'\n",
    "checkpoint = ModelCheckpoint(filename,mointor='val_loss',verbose=1,save_best_only=True,mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 2.6308 - mae: 2.6308 - mse: 11.5501\n",
      "Epoch 1: val_loss improved from inf to 194030.18750, saving model to checkpoint-50-epochs-16-batchs.h5\n",
      "50/50 [==============================] - 41s 778ms/step - loss: 2.6308 - mae: 2.6308 - mse: 11.5501 - val_loss: 194030.1875 - val_mae: 194030.1875 - val_mse: 38434791424.0000\n",
      "Epoch 2/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 2.1537 - mae: 2.1537 - mse: 7.5153\n",
      "Epoch 2: val_loss improved from 194030.18750 to 1034.92200, saving model to checkpoint-50-epochs-16-batchs.h5\n",
      "50/50 [==============================] - 27s 535ms/step - loss: 2.1537 - mae: 2.1537 - mse: 7.5153 - val_loss: 1034.9220 - val_mae: 1034.9220 - val_mse: 1091814.8750\n",
      "Epoch 3/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.5997 - mae: 1.5997 - mse: 4.2564\n",
      "Epoch 3: val_loss improved from 1034.92200 to 230.76282, saving model to checkpoint-50-epochs-16-batchs.h5\n",
      "50/50 [==============================] - 27s 546ms/step - loss: 1.5997 - mae: 1.5997 - mse: 4.2564 - val_loss: 230.7628 - val_mae: 230.7628 - val_mse: 54495.9688\n",
      "Epoch 4/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.6516 - mae: 1.6516 - mse: 4.3251\n",
      "Epoch 4: val_loss improved from 230.76282 to 109.79980, saving model to checkpoint-50-epochs-16-batchs.h5\n",
      "50/50 [==============================] - 27s 547ms/step - loss: 1.6516 - mae: 1.6516 - mse: 4.3251 - val_loss: 109.7998 - val_mae: 109.7998 - val_mse: 12416.4941\n",
      "Epoch 5/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.6110 - mae: 1.6110 - mse: 4.2295\n",
      "Epoch 5: val_loss improved from 109.79980 to 68.82855, saving model to checkpoint-50-epochs-16-batchs.h5\n",
      "50/50 [==============================] - 27s 536ms/step - loss: 1.6110 - mae: 1.6110 - mse: 4.2295 - val_loss: 68.8286 - val_mae: 68.8286 - val_mse: 4898.1914\n",
      "Epoch 6/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.5312 - mae: 1.5312 - mse: 3.7709\n",
      "Epoch 6: val_loss improved from 68.82855 to 18.48287, saving model to checkpoint-50-epochs-16-batchs.h5\n",
      "50/50 [==============================] - 26s 531ms/step - loss: 1.5312 - mae: 1.5312 - mse: 3.7709 - val_loss: 18.4829 - val_mae: 18.4829 - val_mse: 367.9817\n",
      "Epoch 7/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.5341 - mae: 1.5341 - mse: 3.6849\n",
      "Epoch 7: val_loss did not improve from 18.48287\n",
      "50/50 [==============================] - 25s 500ms/step - loss: 1.5341 - mae: 1.5341 - mse: 3.6849 - val_loss: 23.4681 - val_mae: 23.4681 - val_mse: 622.1218\n",
      "Epoch 8/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4362 - mae: 1.4362 - mse: 3.3268\n",
      "Epoch 8: val_loss did not improve from 18.48287\n",
      "50/50 [==============================] - 25s 504ms/step - loss: 1.4362 - mae: 1.4362 - mse: 3.3268 - val_loss: 23.0966 - val_mae: 23.0966 - val_mse: 570.1891\n",
      "Epoch 9/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4270 - mae: 1.4270 - mse: 3.1991\n",
      "Epoch 9: val_loss did not improve from 18.48287\n",
      "50/50 [==============================] - 26s 518ms/step - loss: 1.4270 - mae: 1.4270 - mse: 3.1991 - val_loss: 20.6349 - val_mae: 20.6349 - val_mse: 452.0711\n",
      "Epoch 10/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.5690 - mae: 1.5690 - mse: 3.8758\n",
      "Epoch 10: val_loss did not improve from 18.48287\n",
      "50/50 [==============================] - 25s 502ms/step - loss: 1.5690 - mae: 1.5690 - mse: 3.8758 - val_loss: 25.4752 - val_mae: 25.4752 - val_mse: 777.4846\n",
      "Epoch 11/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4445 - mae: 1.4445 - mse: 3.2794\n",
      "Epoch 11: val_loss did not improve from 18.48287\n",
      "50/50 [==============================] - 25s 501ms/step - loss: 1.4445 - mae: 1.4445 - mse: 3.2794 - val_loss: 22.1708 - val_mae: 22.1708 - val_mse: 514.7240\n",
      "Epoch 12/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.2723 - mae: 1.2723 - mse: 2.7189\n",
      "Epoch 12: val_loss did not improve from 18.48287\n",
      "50/50 [==============================] - 25s 509ms/step - loss: 1.2723 - mae: 1.2723 - mse: 2.7189 - val_loss: 21.0188 - val_mae: 21.0188 - val_mse: 464.0455\n",
      "Epoch 13/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.2232 - mae: 1.2232 - mse: 2.4027\n",
      "Epoch 13: val_loss did not improve from 18.48287\n",
      "50/50 [==============================] - 25s 498ms/step - loss: 1.2232 - mae: 1.2232 - mse: 2.4027 - val_loss: 30.3862 - val_mae: 30.3862 - val_mse: 992.6578\n",
      "Epoch 14/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.0829 - mae: 1.0829 - mse: 1.9031\n",
      "Epoch 14: val_loss improved from 18.48287 to 8.39026, saving model to checkpoint-50-epochs-16-batchs.h5\n",
      "50/50 [==============================] - 27s 546ms/step - loss: 1.0829 - mae: 1.0829 - mse: 1.9031 - val_loss: 8.3903 - val_mae: 8.3903 - val_mse: 78.1637\n",
      "Epoch 15/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.0683 - mae: 1.0683 - mse: 1.8588\n",
      "Epoch 15: val_loss improved from 8.39026 to 7.76640, saving model to checkpoint-50-epochs-16-batchs.h5\n",
      "50/50 [==============================] - 29s 581ms/step - loss: 1.0683 - mae: 1.0683 - mse: 1.8588 - val_loss: 7.7664 - val_mae: 7.7664 - val_mse: 65.3098\n",
      "Epoch 16/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.2121 - mae: 1.2121 - mse: 2.3614\n",
      "Epoch 16: val_loss did not improve from 7.76640\n",
      "50/50 [==============================] - 26s 514ms/step - loss: 1.2121 - mae: 1.2121 - mse: 2.3614 - val_loss: 18.1650 - val_mae: 18.1650 - val_mse: 361.0378\n",
      "Epoch 17/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.1548 - mae: 1.1548 - mse: 2.3847\n",
      "Epoch 17: val_loss improved from 7.76640 to 3.77003, saving model to checkpoint-50-epochs-16-batchs.h5\n",
      "50/50 [==============================] - 27s 532ms/step - loss: 1.1548 - mae: 1.1548 - mse: 2.3847 - val_loss: 3.7700 - val_mae: 3.7700 - val_mse: 18.8093\n",
      "Epoch 18/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.5296 - mae: 1.5296 - mse: 3.9521\n",
      "Epoch 18: val_loss did not improve from 3.77003\n",
      "50/50 [==============================] - 25s 495ms/step - loss: 1.5296 - mae: 1.5296 - mse: 3.9521 - val_loss: 3.8283 - val_mae: 3.8283 - val_mse: 20.8986\n",
      "Epoch 19/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.1924 - mae: 1.1924 - mse: 2.3491\n",
      "Epoch 19: val_loss did not improve from 3.77003\n",
      "50/50 [==============================] - 25s 495ms/step - loss: 1.1924 - mae: 1.1924 - mse: 2.3491 - val_loss: 17.7262 - val_mae: 17.7262 - val_mse: 353.6925\n",
      "Epoch 20/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.1255 - mae: 1.1255 - mse: 2.1047\n",
      "Epoch 20: val_loss did not improve from 3.77003\n",
      "50/50 [==============================] - 25s 501ms/step - loss: 1.1255 - mae: 1.1255 - mse: 2.1047 - val_loss: 22.0868 - val_mae: 22.0868 - val_mse: 540.0989\n",
      "Epoch 21/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.1236 - mae: 1.1236 - mse: 2.1167\n",
      "Epoch 21: val_loss did not improve from 3.77003\n",
      "50/50 [==============================] - 25s 507ms/step - loss: 1.1236 - mae: 1.1236 - mse: 2.1167 - val_loss: 8.0867 - val_mae: 8.0867 - val_mse: 80.3437\n",
      "Epoch 22/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.9899 - mae: 0.9899 - mse: 1.6585\n",
      "Epoch 22: val_loss improved from 3.77003 to 1.53742, saving model to checkpoint-50-epochs-16-batchs.h5\n",
      "50/50 [==============================] - 27s 538ms/step - loss: 0.9899 - mae: 0.9899 - mse: 1.6585 - val_loss: 1.5374 - val_mae: 1.5374 - val_mse: 4.0448\n",
      "Epoch 23/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.2522 - mae: 1.2522 - mse: 2.5233\n",
      "Epoch 23: val_loss did not improve from 1.53742\n",
      "50/50 [==============================] - 25s 492ms/step - loss: 1.2522 - mae: 1.2522 - mse: 2.5233 - val_loss: 5.2914 - val_mae: 5.2914 - val_mse: 35.3765\n",
      "Epoch 24/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.9486 - mae: 0.9486 - mse: 1.4621\n",
      "Epoch 24: val_loss did not improve from 1.53742\n",
      "50/50 [==============================] - 27s 534ms/step - loss: 0.9486 - mae: 0.9486 - mse: 1.4621 - val_loss: 3.8603 - val_mae: 3.8603 - val_mse: 18.2714\n",
      "Epoch 25/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.1308 - mae: 1.1308 - mse: 2.1639\n",
      "Epoch 25: val_loss did not improve from 1.53742\n",
      "50/50 [==============================] - 25s 495ms/step - loss: 1.1308 - mae: 1.1308 - mse: 2.1639 - val_loss: 3.8326 - val_mae: 3.8326 - val_mse: 17.5247\n",
      "Epoch 26/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.0165 - mae: 1.0165 - mse: 1.7110\n",
      "Epoch 26: val_loss did not improve from 1.53742\n",
      "50/50 [==============================] - 25s 493ms/step - loss: 1.0165 - mae: 1.0165 - mse: 1.7110 - val_loss: 6.9558 - val_mae: 6.9558 - val_mse: 57.6229\n",
      "Epoch 27/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8970 - mae: 0.8970 - mse: 1.3628\n",
      "Epoch 27: val_loss did not improve from 1.53742\n",
      "50/50 [==============================] - 25s 504ms/step - loss: 0.8970 - mae: 0.8970 - mse: 1.3628 - val_loss: 1.8785 - val_mae: 1.8785 - val_mse: 5.6815\n",
      "Epoch 28/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.0377 - mae: 1.0377 - mse: 1.6918\n",
      "Epoch 28: val_loss did not improve from 1.53742\n",
      "50/50 [==============================] - 24s 486ms/step - loss: 1.0377 - mae: 1.0377 - mse: 1.6918 - val_loss: 9.7331 - val_mae: 9.7331 - val_mse: 112.0015\n",
      "Epoch 29/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.0245 - mae: 1.0245 - mse: 1.7217\n",
      "Epoch 29: val_loss did not improve from 1.53742\n",
      "50/50 [==============================] - 24s 487ms/step - loss: 1.0245 - mae: 1.0245 - mse: 1.7217 - val_loss: 4.5370 - val_mae: 4.5370 - val_mse: 25.9835\n",
      "Epoch 30/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.9086 - mae: 0.9086 - mse: 1.4082\n",
      "Epoch 30: val_loss did not improve from 1.53742\n",
      "50/50 [==============================] - 25s 509ms/step - loss: 0.9086 - mae: 0.9086 - mse: 1.4082 - val_loss: 7.1654 - val_mae: 7.1654 - val_mse: 62.0228\n",
      "Epoch 31/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.1346 - mae: 1.1346 - mse: 2.2002\n",
      "Epoch 31: val_loss did not improve from 1.53742\n",
      "50/50 [==============================] - 25s 498ms/step - loss: 1.1346 - mae: 1.1346 - mse: 2.2002 - val_loss: 7.0571 - val_mae: 7.0571 - val_mse: 59.5736\n",
      "Epoch 32/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.0596 - mae: 1.0596 - mse: 1.8966\n",
      "Epoch 32: val_loss did not improve from 1.53742\n",
      "50/50 [==============================] - 26s 514ms/step - loss: 1.0596 - mae: 1.0596 - mse: 1.8966 - val_loss: 3.7809 - val_mae: 3.7809 - val_mse: 17.1481\n",
      "Epoch 33/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.9484 - mae: 0.9484 - mse: 1.5017\n",
      "Epoch 33: val_loss did not improve from 1.53742\n",
      "50/50 [==============================] - 25s 495ms/step - loss: 0.9484 - mae: 0.9484 - mse: 1.5017 - val_loss: 7.9001 - val_mae: 7.9001 - val_mse: 73.4129\n",
      "Epoch 34/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.9364 - mae: 0.9364 - mse: 1.4489\n",
      "Epoch 34: val_loss did not improve from 1.53742\n",
      "50/50 [==============================] - 25s 496ms/step - loss: 0.9364 - mae: 0.9364 - mse: 1.4489 - val_loss: 3.2681 - val_mae: 3.2681 - val_mse: 13.9383\n",
      "Epoch 35/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.1201 - mae: 1.1201 - mse: 2.1503\n",
      "Epoch 35: val_loss did not improve from 1.53742\n",
      "50/50 [==============================] - 26s 514ms/step - loss: 1.1201 - mae: 1.1201 - mse: 2.1503 - val_loss: 8.9740 - val_mae: 8.9740 - val_mse: 91.0037\n",
      "Epoch 36/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8060 - mae: 0.8060 - mse: 1.1125\n",
      "Epoch 36: val_loss did not improve from 1.53742\n",
      "50/50 [==============================] - 25s 503ms/step - loss: 0.8060 - mae: 0.8060 - mse: 1.1125 - val_loss: 5.2020 - val_mae: 5.2020 - val_mse: 31.6544\n",
      "Epoch 37/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.0284 - mae: 1.0284 - mse: 1.8769\n",
      "Epoch 37: val_loss did not improve from 1.53742\n",
      "50/50 [==============================] - 26s 522ms/step - loss: 1.0284 - mae: 1.0284 - mse: 1.8769 - val_loss: 3.9867 - val_mae: 3.9867 - val_mse: 21.2120\n",
      "Epoch 38/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8859 - mae: 0.8859 - mse: 1.3643\n",
      "Epoch 38: val_loss did not improve from 1.53742\n",
      "50/50 [==============================] - 27s 545ms/step - loss: 0.8859 - mae: 0.8859 - mse: 1.3643 - val_loss: 2.1070 - val_mae: 2.1070 - val_mse: 7.2652\n",
      "Epoch 39/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7833 - mae: 0.7833 - mse: 1.0191\n",
      "Epoch 39: val_loss did not improve from 1.53742\n",
      "50/50 [==============================] - 26s 529ms/step - loss: 0.7833 - mae: 0.7833 - mse: 1.0191 - val_loss: 3.4874 - val_mae: 3.4874 - val_mse: 16.2783\n",
      "Epoch 40/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7908 - mae: 0.7908 - mse: 1.0249\n",
      "Epoch 40: val_loss improved from 1.53742 to 1.20077, saving model to checkpoint-50-epochs-16-batchs.h5\n",
      "50/50 [==============================] - 28s 558ms/step - loss: 0.7908 - mae: 0.7908 - mse: 1.0249 - val_loss: 1.2008 - val_mae: 1.2008 - val_mse: 2.6095\n",
      "Epoch 41/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8823 - mae: 0.8823 - mse: 1.3458\n",
      "Epoch 41: val_loss did not improve from 1.20077\n",
      "50/50 [==============================] - 24s 488ms/step - loss: 0.8823 - mae: 0.8823 - mse: 1.3458 - val_loss: 7.2988 - val_mae: 7.2988 - val_mse: 67.0717\n",
      "Epoch 42/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8248 - mae: 0.8248 - mse: 1.1886\n",
      "Epoch 42: val_loss did not improve from 1.20077\n",
      "50/50 [==============================] - 25s 497ms/step - loss: 0.8248 - mae: 0.8248 - mse: 1.1886 - val_loss: 1.9649 - val_mae: 1.9649 - val_mse: 5.8236\n",
      "Epoch 43/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7774 - mae: 0.7774 - mse: 1.0280\n",
      "Epoch 43: val_loss did not improve from 1.20077\n",
      "50/50 [==============================] - 25s 507ms/step - loss: 0.7774 - mae: 0.7774 - mse: 1.0280 - val_loss: 1.8854 - val_mae: 1.8854 - val_mse: 5.2248\n",
      "Epoch 44/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.9807 - mae: 0.9807 - mse: 1.6155\n",
      "Epoch 44: val_loss did not improve from 1.20077\n",
      "50/50 [==============================] - 25s 500ms/step - loss: 0.9807 - mae: 0.9807 - mse: 1.6155 - val_loss: 2.7829 - val_mae: 2.7829 - val_mse: 12.5847\n",
      "Epoch 45/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8376 - mae: 0.8376 - mse: 1.1511\n",
      "Epoch 45: val_loss did not improve from 1.20077\n",
      "50/50 [==============================] - 25s 499ms/step - loss: 0.8376 - mae: 0.8376 - mse: 1.1511 - val_loss: 6.7843 - val_mae: 6.7843 - val_mse: 53.9527\n",
      "Epoch 46/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8572 - mae: 0.8572 - mse: 1.3078\n",
      "Epoch 46: val_loss did not improve from 1.20077\n",
      "50/50 [==============================] - 26s 513ms/step - loss: 0.8572 - mae: 0.8572 - mse: 1.3078 - val_loss: 2.5708 - val_mae: 2.5708 - val_mse: 10.4857\n",
      "Epoch 47/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8155 - mae: 0.8155 - mse: 1.1193\n",
      "Epoch 47: val_loss did not improve from 1.20077\n",
      "50/50 [==============================] - 26s 521ms/step - loss: 0.8155 - mae: 0.8155 - mse: 1.1193 - val_loss: 1.2841 - val_mae: 1.2841 - val_mse: 2.8503\n",
      "Epoch 48/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.9454 - mae: 0.9454 - mse: 1.6103\n",
      "Epoch 48: val_loss did not improve from 1.20077\n",
      "50/50 [==============================] - 24s 482ms/step - loss: 0.9454 - mae: 0.9454 - mse: 1.6103 - val_loss: 1.6094 - val_mae: 1.6094 - val_mse: 4.3980\n",
      "Epoch 49/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.9315 - mae: 0.9315 - mse: 1.5794\n",
      "Epoch 49: val_loss did not improve from 1.20077\n",
      "50/50 [==============================] - 25s 497ms/step - loss: 0.9315 - mae: 0.9315 - mse: 1.5794 - val_loss: 1.3312 - val_mae: 1.3312 - val_mse: 3.1515\n",
      "Epoch 50/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7542 - mae: 0.7542 - mse: 0.9538\n",
      "Epoch 50: val_loss did not improve from 1.20077\n",
      "50/50 [==============================] - 24s 484ms/step - loss: 0.7542 - mae: 0.7542 - mse: 0.9538 - val_loss: 1.8344 - val_mae: 1.8344 - val_mse: 5.0140\n",
      "Epoch 51/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7705 - mae: 0.7705 - mse: 1.0241\n",
      "Epoch 51: val_loss did not improve from 1.20077\n",
      "50/50 [==============================] - 24s 484ms/step - loss: 0.7705 - mae: 0.7705 - mse: 1.0241 - val_loss: 6.6442 - val_mae: 6.6442 - val_mse: 51.4613\n",
      "Epoch 52/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6869 - mae: 0.6869 - mse: 0.7749\n",
      "Epoch 52: val_loss did not improve from 1.20077\n",
      "50/50 [==============================] - 25s 497ms/step - loss: 0.6869 - mae: 0.6869 - mse: 0.7749 - val_loss: 4.6850 - val_mae: 4.6850 - val_mse: 27.1084\n",
      "Epoch 53/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7451 - mae: 0.7451 - mse: 0.9073\n",
      "Epoch 53: val_loss did not improve from 1.20077\n",
      "50/50 [==============================] - 25s 496ms/step - loss: 0.7451 - mae: 0.7451 - mse: 0.9073 - val_loss: 1.9656 - val_mae: 1.9656 - val_mse: 5.9288\n",
      "Epoch 54/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7546 - mae: 0.7546 - mse: 0.9506\n",
      "Epoch 54: val_loss did not improve from 1.20077\n",
      "50/50 [==============================] - 25s 495ms/step - loss: 0.7546 - mae: 0.7546 - mse: 0.9506 - val_loss: 1.7897 - val_mae: 1.7897 - val_mse: 5.4425\n",
      "Epoch 55/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8249 - mae: 0.8249 - mse: 1.1452\n",
      "Epoch 55: val_loss did not improve from 1.20077\n",
      "50/50 [==============================] - 25s 510ms/step - loss: 0.8249 - mae: 0.8249 - mse: 1.1452 - val_loss: 6.0561 - val_mae: 6.0561 - val_mse: 43.3619\n",
      "Epoch 56/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7224 - mae: 0.7224 - mse: 0.8913\n",
      "Epoch 56: val_loss did not improve from 1.20077\n",
      "50/50 [==============================] - 24s 488ms/step - loss: 0.7224 - mae: 0.7224 - mse: 0.8913 - val_loss: 2.9628 - val_mae: 2.9628 - val_mse: 11.2157\n",
      "Epoch 57/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.1004 - mae: 1.1004 - mse: 2.0742\n",
      "Epoch 57: val_loss did not improve from 1.20077\n",
      "50/50 [==============================] - 26s 523ms/step - loss: 1.1004 - mae: 1.1004 - mse: 2.0742 - val_loss: 11.9647 - val_mae: 11.9647 - val_mse: 171.9141\n",
      "Epoch 58/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7989 - mae: 0.7989 - mse: 1.0154\n",
      "Epoch 58: val_loss did not improve from 1.20077\n",
      "50/50 [==============================] - 25s 496ms/step - loss: 0.7989 - mae: 0.7989 - mse: 1.0154 - val_loss: 12.2111 - val_mae: 12.2111 - val_mse: 179.1992\n",
      "Epoch 59/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7424 - mae: 0.7424 - mse: 0.9603\n",
      "Epoch 59: val_loss did not improve from 1.20077\n",
      "50/50 [==============================] - 24s 477ms/step - loss: 0.7424 - mae: 0.7424 - mse: 0.9603 - val_loss: 6.1673 - val_mae: 6.1673 - val_mse: 43.2161\n",
      "Epoch 60/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7066 - mae: 0.7066 - mse: 0.8316\n",
      "Epoch 60: val_loss did not improve from 1.20077\n",
      "50/50 [==============================] - 24s 484ms/step - loss: 0.7066 - mae: 0.7066 - mse: 0.8316 - val_loss: 6.3280 - val_mae: 6.3280 - val_mse: 47.6940\n",
      "Epoch 61/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6613 - mae: 0.6613 - mse: 0.7404\n",
      "Epoch 61: val_loss did not improve from 1.20077\n",
      "50/50 [==============================] - 25s 496ms/step - loss: 0.6613 - mae: 0.6613 - mse: 0.7404 - val_loss: 1.9461 - val_mae: 1.9461 - val_mse: 5.4893\n",
      "Epoch 62/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8286 - mae: 0.8286 - mse: 1.1513\n",
      "Epoch 62: val_loss did not improve from 1.20077\n",
      "50/50 [==============================] - 24s 489ms/step - loss: 0.8286 - mae: 0.8286 - mse: 1.1513 - val_loss: 2.2122 - val_mae: 2.2122 - val_mse: 6.9644\n",
      "Epoch 63/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7832 - mae: 0.7832 - mse: 1.0677\n",
      "Epoch 63: val_loss did not improve from 1.20077\n",
      "50/50 [==============================] - 25s 503ms/step - loss: 0.7832 - mae: 0.7832 - mse: 1.0677 - val_loss: 2.1141 - val_mae: 2.1141 - val_mse: 6.4410\n",
      "Epoch 64/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6215 - mae: 0.6215 - mse: 0.6451\n",
      "Epoch 64: val_loss did not improve from 1.20077\n",
      "50/50 [==============================] - 25s 496ms/step - loss: 0.6215 - mae: 0.6215 - mse: 0.6451 - val_loss: 1.8978 - val_mae: 1.8978 - val_mse: 5.3689\n",
      "Epoch 65/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6941 - mae: 0.6941 - mse: 0.8253\n",
      "Epoch 65: val_loss improved from 1.20077 to 1.18741, saving model to checkpoint-50-epochs-16-batchs.h5\n",
      "50/50 [==============================] - 27s 542ms/step - loss: 0.6941 - mae: 0.6941 - mse: 0.8253 - val_loss: 1.1874 - val_mae: 1.1874 - val_mse: 2.5554\n",
      "Epoch 66/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7444 - mae: 0.7444 - mse: 0.9607\n",
      "Epoch 66: val_loss did not improve from 1.18741\n",
      "50/50 [==============================] - 26s 510ms/step - loss: 0.7444 - mae: 0.7444 - mse: 0.9607 - val_loss: 3.8025 - val_mae: 3.8025 - val_mse: 17.0089\n",
      "Epoch 67/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7174 - mae: 0.7174 - mse: 0.9234\n",
      "Epoch 67: val_loss did not improve from 1.18741\n",
      "50/50 [==============================] - 25s 500ms/step - loss: 0.7174 - mae: 0.7174 - mse: 0.9234 - val_loss: 2.2878 - val_mae: 2.2878 - val_mse: 8.3693\n",
      "Epoch 68/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6469 - mae: 0.6469 - mse: 0.7032\n",
      "Epoch 68: val_loss did not improve from 1.18741\n",
      "50/50 [==============================] - 25s 496ms/step - loss: 0.6469 - mae: 0.6469 - mse: 0.7032 - val_loss: 1.8911 - val_mae: 1.8911 - val_mse: 5.3611\n",
      "Epoch 69/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8108 - mae: 0.8108 - mse: 1.1429\n",
      "Epoch 69: val_loss did not improve from 1.18741\n",
      "50/50 [==============================] - 25s 505ms/step - loss: 0.8108 - mae: 0.8108 - mse: 1.1429 - val_loss: 6.8681 - val_mae: 6.8681 - val_mse: 52.9115\n",
      "Epoch 70/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7788 - mae: 0.7788 - mse: 1.0287\n",
      "Epoch 70: val_loss did not improve from 1.18741\n",
      "50/50 [==============================] - 25s 497ms/step - loss: 0.7788 - mae: 0.7788 - mse: 1.0287 - val_loss: 1.7579 - val_mae: 1.7579 - val_mse: 4.6702\n",
      "Epoch 71/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6406 - mae: 0.6406 - mse: 0.7388\n",
      "Epoch 71: val_loss did not improve from 1.18741\n",
      "50/50 [==============================] - 25s 492ms/step - loss: 0.6406 - mae: 0.6406 - mse: 0.7388 - val_loss: 4.7284 - val_mae: 4.7284 - val_mse: 25.2834\n",
      "Epoch 72/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6627 - mae: 0.6627 - mse: 0.7140\n",
      "Epoch 72: val_loss did not improve from 1.18741\n",
      "50/50 [==============================] - 25s 500ms/step - loss: 0.6627 - mae: 0.6627 - mse: 0.7140 - val_loss: 4.3897 - val_mae: 4.3897 - val_mse: 23.7299\n",
      "Epoch 73/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7641 - mae: 0.7641 - mse: 0.9811\n",
      "Epoch 73: val_loss did not improve from 1.18741\n",
      "50/50 [==============================] - 25s 499ms/step - loss: 0.7641 - mae: 0.7641 - mse: 0.9811 - val_loss: 4.0392 - val_mae: 4.0392 - val_mse: 22.9954\n",
      "Epoch 74/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7413 - mae: 0.7413 - mse: 0.9463\n",
      "Epoch 74: val_loss did not improve from 1.18741\n",
      "50/50 [==============================] - 25s 499ms/step - loss: 0.7413 - mae: 0.7413 - mse: 0.9463 - val_loss: 1.3489 - val_mae: 1.3489 - val_mse: 2.9041\n",
      "Epoch 75/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6630 - mae: 0.6630 - mse: 0.7565\n",
      "Epoch 75: val_loss did not improve from 1.18741\n",
      "50/50 [==============================] - 25s 500ms/step - loss: 0.6630 - mae: 0.6630 - mse: 0.7565 - val_loss: 1.4678 - val_mae: 1.4678 - val_mse: 3.4951\n",
      "Epoch 76/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6427 - mae: 0.6427 - mse: 0.7190\n",
      "Epoch 76: val_loss did not improve from 1.18741\n",
      "50/50 [==============================] - 24s 487ms/step - loss: 0.6427 - mae: 0.6427 - mse: 0.7190 - val_loss: 1.2194 - val_mae: 1.2194 - val_mse: 2.6110\n",
      "Epoch 77/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7125 - mae: 0.7125 - mse: 0.8169\n",
      "Epoch 77: val_loss did not improve from 1.18741\n",
      "50/50 [==============================] - 24s 488ms/step - loss: 0.7125 - mae: 0.7125 - mse: 0.8169 - val_loss: 1.7964 - val_mae: 1.7964 - val_mse: 4.7900\n",
      "Epoch 78/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7663 - mae: 0.7663 - mse: 0.9852\n",
      "Epoch 78: val_loss did not improve from 1.18741\n",
      "50/50 [==============================] - 27s 536ms/step - loss: 0.7663 - mae: 0.7663 - mse: 0.9852 - val_loss: 1.4838 - val_mae: 1.4838 - val_mse: 3.3725\n",
      "Epoch 79/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6550 - mae: 0.6550 - mse: 0.7074\n",
      "Epoch 79: val_loss did not improve from 1.18741\n",
      "50/50 [==============================] - 26s 522ms/step - loss: 0.6550 - mae: 0.6550 - mse: 0.7074 - val_loss: 2.3406 - val_mae: 2.3406 - val_mse: 7.4074\n",
      "Epoch 80/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7990 - mae: 0.7990 - mse: 1.0258\n",
      "Epoch 80: val_loss did not improve from 1.18741\n",
      "50/50 [==============================] - 26s 514ms/step - loss: 0.7990 - mae: 0.7990 - mse: 1.0258 - val_loss: 3.6272 - val_mae: 3.6272 - val_mse: 16.7647\n",
      "Epoch 81/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.5181 - mae: 0.5181 - mse: 0.4681\n",
      "Epoch 81: val_loss did not improve from 1.18741\n",
      "50/50 [==============================] - 25s 497ms/step - loss: 0.5181 - mae: 0.5181 - mse: 0.4681 - val_loss: 1.4386 - val_mae: 1.4386 - val_mse: 3.3817\n",
      "Epoch 82/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.5391 - mae: 0.5391 - mse: 0.5058\n",
      "Epoch 82: val_loss did not improve from 1.18741\n",
      "50/50 [==============================] - 24s 486ms/step - loss: 0.5391 - mae: 0.5391 - mse: 0.5058 - val_loss: 1.2228 - val_mae: 1.2228 - val_mse: 2.5643\n",
      "Epoch 83/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.5883 - mae: 0.5883 - mse: 0.5840\n",
      "Epoch 83: val_loss did not improve from 1.18741\n",
      "50/50 [==============================] - 25s 496ms/step - loss: 0.5883 - mae: 0.5883 - mse: 0.5840 - val_loss: 1.4403 - val_mae: 1.4403 - val_mse: 3.4295\n",
      "Epoch 84/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.5747 - mae: 0.5747 - mse: 0.5285\n",
      "Epoch 84: val_loss did not improve from 1.18741\n",
      "50/50 [==============================] - 24s 487ms/step - loss: 0.5747 - mae: 0.5747 - mse: 0.5285 - val_loss: 2.3599 - val_mae: 2.3599 - val_mse: 7.3332\n",
      "Epoch 85/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.5796 - mae: 0.5796 - mse: 0.5445\n",
      "Epoch 85: val_loss improved from 1.18741 to 1.13059, saving model to checkpoint-50-epochs-16-batchs.h5\n",
      "50/50 [==============================] - 27s 543ms/step - loss: 0.5796 - mae: 0.5796 - mse: 0.5445 - val_loss: 1.1306 - val_mae: 1.1306 - val_mse: 2.3605\n",
      "Epoch 86/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.5273 - mae: 0.5273 - mse: 0.4665\n",
      "Epoch 86: val_loss did not improve from 1.13059\n",
      "50/50 [==============================] - 25s 502ms/step - loss: 0.5273 - mae: 0.5273 - mse: 0.4665 - val_loss: 3.1090 - val_mae: 3.1090 - val_mse: 13.6440\n",
      "Epoch 87/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7221 - mae: 0.7221 - mse: 0.9944\n",
      "Epoch 87: val_loss did not improve from 1.13059\n",
      "50/50 [==============================] - 24s 490ms/step - loss: 0.7221 - mae: 0.7221 - mse: 0.9944 - val_loss: 3.5054 - val_mae: 3.5054 - val_mse: 16.5006\n",
      "Epoch 88/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6055 - mae: 0.6055 - mse: 0.6011\n",
      "Epoch 88: val_loss did not improve from 1.13059\n",
      "50/50 [==============================] - 24s 484ms/step - loss: 0.6055 - mae: 0.6055 - mse: 0.6011 - val_loss: 2.5001 - val_mae: 2.5001 - val_mse: 9.3414\n",
      "Epoch 89/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.5648 - mae: 0.5648 - mse: 0.5460\n",
      "Epoch 89: val_loss did not improve from 1.13059\n",
      "50/50 [==============================] - 25s 499ms/step - loss: 0.5648 - mae: 0.5648 - mse: 0.5460 - val_loss: 1.5499 - val_mae: 1.5499 - val_mse: 3.6752\n",
      "Epoch 90/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.5327 - mae: 0.5327 - mse: 0.4804\n",
      "Epoch 90: val_loss did not improve from 1.13059\n",
      "50/50 [==============================] - 25s 497ms/step - loss: 0.5327 - mae: 0.5327 - mse: 0.4804 - val_loss: 1.2586 - val_mae: 1.2586 - val_mse: 2.5900\n",
      "Epoch 91/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4926 - mae: 0.4926 - mse: 0.3990\n",
      "Epoch 91: val_loss did not improve from 1.13059\n",
      "50/50 [==============================] - 25s 495ms/step - loss: 0.4926 - mae: 0.4926 - mse: 0.3990 - val_loss: 2.3670 - val_mae: 2.3670 - val_mse: 7.7616\n",
      "Epoch 92/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4625 - mae: 0.4625 - mse: 0.3656\n",
      "Epoch 92: val_loss did not improve from 1.13059\n",
      "50/50 [==============================] - 25s 509ms/step - loss: 0.4625 - mae: 0.4625 - mse: 0.3656 - val_loss: 1.2838 - val_mae: 1.2838 - val_mse: 2.6527\n",
      "Epoch 93/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4840 - mae: 0.4840 - mse: 0.3928\n",
      "Epoch 93: val_loss did not improve from 1.13059\n",
      "50/50 [==============================] - 25s 497ms/step - loss: 0.4840 - mae: 0.4840 - mse: 0.3928 - val_loss: 3.6453 - val_mae: 3.6453 - val_mse: 17.7435\n",
      "Epoch 94/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6182 - mae: 0.6182 - mse: 0.6466\n",
      "Epoch 94: val_loss did not improve from 1.13059\n",
      "50/50 [==============================] - 24s 487ms/step - loss: 0.6182 - mae: 0.6182 - mse: 0.6466 - val_loss: 1.4956 - val_mae: 1.4956 - val_mse: 3.5981\n",
      "Epoch 95/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.5025 - mae: 0.5025 - mse: 0.4178\n",
      "Epoch 95: val_loss did not improve from 1.13059\n",
      "50/50 [==============================] - 25s 497ms/step - loss: 0.5025 - mae: 0.5025 - mse: 0.4178 - val_loss: 1.6828 - val_mae: 1.6828 - val_mse: 4.2180\n",
      "Epoch 96/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4629 - mae: 0.4629 - mse: 0.3558\n",
      "Epoch 96: val_loss did not improve from 1.13059\n",
      "50/50 [==============================] - 24s 485ms/step - loss: 0.4629 - mae: 0.4629 - mse: 0.3558 - val_loss: 2.5754 - val_mae: 2.5754 - val_mse: 9.0576\n",
      "Epoch 97/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4928 - mae: 0.4928 - mse: 0.4160\n",
      "Epoch 97: val_loss did not improve from 1.13059\n",
      "50/50 [==============================] - 24s 488ms/step - loss: 0.4928 - mae: 0.4928 - mse: 0.4160 - val_loss: 1.2259 - val_mae: 1.2259 - val_mse: 2.5057\n",
      "Epoch 98/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4079 - mae: 0.4079 - mse: 0.2821\n",
      "Epoch 98: val_loss did not improve from 1.13059\n",
      "50/50 [==============================] - 25s 504ms/step - loss: 0.4079 - mae: 0.4079 - mse: 0.2821 - val_loss: 2.0040 - val_mae: 2.0040 - val_mse: 5.6838\n",
      "Epoch 99/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.5361 - mae: 0.5361 - mse: 0.4764\n",
      "Epoch 99: val_loss did not improve from 1.13059\n",
      "50/50 [==============================] - 25s 491ms/step - loss: 0.5361 - mae: 0.5361 - mse: 0.4764 - val_loss: 2.3345 - val_mae: 2.3345 - val_mse: 7.3715\n",
      "Epoch 100/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4651 - mae: 0.4651 - mse: 0.3610\n",
      "Epoch 100: val_loss did not improve from 1.13059\n",
      "50/50 [==============================] - 24s 485ms/step - loss: 0.4651 - mae: 0.4651 - mse: 0.3610 - val_loss: 1.5081 - val_mae: 1.5081 - val_mse: 3.3748\n",
      "Epoch 101/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4218 - mae: 0.4218 - mse: 0.2863\n",
      "Epoch 101: val_loss did not improve from 1.13059\n",
      "50/50 [==============================] - 25s 496ms/step - loss: 0.4218 - mae: 0.4218 - mse: 0.2863 - val_loss: 3.4124 - val_mae: 3.4124 - val_mse: 14.5352\n",
      "Epoch 102/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4650 - mae: 0.4650 - mse: 0.3419\n",
      "Epoch 102: val_loss did not improve from 1.13059\n",
      "50/50 [==============================] - 26s 513ms/step - loss: 0.4650 - mae: 0.4650 - mse: 0.3419 - val_loss: 1.6437 - val_mae: 1.6437 - val_mse: 4.0235\n",
      "Epoch 103/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4766 - mae: 0.4766 - mse: 0.3530\n",
      "Epoch 103: val_loss did not improve from 1.13059\n",
      "50/50 [==============================] - 26s 526ms/step - loss: 0.4766 - mae: 0.4766 - mse: 0.3530 - val_loss: 2.0767 - val_mae: 2.0767 - val_mse: 6.0738\n",
      "Epoch 104/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4724 - mae: 0.4724 - mse: 0.3626\n",
      "Epoch 104: val_loss did not improve from 1.13059\n",
      "50/50 [==============================] - 26s 521ms/step - loss: 0.4724 - mae: 0.4724 - mse: 0.3626 - val_loss: 2.3445 - val_mae: 2.3445 - val_mse: 7.1676\n",
      "Epoch 105/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.5719 - mae: 0.5719 - mse: 0.5518\n",
      "Epoch 105: val_loss did not improve from 1.13059\n",
      "50/50 [==============================] - 25s 493ms/step - loss: 0.5719 - mae: 0.5719 - mse: 0.5518 - val_loss: 1.6853 - val_mae: 1.6853 - val_mse: 4.3811\n",
      "Epoch 106/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4367 - mae: 0.4367 - mse: 0.3144\n",
      "Epoch 106: val_loss improved from 1.13059 to 1.11073, saving model to checkpoint-50-epochs-16-batchs.h5\n",
      "50/50 [==============================] - 27s 550ms/step - loss: 0.4367 - mae: 0.4367 - mse: 0.3144 - val_loss: 1.1107 - val_mae: 1.1107 - val_mse: 2.2953\n",
      "Epoch 107/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4681 - mae: 0.4681 - mse: 0.3469\n",
      "Epoch 107: val_loss did not improve from 1.11073\n",
      "50/50 [==============================] - 24s 489ms/step - loss: 0.4681 - mae: 0.4681 - mse: 0.3469 - val_loss: 1.8759 - val_mae: 1.8759 - val_mse: 4.9934\n",
      "Epoch 108/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6413 - mae: 0.6413 - mse: 0.6839\n",
      "Epoch 108: val_loss did not improve from 1.11073\n",
      "50/50 [==============================] - 24s 488ms/step - loss: 0.6413 - mae: 0.6413 - mse: 0.6839 - val_loss: 1.1698 - val_mae: 1.1698 - val_mse: 2.3797\n",
      "Epoch 109/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4509 - mae: 0.4509 - mse: 0.3178\n",
      "Epoch 109: val_loss did not improve from 1.11073\n",
      "50/50 [==============================] - 25s 502ms/step - loss: 0.4509 - mae: 0.4509 - mse: 0.3178 - val_loss: 1.6796 - val_mae: 1.6796 - val_mse: 4.0668\n",
      "Epoch 110/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4960 - mae: 0.4960 - mse: 0.3919\n",
      "Epoch 110: val_loss did not improve from 1.11073\n",
      "50/50 [==============================] - 25s 491ms/step - loss: 0.4960 - mae: 0.4960 - mse: 0.3919 - val_loss: 1.2387 - val_mae: 1.2387 - val_mse: 2.4845\n",
      "Epoch 111/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.3865 - mae: 0.3865 - mse: 0.2445\n",
      "Epoch 111: val_loss did not improve from 1.11073\n",
      "50/50 [==============================] - 24s 485ms/step - loss: 0.3865 - mae: 0.3865 - mse: 0.2445 - val_loss: 1.3760 - val_mae: 1.3760 - val_mse: 2.9150\n",
      "Epoch 112/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4197 - mae: 0.4197 - mse: 0.2920\n",
      "Epoch 112: val_loss did not improve from 1.11073\n",
      "50/50 [==============================] - 25s 497ms/step - loss: 0.4197 - mae: 0.4197 - mse: 0.2920 - val_loss: 1.5251 - val_mae: 1.5251 - val_mse: 3.5590\n",
      "Epoch 113/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4249 - mae: 0.4249 - mse: 0.3146\n",
      "Epoch 113: val_loss did not improve from 1.11073\n",
      "50/50 [==============================] - 24s 486ms/step - loss: 0.4249 - mae: 0.4249 - mse: 0.3146 - val_loss: 1.5763 - val_mae: 1.5763 - val_mse: 3.8073\n",
      "Epoch 114/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4690 - mae: 0.4690 - mse: 0.3604\n",
      "Epoch 114: val_loss did not improve from 1.11073\n",
      "50/50 [==============================] - 24s 487ms/step - loss: 0.4690 - mae: 0.4690 - mse: 0.3604 - val_loss: 1.6866 - val_mae: 1.6866 - val_mse: 4.1499\n",
      "Epoch 115/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4461 - mae: 0.4461 - mse: 0.3550\n",
      "Epoch 115: val_loss did not improve from 1.11073\n",
      "50/50 [==============================] - 25s 502ms/step - loss: 0.4461 - mae: 0.4461 - mse: 0.3550 - val_loss: 1.4781 - val_mae: 1.4781 - val_mse: 3.3015\n",
      "Epoch 116/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.3949 - mae: 0.3949 - mse: 0.2475\n",
      "Epoch 116: val_loss did not improve from 1.11073\n",
      "50/50 [==============================] - 24s 489ms/step - loss: 0.3949 - mae: 0.3949 - mse: 0.2475 - val_loss: 1.1483 - val_mae: 1.1483 - val_mse: 2.2559\n",
      "Epoch 117/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.3933 - mae: 0.3933 - mse: 0.2554\n",
      "Epoch 117: val_loss did not improve from 1.11073\n",
      "50/50 [==============================] - 26s 520ms/step - loss: 0.3933 - mae: 0.3933 - mse: 0.2554 - val_loss: 1.5370 - val_mae: 1.5370 - val_mse: 3.5124\n",
      "Epoch 118/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.3430 - mae: 0.3430 - mse: 0.1961\n",
      "Epoch 118: val_loss did not improve from 1.11073\n",
      "50/50 [==============================] - 27s 537ms/step - loss: 0.3430 - mae: 0.3430 - mse: 0.1961 - val_loss: 1.1278 - val_mae: 1.1278 - val_mse: 2.1994\n",
      "Epoch 119/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4353 - mae: 0.4353 - mse: 0.3294\n",
      "Epoch 119: val_loss did not improve from 1.11073\n",
      "50/50 [==============================] - 25s 497ms/step - loss: 0.4353 - mae: 0.4353 - mse: 0.3294 - val_loss: 2.1290 - val_mae: 2.1290 - val_mse: 6.3923\n",
      "Epoch 120/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4682 - mae: 0.4682 - mse: 0.3680\n",
      "Epoch 120: val_loss did not improve from 1.11073\n",
      "50/50 [==============================] - 25s 504ms/step - loss: 0.4682 - mae: 0.4682 - mse: 0.3680 - val_loss: 1.4000 - val_mae: 1.4000 - val_mse: 3.0345\n",
      "Epoch 121/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4318 - mae: 0.4318 - mse: 0.3002\n",
      "Epoch 121: val_loss did not improve from 1.11073\n",
      "50/50 [==============================] - 25s 498ms/step - loss: 0.4318 - mae: 0.4318 - mse: 0.3002 - val_loss: 3.6472 - val_mae: 3.6472 - val_mse: 15.8374\n",
      "Epoch 122/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4211 - mae: 0.4211 - mse: 0.2729\n",
      "Epoch 122: val_loss improved from 1.11073 to 1.09702, saving model to checkpoint-50-epochs-16-batchs.h5\n",
      "50/50 [==============================] - 27s 540ms/step - loss: 0.4211 - mae: 0.4211 - mse: 0.2729 - val_loss: 1.0970 - val_mae: 1.0970 - val_mse: 2.1919\n",
      "Epoch 123/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4056 - mae: 0.4056 - mse: 0.2757\n",
      "Epoch 123: val_loss did not improve from 1.09702\n",
      "50/50 [==============================] - 25s 504ms/step - loss: 0.4056 - mae: 0.4056 - mse: 0.2757 - val_loss: 2.1456 - val_mae: 2.1456 - val_mse: 6.4120\n",
      "Epoch 124/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.3777 - mae: 0.3777 - mse: 0.2361\n",
      "Epoch 124: val_loss did not improve from 1.09702\n",
      "50/50 [==============================] - 25s 510ms/step - loss: 0.3777 - mae: 0.3777 - mse: 0.2361 - val_loss: 1.5894 - val_mae: 1.5894 - val_mse: 3.7658\n",
      "Epoch 125/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.3664 - mae: 0.3664 - mse: 0.2178\n",
      "Epoch 125: val_loss did not improve from 1.09702\n",
      "50/50 [==============================] - 24s 484ms/step - loss: 0.3664 - mae: 0.3664 - mse: 0.2178 - val_loss: 1.6541 - val_mae: 1.6541 - val_mse: 4.0423\n",
      "Epoch 126/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4732 - mae: 0.4732 - mse: 0.3668\n",
      "Epoch 126: val_loss improved from 1.09702 to 1.07551, saving model to checkpoint-50-epochs-16-batchs.h5\n",
      "50/50 [==============================] - 27s 543ms/step - loss: 0.4732 - mae: 0.4732 - mse: 0.3668 - val_loss: 1.0755 - val_mae: 1.0755 - val_mse: 2.1255\n",
      "Epoch 127/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4014 - mae: 0.4014 - mse: 0.2693\n",
      "Epoch 127: val_loss did not improve from 1.07551\n",
      "50/50 [==============================] - 25s 497ms/step - loss: 0.4014 - mae: 0.4014 - mse: 0.2693 - val_loss: 1.4566 - val_mae: 1.4566 - val_mse: 3.3868\n",
      "Epoch 128/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.3683 - mae: 0.3683 - mse: 0.2190\n",
      "Epoch 128: val_loss did not improve from 1.07551\n",
      "50/50 [==============================] - 25s 494ms/step - loss: 0.3683 - mae: 0.3683 - mse: 0.2190 - val_loss: 2.3610 - val_mae: 2.3610 - val_mse: 7.4574\n",
      "Epoch 129/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.3771 - mae: 0.3771 - mse: 0.2266\n",
      "Epoch 129: val_loss did not improve from 1.07551\n",
      "50/50 [==============================] - 25s 506ms/step - loss: 0.3771 - mae: 0.3771 - mse: 0.2266 - val_loss: 1.1963 - val_mae: 1.1963 - val_mse: 2.4009\n",
      "Epoch 130/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.3522 - mae: 0.3522 - mse: 0.2087\n",
      "Epoch 130: val_loss did not improve from 1.07551\n",
      "50/50 [==============================] - 25s 495ms/step - loss: 0.3522 - mae: 0.3522 - mse: 0.2087 - val_loss: 1.2839 - val_mae: 1.2839 - val_mse: 2.8014\n",
      "Epoch 131/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.3673 - mae: 0.3673 - mse: 0.2228\n",
      "Epoch 131: val_loss did not improve from 1.07551\n",
      "50/50 [==============================] - 24s 490ms/step - loss: 0.3673 - mae: 0.3673 - mse: 0.2228 - val_loss: 1.0959 - val_mae: 1.0959 - val_mse: 2.1370\n",
      "Epoch 132/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.3838 - mae: 0.3838 - mse: 0.2364\n",
      "Epoch 132: val_loss did not improve from 1.07551\n",
      "50/50 [==============================] - 25s 506ms/step - loss: 0.3838 - mae: 0.3838 - mse: 0.2364 - val_loss: 1.2091 - val_mae: 1.2091 - val_mse: 2.5292\n",
      "Epoch 133/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.3504 - mae: 0.3504 - mse: 0.1945\n",
      "Epoch 133: val_loss did not improve from 1.07551\n",
      "50/50 [==============================] - 25s 493ms/step - loss: 0.3504 - mae: 0.3504 - mse: 0.1945 - val_loss: 1.0939 - val_mae: 1.0939 - val_mse: 2.3146\n",
      "Epoch 134/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.3379 - mae: 0.3379 - mse: 0.1964\n",
      "Epoch 134: val_loss did not improve from 1.07551\n",
      "50/50 [==============================] - 25s 505ms/step - loss: 0.3379 - mae: 0.3379 - mse: 0.1964 - val_loss: 1.2516 - val_mae: 1.2516 - val_mse: 2.5206\n",
      "Epoch 135/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.3919 - mae: 0.3919 - mse: 0.2699\n",
      "Epoch 135: val_loss did not improve from 1.07551\n",
      "50/50 [==============================] - 25s 494ms/step - loss: 0.3919 - mae: 0.3919 - mse: 0.2699 - val_loss: 1.3678 - val_mae: 1.3678 - val_mse: 3.0791\n",
      "Epoch 136/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.3172 - mae: 0.3172 - mse: 0.1673\n",
      "Epoch 136: val_loss did not improve from 1.07551\n",
      "50/50 [==============================] - 24s 488ms/step - loss: 0.3172 - mae: 0.3172 - mse: 0.1673 - val_loss: 1.7171 - val_mae: 1.7171 - val_mse: 4.4100\n",
      "Epoch 137/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.3813 - mae: 0.3813 - mse: 0.2442\n",
      "Epoch 137: val_loss did not improve from 1.07551\n",
      "50/50 [==============================] - 25s 496ms/step - loss: 0.3813 - mae: 0.3813 - mse: 0.2442 - val_loss: 2.0232 - val_mae: 2.0232 - val_mse: 5.8004\n",
      "Epoch 138/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.3171 - mae: 0.3171 - mse: 0.1633\n",
      "Epoch 138: val_loss did not improve from 1.07551\n",
      "50/50 [==============================] - 24s 485ms/step - loss: 0.3171 - mae: 0.3171 - mse: 0.1633 - val_loss: 1.1607 - val_mae: 1.1607 - val_mse: 2.4443\n",
      "Epoch 139/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.3436 - mae: 0.3436 - mse: 0.1877\n",
      "Epoch 139: val_loss did not improve from 1.07551\n",
      "50/50 [==============================] - 24s 484ms/step - loss: 0.3436 - mae: 0.3436 - mse: 0.1877 - val_loss: 1.7989 - val_mae: 1.7989 - val_mse: 4.7209\n",
      "Epoch 140/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.3435 - mae: 0.3435 - mse: 0.1882\n",
      "Epoch 140: val_loss did not improve from 1.07551\n",
      "50/50 [==============================] - 25s 502ms/step - loss: 0.3435 - mae: 0.3435 - mse: 0.1882 - val_loss: 3.1595 - val_mae: 3.1595 - val_mse: 12.7819\n",
      "Epoch 141/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.3665 - mae: 0.3665 - mse: 0.2266\n",
      "Epoch 141: val_loss did not improve from 1.07551\n",
      "50/50 [==============================] - 24s 484ms/step - loss: 0.3665 - mae: 0.3665 - mse: 0.2266 - val_loss: 1.1546 - val_mae: 1.1546 - val_mse: 2.3568\n",
      "Epoch 142/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.3593 - mae: 0.3593 - mse: 0.2089\n",
      "Epoch 142: val_loss did not improve from 1.07551\n",
      "50/50 [==============================] - 24s 485ms/step - loss: 0.3593 - mae: 0.3593 - mse: 0.2089 - val_loss: 1.3282 - val_mae: 1.3282 - val_mse: 2.9790\n",
      "Epoch 143/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.3400 - mae: 0.3400 - mse: 0.1990\n",
      "Epoch 143: val_loss did not improve from 1.07551\n",
      "50/50 [==============================] - 24s 488ms/step - loss: 0.3400 - mae: 0.3400 - mse: 0.1990 - val_loss: 1.7998 - val_mae: 1.7998 - val_mse: 4.6863\n",
      "Epoch 144/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.3693 - mae: 0.3693 - mse: 0.2191\n",
      "Epoch 144: val_loss did not improve from 1.07551\n",
      "50/50 [==============================] - 24s 475ms/step - loss: 0.3693 - mae: 0.3693 - mse: 0.2191 - val_loss: 1.2487 - val_mae: 1.2487 - val_mse: 2.8283\n",
      "Epoch 145/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.3526 - mae: 0.3526 - mse: 0.2068\n",
      "Epoch 145: val_loss did not improve from 1.07551\n",
      "50/50 [==============================] - 24s 479ms/step - loss: 0.3526 - mae: 0.3526 - mse: 0.2068 - val_loss: 1.9404 - val_mae: 1.9404 - val_mse: 5.4495\n",
      "Epoch 146/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.5268 - mae: 0.5268 - mse: 0.4911\n",
      "Epoch 146: val_loss did not improve from 1.07551\n",
      "50/50 [==============================] - 25s 493ms/step - loss: 0.5268 - mae: 0.5268 - mse: 0.4911 - val_loss: 1.1583 - val_mae: 1.1583 - val_mse: 2.3460\n",
      "Epoch 147/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.3764 - mae: 0.3764 - mse: 0.2414\n",
      "Epoch 147: val_loss did not improve from 1.07551\n",
      "50/50 [==============================] - 24s 487ms/step - loss: 0.3764 - mae: 0.3764 - mse: 0.2414 - val_loss: 1.3152 - val_mae: 1.3152 - val_mse: 2.9511\n",
      "Epoch 148/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4208 - mae: 0.4208 - mse: 0.2909\n",
      "Epoch 148: val_loss did not improve from 1.07551\n",
      "50/50 [==============================] - 24s 489ms/step - loss: 0.4208 - mae: 0.4208 - mse: 0.2909 - val_loss: 1.5936 - val_mae: 1.5936 - val_mse: 3.8279\n",
      "Epoch 149/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.3790 - mae: 0.3790 - mse: 0.2386\n",
      "Epoch 149: val_loss did not improve from 1.07551\n",
      "50/50 [==============================] - 25s 499ms/step - loss: 0.3790 - mae: 0.3790 - mse: 0.2386 - val_loss: 1.6205 - val_mae: 1.6205 - val_mse: 3.9567\n",
      "Epoch 150/150\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.3493 - mae: 0.3493 - mse: 0.1963\n",
      "Epoch 150: val_loss did not improve from 1.07551\n",
      "50/50 [==============================] - 24s 489ms/step - loss: 0.3493 - mae: 0.3493 - mse: 0.1963 - val_loss: 1.2317 - val_mae: 1.2317 - val_mse: 2.4709\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f29379b4190>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y, batch_size=16, epochs=150, callbacks=checkpoint, validation_data=(val_x,val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 153ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[14.442653 ],\n",
       "       [ 9.655738 ],\n",
       "       [ 7.687991 ],\n",
       "       [10.086453 ],\n",
       "       [15.9456625],\n",
       "       [ 6.835203 ],\n",
       "       [ 6.3949594],\n",
       "       [ 6.9515557],\n",
       "       [17.991888 ],\n",
       "       [12.890481 ],\n",
       "       [ 7.384145 ],\n",
       "       [17.18728  ],\n",
       "       [ 5.4220695],\n",
       "       [ 8.57199  ],\n",
       "       [13.872296 ],\n",
       "       [ 4.834116 ],\n",
       "       [ 4.787738 ],\n",
       "       [16.535786 ],\n",
       "       [17.534716 ],\n",
       "       [15.488405 ],\n",
       "       [ 9.052772 ],\n",
       "       [ 9.701413 ],\n",
       "       [14.6275425],\n",
       "       [14.170861 ],\n",
       "       [ 9.099381 ],\n",
       "       [13.9533   ],\n",
       "       [ 8.260684 ],\n",
       "       [ 5.3929987],\n",
       "       [11.377481 ],\n",
       "       [12.96967  ],\n",
       "       [ 8.793435 ],\n",
       "       [ 4.715596 ],\n",
       "       [ 7.529928 ],\n",
       "       [12.424624 ],\n",
       "       [ 8.431443 ],\n",
       "       [ 5.912588 ],\n",
       "       [ 5.3194923],\n",
       "       [14.97169  ],\n",
       "       [ 9.906859 ],\n",
       "       [10.652986 ],\n",
       "       [ 6.5545883],\n",
       "       [ 9.898731 ],\n",
       "       [11.924257 ],\n",
       "       [10.00672  ],\n",
       "       [ 5.1624193],\n",
       "       [ 4.102349 ],\n",
       "       [ 3.6151857],\n",
       "       [ 7.682583 ],\n",
       "       [ 6.4047465],\n",
       "       [ 4.6150618],\n",
       "       [14.502048 ],\n",
       "       [ 5.456656 ],\n",
       "       [ 5.603877 ],\n",
       "       [ 4.237384 ],\n",
       "       [16.807886 ],\n",
       "       [ 9.2357235],\n",
       "       [17.514313 ],\n",
       "       [16.015045 ],\n",
       "       [ 5.004095 ],\n",
       "       [16.06441  ],\n",
       "       [ 9.533387 ],\n",
       "       [ 6.1674576],\n",
       "       [ 5.6992393],\n",
       "       [10.902539 ],\n",
       "       [ 8.853208 ],\n",
       "       [ 7.0808973],\n",
       "       [13.427911 ],\n",
       "       [11.878977 ],\n",
       "       [14.961625 ],\n",
       "       [12.651728 ],\n",
       "       [ 8.508492 ],\n",
       "       [ 3.54121  ],\n",
       "       [ 4.98629  ],\n",
       "       [ 8.642401 ],\n",
       "       [10.154111 ],\n",
       "       [10.771943 ],\n",
       "       [ 4.8816   ],\n",
       "       [17.630486 ],\n",
       "       [ 3.6866295],\n",
       "       [16.009304 ],\n",
       "       [ 5.243725 ],\n",
       "       [14.631122 ],\n",
       "       [17.639797 ],\n",
       "       [ 7.510689 ],\n",
       "       [ 8.285964 ],\n",
       "       [10.155665 ],\n",
       "       [11.3727455],\n",
       "       [10.3780775],\n",
       "       [ 4.957121 ],\n",
       "       [ 3.0564797],\n",
       "       [13.553936 ],\n",
       "       [13.963593 ],\n",
       "       [15.966142 ],\n",
       "       [ 6.0047283],\n",
       "       [17.003796 ],\n",
       "       [11.732419 ],\n",
       "       [12.625336 ],\n",
       "       [ 8.766436 ],\n",
       "       [ 7.1918197],\n",
       "       [ 6.1139283],\n",
       "       [12.638368 ],\n",
       "       [ 9.737298 ],\n",
       "       [15.943168 ],\n",
       "       [13.113852 ],\n",
       "       [14.9070215],\n",
       "       [ 3.729565 ],\n",
       "       [12.429816 ],\n",
       "       [14.446986 ],\n",
       "       [12.591581 ],\n",
       "       [13.095146 ],\n",
       "       [ 9.13428  ],\n",
       "       [ 5.250784 ],\n",
       "       [ 9.711743 ],\n",
       "       [ 7.0372887],\n",
       "       [16.253138 ],\n",
       "       [13.555578 ],\n",
       "       [12.0238695],\n",
       "       [ 9.545058 ],\n",
       "       [14.407609 ],\n",
       "       [ 6.412772 ],\n",
       "       [ 5.447932 ],\n",
       "       [ 8.289092 ],\n",
       "       [ 4.014468 ],\n",
       "       [ 6.8687615],\n",
       "       [ 8.986723 ],\n",
       "       [14.98051  ],\n",
       "       [12.801054 ],\n",
       "       [ 7.2314596],\n",
       "       [ 4.885535 ],\n",
       "       [ 4.199683 ],\n",
       "       [16.366909 ],\n",
       "       [12.706137 ],\n",
       "       [ 6.95921  ],\n",
       "       [14.675952 ],\n",
       "       [ 5.758522 ],\n",
       "       [16.086496 ],\n",
       "       [14.773223 ],\n",
       "       [ 7.3688893],\n",
       "       [ 7.439953 ],\n",
       "       [12.892232 ],\n",
       "       [12.548972 ],\n",
       "       [ 8.184762 ],\n",
       "       [ 5.717348 ],\n",
       "       [14.066393 ],\n",
       "       [ 8.683603 ],\n",
       "       [13.375877 ],\n",
       "       [10.924064 ],\n",
       "       [ 5.6987805],\n",
       "       [ 5.5837994],\n",
       "       [ 9.366076 ],\n",
       "       [ 6.346776 ],\n",
       "       [12.92398  ],\n",
       "       [11.623198 ],\n",
       "       [ 6.42912  ],\n",
       "       [ 7.337397 ],\n",
       "       [ 9.514136 ],\n",
       "       [11.404825 ],\n",
       "       [16.866503 ],\n",
       "       [ 6.4263554],\n",
       "       [ 6.9887743],\n",
       "       [ 9.330178 ],\n",
       "       [13.430642 ],\n",
       "       [ 8.238104 ],\n",
       "       [16.85261  ],\n",
       "       [ 6.5434937],\n",
       "       [13.25797  ],\n",
       "       [ 5.6642857],\n",
       "       [17.221273 ],\n",
       "       [12.662316 ],\n",
       "       [ 6.0183144],\n",
       "       [13.195012 ],\n",
       "       [ 9.5034075],\n",
       "       [14.613797 ],\n",
       "       [ 8.4426565],\n",
       "       [ 9.890971 ],\n",
       "       [ 8.5631695],\n",
       "       [ 8.693969 ],\n",
       "       [10.552545 ],\n",
       "       [18.45703  ],\n",
       "       [12.039897 ],\n",
       "       [12.385541 ],\n",
       "       [ 5.673691 ],\n",
       "       [11.455377 ],\n",
       "       [17.556185 ],\n",
       "       [ 7.5520315],\n",
       "       [11.816459 ],\n",
       "       [ 9.594776 ],\n",
       "       [ 7.321208 ],\n",
       "       [15.301569 ],\n",
       "       [ 7.945257 ],\n",
       "       [ 6.1908264],\n",
       "       [13.916076 ],\n",
       "       [13.459174 ],\n",
       "       [16.963816 ],\n",
       "       [ 5.956341 ],\n",
       "       [13.657287 ],\n",
       "       [12.8764   ],\n",
       "       [11.545196 ]], dtype=float32)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_prediction = model.predict(val_x)\n",
    "val_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae : 1.2316612089523162\n",
      "rmse : 1.571903692726792\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "print('mae :', mean_absolute_error(val_prediction, val_y))\n",
    "print('rmse :', np.sqrt(mean_squared_error(val_prediction, val_y)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "junoflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
