{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import zipfile\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cuda\n",
      "Current : 0\n",
      "Count : 1\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "print('device :', device)\n",
    "print('Current :', torch.cuda.current_device())\n",
    "print('Count :', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>등록번호</th>\n",
       "      <th>생년월일</th>\n",
       "      <th>성별</th>\n",
       "      <th>진료의</th>\n",
       "      <th>검사 시 나이</th>\n",
       "      <th>신장</th>\n",
       "      <th>체중</th>\n",
       "      <th>BMI</th>\n",
       "      <th>처방일자</th>\n",
       "      <th>시행일자</th>\n",
       "      <th>BA 1</th>\n",
       "      <th>BA 2</th>\n",
       "      <th>Unnamed: 14</th>\n",
       "      <th>No</th>\n",
       "      <th>boneage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1698</td>\n",
       "      <td>8255049</td>\n",
       "      <td>2007-08-03</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>9.969863</td>\n",
       "      <td>129.5</td>\n",
       "      <td>26.9</td>\n",
       "      <td>16.1</td>\n",
       "      <td>2017-01-09</td>\n",
       "      <td>2017-07-20</td>\n",
       "      <td>9.75</td>\n",
       "      <td>9.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.jpg</td>\n",
       "      <td>9.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1897</td>\n",
       "      <td>8537405</td>\n",
       "      <td>2008-08-22</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>9.989041</td>\n",
       "      <td>132.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>2018-02-28</td>\n",
       "      <td>2018-08-16</td>\n",
       "      <td>10.50</td>\n",
       "      <td>11.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.jpg</td>\n",
       "      <td>10.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1422</td>\n",
       "      <td>7942635</td>\n",
       "      <td>2005-01-19</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10.008219</td>\n",
       "      <td>136.4</td>\n",
       "      <td>33.2</td>\n",
       "      <td>17.9</td>\n",
       "      <td>2015-01-20</td>\n",
       "      <td>2015-01-20</td>\n",
       "      <td>11.00</td>\n",
       "      <td>11.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.jpg</td>\n",
       "      <td>11.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1475</td>\n",
       "      <td>7995857</td>\n",
       "      <td>2005-02-09</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10.049315</td>\n",
       "      <td>133.5</td>\n",
       "      <td>31.2</td>\n",
       "      <td>17.6</td>\n",
       "      <td>2015-02-25</td>\n",
       "      <td>2015-02-25</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.jpg</td>\n",
       "      <td>10.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1888</td>\n",
       "      <td>8520261</td>\n",
       "      <td>2008-09-11</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10.060274</td>\n",
       "      <td>130.6</td>\n",
       "      <td>23.7</td>\n",
       "      <td>13.9</td>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>10.00</td>\n",
       "      <td>9.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.jpg</td>\n",
       "      <td>9.875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Group     등록번호        생년월일 성별 진료의    검사 시 나이     신장    체중   BMI  \\\n",
       "0   1698  8255049  2007-08-03  F   1   9.969863  129.5  26.9  16.1   \n",
       "1   1897  8537405  2008-08-22  F   1   9.989041  132.0  31.0  17.8   \n",
       "2   1422  7942635  2005-01-19  F   1  10.008219  136.4  33.2  17.9   \n",
       "3   1475  7995857  2005-02-09  F   1  10.049315  133.5  31.2  17.6   \n",
       "4   1888  8520261  2008-09-11  F   1  10.060274  130.6  23.7  13.9   \n",
       "\n",
       "         처방일자        시행일자   BA 1   BA 2 Unnamed: 14     No  boneage  \n",
       "0  2017-01-09  2017-07-20   9.75   9.75         NaN  1.jpg    9.750  \n",
       "1  2018-02-28  2018-08-16  10.50  11.00         NaN  2.jpg   10.750  \n",
       "2  2015-01-20  2015-01-20  11.00  11.25         NaN  3.jpg   11.125  \n",
       "3  2015-02-25  2015-02-25  10.00  10.25         NaN  4.jpg   10.125  \n",
       "4  2018-10-01  2018-10-01  10.00   9.75         NaN  5.jpg    9.875  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('골밀도 데이터/total_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_img(r1,r2,r3,r4):\n",
    "    tmp_binary_img = []\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8)) #CLAHE 생성\n",
    "    for img in [r1,r2,r3,r4]:\n",
    "        if img is not None:\n",
    "            # resized_img = cv2.resize(img,(1000,400)) # (400, 500)\n",
    "            blured_img = cv2.GaussianBlur(img,(5,5),0)            \n",
    "            clahed_img = clahe.apply(blured_img)          #CLAHE 적용\n",
    "            # _,binary_img = cv2.threshold(clahed_img,clahed_img.mean()*1.25,255,cv2.THRESH_BINARY)\n",
    "            \n",
    "            target_length = 500\n",
    "            (original_height, original_width) = clahed_img.shape\n",
    "            # 가로세로 비율을 유지하면서 긴 부분을 target_length로 조정합니다.\n",
    "            if original_width > original_height:\n",
    "                # 가로가 길 경우\n",
    "                new_width = target_length\n",
    "                new_height = int((new_width / original_width) * original_height)\n",
    "            else:\n",
    "                # 세로가 길 경우\n",
    "                new_height = target_length\n",
    "                new_width = int((new_height / original_height) * original_width)\n",
    "\n",
    "            # 이미지 크기 조정\n",
    "            resized_image = cv2.resize(clahed_img, (new_width, new_height))\n",
    "            \n",
    "            # 최종 이미지 크기\n",
    "            target_size = 600\n",
    "            old_size = resized_image.shape\n",
    "\n",
    "            # 새로운 이미지 생성 (검은색 배경)\n",
    "            new_image = np.zeros((target_size, target_size), dtype=np.uint8)\n",
    "\n",
    "            # 이미지 중앙에 배치하기 위한 좌표 계산\n",
    "            start_x = (target_size - old_size[1]) // 2\n",
    "            start_y = (target_size - old_size[0]) // 2\n",
    "\n",
    "            # 원본 이미지를 중앙에 배치\n",
    "            new_image[start_y:start_y+old_size[0], start_x:start_x+old_size[1]] = resized_image\n",
    "            \n",
    "            tmp_binary_img.append(new_image)\n",
    "                \n",
    "    return np.array(tmp_binary_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_img(img, roi_1, roi_2, roi_3, roi_4):\n",
    "    \n",
    "    cropped_roi_1_img = img[roi_1[0][1]:roi_1[1][1],roi_1[0][0]:roi_1[1][0]]\n",
    "    cropped_roi_2_img = img[roi_2[0][1]:roi_2[1][1],roi_2[0][0]:roi_2[1][0]]\n",
    "    cropped_roi_3_img = img[roi_3[0][1]:roi_3[1][1],roi_3[0][0]:roi_3[1][0]]\n",
    "    cropped_roi_4_img = img[roi_4[0][1]:roi_4[1][1],roi_4[0][0]:roi_4[1][0]]\n",
    "\n",
    "    optimzed_imgs = optimize_img(cropped_roi_1_img, cropped_roi_2_img, cropped_roi_3_img, cropped_roi_4_img)\n",
    "    return optimzed_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 101\u001b[0m\n\u001b[1;32m     98\u001b[0m thumb_sp \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mmax\u001b[39m(far_xrange[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m4\u001b[39m])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1.05\u001b[39m), \u001b[38;5;28mint\u001b[39m(start_yrange[start_xrange\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mmax\u001b[39m(start_xrange))]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.95\u001b[39m))\n\u001b[1;32m     99\u001b[0m thumb_ep \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mmax\u001b[39m(start_xrange)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1.05\u001b[39m), \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mmax\u001b[39m(far_yrange)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.9\u001b[39m))\n\u001b[0;32m--> 101\u001b[0m optimized_imgs \u001b[38;5;241m=\u001b[39m \u001b[43mcrop_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcarpus_sp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcarpus_ep\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mfour_sp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfour_ep\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m                          \u001b[49m\u001b[43m(\u001b[49m\u001b[43mmiddle_finger_sp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmiddle_finger_ep\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mthumb_sp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthumb_ep\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m row1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack((optimized_imgs[\u001b[38;5;241m0\u001b[39m], optimized_imgs[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m    105\u001b[0m row2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack((optimized_imgs[\u001b[38;5;241m2\u001b[39m], optimized_imgs[\u001b[38;5;241m3\u001b[39m]))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "X_data = []\n",
    "y_data = []\n",
    "\n",
    "X_aug_rot = []\n",
    "y_aug_rot = []\n",
    "\n",
    "X_aug_noise = []\n",
    "y_aug_noise = []\n",
    "\n",
    "for k in range(len(data)):\n",
    "    if k == 354 or k == 355 or k == 916:\n",
    "        continue\n",
    "    img0 = cv2.imread('골밀도 데이터/rotate_image/' + data.No[k], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # 이미지 thresholding\n",
    "    r_img = np.copy(img0)\n",
    "    height, width = img0.shape\n",
    "    img = img0[0:(int)(height*0.9),0:(int)(width*0.95)]\n",
    "    ret, img = cv2.threshold(img, img0.mean(), 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # 이미지 contouring\n",
    "    contours, hierarchy = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    max_cnt = max(contours, key = cv2.contourArea)\n",
    "    mask = np.zeros(img.shape, dtype= np.uint8)\n",
    "    cv2.drawContours(mask, [max_cnt], -1, (255, 255, 255), -1)\n",
    "\n",
    "    # 볼록한 점 구하기\n",
    "    hull = cv2.convexHull(max_cnt, returnPoints= False)\n",
    "    hull1 = cv2.convexHull(max_cnt)\n",
    "\n",
    "    # 오목한 지점 구하기\n",
    "    defects = cv2.convexityDefects(max_cnt, hull) # 인덱스로 반환\n",
    "\n",
    "    # 거리를 저장할 수 있는 공간 생성\n",
    "    di = []\n",
    "\n",
    "    for index in range(defects.shape[0]):\n",
    "        # 시작점, 끝점, far 점, 거리 할당\n",
    "        sp, ep, fp, distance = defects[index, 0]\n",
    "        \n",
    "        # 거리 저장\n",
    "        di.append(distance)\n",
    "\n",
    "    far_xrange = []\n",
    "    far_yrange = []\n",
    "    start_xrange = []\n",
    "    start_yrange = []\n",
    "\n",
    "    # 가장 오목하게 들어가 있는 부분을 찾기 위해 sorting(내림차순)\n",
    "    di = np.array(di)\n",
    "    s_di = np.sort(di)[::-1]\n",
    "\n",
    "    # 내림차순된 거리들을 6개만 뽑아내기 위해 slice\n",
    "    for i in list(s_di[:6]):\n",
    "        index = np.where(di == i)[0][0]\n",
    "        \n",
    "        sp, ep, fp, _ = defects[index, 0]\n",
    "        \n",
    "        far_xrange.append(max_cnt[fp][0][0])\n",
    "        far_yrange.append(max_cnt[fp][0][1])\n",
    "        \n",
    "        start_xrange.append(max_cnt[sp][0][0])\n",
    "        start_yrange.append(max_cnt[sp][0][1])\n",
    "        \n",
    "\n",
    "    #손목뼈 ROI\n",
    "    carpus_sp = ((int)(min(far_xrange[4:6])*0.90),(int)(max(far_yrange[4:6])*0.90))\n",
    "    carpus_ep = (int(max(far_xrange[4:6])*1.05),(int)(max(far_yrange[4:6])*1.15))\n",
    "\n",
    "    #손목뼈 위쪽에 있는 관절 4개를 추출\n",
    "    four_sp = ((int)(min(far_xrange[0:4])*0.70),int(min(far_yrange[0:4])*0.95))\n",
    "    four_ep = (int(max(far_xrange[0:4])*1.05),(int)(max(far_yrange[0:4])*1.05))\n",
    "\n",
    "    #중지 ROI 추출\n",
    "    #중지 끝 좌표 구하기\n",
    "    for y,x_r in enumerate(mask) :\n",
    "        if 255 in x_r:\n",
    "            #y에 따른 x rows 중 255인 x값 추출\n",
    "            x_255_indexs = np.where(x_r == 255)[0]\n",
    "\n",
    "            #255인 x값들 중 median 추출\n",
    "            x_255_mid_index = x_255_indexs[(int)(len(x_255_indexs)/2)]\n",
    "            first_255_x_point = x_255_mid_index\n",
    "\n",
    "            first_255_y_point = y\n",
    "            break\n",
    "        \n",
    "    ## 중지 끝 좌표에서 처음 오목한 곳의 x 좌표를 뺀 간격만큼\n",
    "    sub = min(abs(first_255_x_point - far_xrange[0]), abs(first_255_x_point - far_xrange[1]))\n",
    "    middle_finger_sp = (int((first_255_x_point - sub*1.5)), int(first_255_y_point*0.85))\n",
    "    middle_finger_ep = (int((first_255_x_point + sub*1.5)), int(far_yrange[0]*1.05))\n",
    "\n",
    "    # # 새끼손가락 좌표\n",
    "    # little_finger_sp = (int(min(end_xrange)*0.7), int(end_yrange[end_xrange.index(min(end_xrange[0:4]))]*0.9))\n",
    "    # little_finger_ep = (int(min(far_xrange[0:4])*0.95), int(far_yrange[far_xrange.index(min(far_xrange[0:4]))]*1.05))\n",
    "\n",
    "    #엄지손가락 좌표\n",
    "    thumb_sp = (int(max(far_xrange[0:4])*1.05), int(start_yrange[start_xrange.index(max(start_xrange))]*0.95))\n",
    "    thumb_ep = (int(max(start_xrange)*1.05), int(max(far_yrange)*0.9))\n",
    "    \n",
    "    optimized_imgs = crop_img(img0,(carpus_sp,carpus_ep), (four_sp, four_ep), \n",
    "                              (middle_finger_sp,middle_finger_ep), (thumb_sp, thumb_ep))\n",
    "    \n",
    "    row1 = np.vstack((optimized_imgs[0], optimized_imgs[1]))\n",
    "    row2 = np.vstack((optimized_imgs[2], optimized_imgs[3]))\n",
    "\n",
    "    combined_image = np.hstack((row1, row2))\n",
    "    combined_image = cv2.resize(combined_image, (256, 256))\n",
    "    \n",
    "    X_data.append(combined_image)\n",
    "    y_data.append(data.boneage[k])\n",
    "    \n",
    "    # data augmentation - rotation\n",
    "    aug_rot_img = []\n",
    "    for crop_img in optimized_imgs:\n",
    "        # 이미지 중심을 계산\n",
    "        center = (crop_img.shape[1] // 2, crop_img.shape[0] // 2)\n",
    "        angle = np.random.randint(0, 90)\n",
    "        rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "\n",
    "        # 회전된 이미지의 경계가 잘리지 않도록 출력 이미지의 크기 조정\n",
    "        cos = np.abs(rotation_matrix[0, 0])\n",
    "        sin = np.abs(rotation_matrix[0, 1])\n",
    "\n",
    "        # 새로운 경계 차원 계산\n",
    "        new_width = int((crop_img.shape[0] * sin) + (crop_img.shape[1] * cos))\n",
    "        new_height = int((crop_img.shape[0] * cos) + (crop_img.shape[1] * sin))\n",
    "\n",
    "        # 변환 행렬의 이동 부분 조정\n",
    "        rotation_matrix[0, 2] += (new_width / 2) - center[0]\n",
    "        rotation_matrix[1, 2] += (new_height / 2) - center[1]\n",
    "\n",
    "        # 회전된 이미지 얻기\n",
    "        rotated_img = cv2.warpAffine(crop_img, rotation_matrix, (new_width, new_height))\n",
    "        rotated_img = cv2.resize(rotated_img, (600, 600))\n",
    "        aug_rot_img.append(rotated_img)\n",
    "        \n",
    "    row1 = np.vstack((aug_rot_img[0], aug_rot_img[1]))\n",
    "    row2 = np.vstack((aug_rot_img[2], aug_rot_img[3]))\n",
    "\n",
    "    combined_image_rot = np.hstack((row1, row2))\n",
    "    combined_image_rot = cv2.resize(combined_image_rot, (256, 256))\n",
    "    \n",
    "    X_aug_rot.append(combined_image_rot)\n",
    "    y_aug_rot.append(data.boneage[k])\n",
    "\n",
    "X_data = np.array(X_data)\n",
    "y_data = np.array(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.70710678,   0.70710678, -53.01933598],\n",
       "       [ -0.70710678,   0.70710678, 128.        ]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이미지 중심을 계산\n",
    "center = (X_data[0].shape[1] // 2, X_data[0].shape[0] // 2)\n",
    "angle = 45\n",
    "rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "\n",
    "# # 회전된 이미지의 경계가 잘리지 않도록 출력 이미지의 크기 조정\n",
    "# cos = np.abs(rotation_matrix[0, 0])\n",
    "# sin = np.abs(rotation_matrix[0, 1])\n",
    "\n",
    "# # 새로운 경계 차원 계산\n",
    "# new_width = int((X_data[0].shape[0] * sin) + (X_data[0].shape[1] * cos))\n",
    "# new_height = int((X_data[0].shape[0] * cos) + (X_data[0].shape[1] * sin))\n",
    "\n",
    "# # 변환 행렬의 이동 부분 조정\n",
    "# rotation_matrix[0, 2] += (new_width / 2) - center[0]\n",
    "# rotation_matrix[1, 2] += (new_height / 2) - center[1]\n",
    "\n",
    "# # 회전된 이미지 얻기\n",
    "# rotated_img = cv2.warpAffine(X_data[0], rotation_matrix, (new_width, new_height))\n",
    "# plt.imshow(rotated_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 중심을 계산\n",
    "center = (img.shape[1] // 2, img.shape[0] // 2)\n",
    "\n",
    "# 회전을 위한 변환 행렬 생성. 여기서는 45도 회전하도록 설정\n",
    "angle = 45\n",
    "rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "\n",
    "# 회전된 이미지의 경계가 잘리지 않도록 출력 이미지의 크기 조정\n",
    "cos = np.abs(rotation_matrix[0, 0])\n",
    "sin = np.abs(rotation_matrix[0, 1])\n",
    "\n",
    "# 새로운 경계 차원 계산\n",
    "new_width = int((img.shape[0] * sin) + (img.shape[1] * cos))\n",
    "new_height = int((img.shape[0] * cos) + (img.shape[1] * sin))\n",
    "\n",
    "# 변환 행렬의 이동 부분 조정\n",
    "rotation_matrix[0, 2] += (new_width / 2) - center[0]\n",
    "rotation_matrix[1, 2] += (new_height / 2) - center[1]\n",
    "\n",
    "# 회전된 이미지 얻기\n",
    "rotated_img = cv2.warpAffine(img, rotation_matrix, (new_width, new_height))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fed805019c0>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa8AAAGiCAYAAABQ9UnfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAADhTUlEQVR4nOz9aYyt2Vkdjq8zz2NN997u25O7jR3cQmCLBivE5mds0hIg4ihGQYpA4oMjiKWWsVAcK1I7It2yPwASDkhICBssx3yJSaSghPYHTCwLBawgaBvcdk+3h1vzmec65/w/1H/tWu9T+1Td231u3yrOXlKpqs5553fvvZ5h7WfH5vP5HAEBAQEBAZcI8bt9AQEBAQEBAbeLQF4BAQEBAZcOgbwCAgICAi4dAnkFBAQEBFw6BPIKCAgICLh0COQVEBAQEHDpEMgrICAgIODSIZBXQEBAQMClQyCvgICAgIBLh0BeAQEBAQGXDneVvH7nd34HDz74ILLZLN797nfj//yf/3M3LycgICAg4JLgrpHXH//xH+OJJ57Apz71Kfy///f/8GM/9mN4/PHHcePGjbt1SQEBAQEBlwSxu1WY97HHHsMP/dAP4Xd/93fdZ+985zvxsz/7s3j66afvxiUFBAQEBFwSJO/GScfjMb75zW/i3//7fx/5/EMf+hC+8Y1vnNp+NBphNBq5/2ezGQ4PD7G2toZYLHbHrzcgICAgYLmYz+fodDq4du0a4vHbDwLeFfLa39/HdDrF1tZW5POtrS1sb2+f2v7pp5/Gpz/96bfq8gICAgIC3iK88soruPfee297v7sq2LBe03w+93pSn/zkJ9FqtdxPyIsFBAQE/ONAqVR6Q/vdFc9rfX0diUTilJe1u7t7yhsDgEwmg0wm81ZdXkBAQEDAW4Q3mvq5K55XOp3Gu9/9bjzzzDORz5955hm8973vvRuXFBAQEBBwiXBXPC8A+PjHP45/82/+Dd7znvfgR3/0R/F7v/d7uHHjBv7tv/23d+uSAgICAgIuCe4aef3cz/0cDg4O8J/+03/CzZs38a53vQt/+qd/ivvvv/9uXVJAQEBAwCXBXZvn9WbQbrdRqVTu9mUEBAQEBLxJtFotlMvl294v1DYMCAgICLh0COQVEBAQEHDpEMgrICAgIODSIZBXQEBAQMClQyCvgICAgIBLh0BeAQEBAQGXDoG8AgICAgIuHe7aJOWAgICA20Eul7vlbTl9dT6fQ6ey+v62U119tfb4mf3Nv2+1Pt9gMLil7QLORyCvgICAC49sNosPfOADAI7JZjabue8SiQSAE0I5OjrCdDrFZDLBeDx2/x8dHWE2m2E6nWI6nWI8HmM2mzmC8xFTMplEIpFALBZDIpFAKpVCMpl0n/Mnm826a0kkEpHri8fjiMVimM1m+N//+39H1iYMeOMI5BUQEHBpYBctVK+HJHF0dIRYLOb+598kFf7P7+fzOabTaeQ4Sj4kNt2Px0skEpG/9Ri8Pv0dsDwE8goICLjwYGjPF66zn/n+1u35dyKRcMcl2fAzkhcJzndOJSjftfiuIWB5COQVEBBw4WHJIZlMRoiH4b/pdOpIRXNaJCOS1nQ6RSaTccfU8OB8PnehRoYZfdfDc+v5fGRlPbmA5SCQV0BAwKXEIrLQPBa3IcFks1kkk0mkUqlImE/zWMyPMTc2Go1wdHSE8Xh86nwMQwKLvT714gKWh0BeAQEBlwa+RTAWfWY/T6VSyGazSKfTTmBBQiF5xePxiNc1n88xGo0wHo/R7/dxdHTkPWcID771COQVEBBwaeCTvvvyYfyOv2OxGPL5PKrVqiMw7sOwIXBCZkdHR5jP50gkEhgOhxiPx+j1emi1WphMJhFvSoUhej6C217C1acuNAJ5BQQEXAow/EdyOM8LU1FGMplEvV5HrVZDJpNBMpmMEJDm0LjPfD53nhjDhrlczpEYc2lUMZLIbI7sduaBBdw6AnkFBARcKigRLMp7qUcWj8eRTqeRz+eRy+WQTqcj+SfK3JW8GDIkyXHeGIlpMpmg3+9Hrolkpjk2ABGxRiCx5SGQV0BAwKWADRnacJyG7Ugks9kMqVQKmUwGpVLJCTZ0zhaFGnps5rwYXpzP55Fw4dHREYbDYeT8/C6VSkVIKgg17gwCeQUEBFwaLMpraRjRqvtSqRRyuRxyuZwjr3g8jmw2i1wuh0KhgFwu50KD/X4fg8EAR0dHSKVSjgxtXqzf7ztCY4iR10cPTattTKfTkPdaIgJ5BQQEXCosynf5vo/H40ilUu6HMvlkMolisYhsNotsNotMJoNMJoNcLudyYqPRyJEQySmdTuPo6MgRIcOJSpi2OoeddxawHATyCggIuDTwzaWy4UR+zvxTMplEOp2O1CPMZDLI5/NIp9PIZDKRPBjFG/F4HL1eL3LMVCoV2Wc4HEZyXT6CosgkYLkI5BUQEHBp4BM/6GcM4dmCu1p/kPO96G1lMhkkEgkXNhyPx67YrlaBp6KQ+2ezWQyHw4iQwyfd57VwjljAchDIKyAg4NJBK2gQvvlUrKJhyzhRyEFxhRbbVU/JlnVKJBLOg0un0xHva9F12lqIActBIK+AgIBLByUvSzD6GcnI7jebzSJVNWy1eJ1Lpp4UBRy6L7/TcwRJ/J1HIK+AgIBLh7M8Hf6mWlAnCc9mMxcepAoxmUxiPB478kqlUhFJvM7/4sRnijpIXlxShVU5fNcVivMuF8GPDQgIuHSgR+XzqvRvzXXx89ls5siKi0R2Oh1HaPl8HvP53JWFoieli0/6oBXorSw+hA2Xj+B5BQQEXEpYstLPCV2UkopAVQaSbIbDYWSVZE5CHo1GC9fu8l0PsLiWYSCv5SKQV0BAwKUDlytR0Kui10NvSb/nPiQx1i0cjUYu5JdOpzEYDNDpdNDv952Hl8lknHKQIUSrKOTfPL6qHXX5lIA3j2AKBAQEXHhouSeSh6oH9XsAke1YDoqkpsud6AKWSiwkNBIgPTI9n26vpKgTpHmNeu0By0Egr4CAgEuBReE6JQU7iZk5KiUe/uaik5PJ5FRZJ9Y8ZCjRkqWuzKzXEfDWIYQNAwICLjwW5Zp8XpB6ZrpqspXQUyF4dHQUqQrP7xkmpOemFT3orVEQ4rsefh9I7c4geF4BAQGXGjYUqJ5YMplEJpNxFTFIKIVCAfP5HOPxGN1u99S8sUqlgvX19Yj3xpyVTlRW8tK5YsAJ4XJ/G5oMeHMInldAQMClhnpEFul02i2JwpqF6XQa5XLZERVJhSQ4nU7dHDD1+HSdLz0vz20FGUEqf2cRnmZAQMClhs07WQ+JRXn5k06nUSqVThXUVQ+O9QvVi2IIUAnsVirGnyWvD3jjCJ5XQEDAhYcSjSUBDReqPJ0V4ElaDN3lcjnk83l0u1237AknNLOuYTqddr8p2rAkpGFCDTuSOEls9OwCeS0XwfMKCAi48NDJxr7vFBRiMBelxMH803Q6xWg0cmFDlnqigIOqQ/W+AH8VD55T9/N5W0Emv1wEzysgIODSYtHKygCcmEJzYiQpEg3X+6KykD9HR0eYzWbIZDKRydD2fIvUjwF3HoG8AgICLiVUhq7EA8Ct26XkxVxWMpnEcDh0cvhcLod6vY7xeIzBYODmfs1mM2SzWfT7/chxdY6XyuX5P70whhPp3QUsF4G8AgICLgWsYk/nT2nhXZJJKpWKrNEVi8UwmUzcQpNra2soFosolUrI5/Ou2ny73Ua328VgMMB0OnUTnHmO2Wzmwo++8KDmuzTcGTyy5WIlySsWi6FUKrn/FxX1tCEAWwTU/mZD1fi5dh7fHJBFx170GTuklp1RLFrjyE6y5JIRDJ/oRE+GUvQefM9kPp87C1VlwovWM/JJmn3PgOfMZrPOUtb70v1tjTlVjdm5O/batBYef/uu0TcJVq+d7zybzUYGuNdee+1U/b2AN45FggdLDDY/pm2W75ltnz/j8dj1BX6mKyT7woUqxLD9R88bxBp3BitJXslkEg899BCA0w1bB2bGvgmu7WOVTfyb6qZyuexWaeXkyHQ67dYO0kmLhG/5cp13Ahx3inq97hRQup9uO51OHakQ8XjcKajYQbvdLlqtlguLkBSTySQKhYJbKZbnss9gMpmg2WxiNBq5JSYI7chacQCI1oHT6gSsdAAcJ8rvuecerK+vo1gsuoFlMplgPB4DOKmCwGcdj8cxHo8xmUwwGAzc4MPBidet4ZzxeIxer4fBYIDxeHyqTJC2D63CoMZIOp1GoVDAtWvXUK1WAQCDwQBf/OIX0e12b6ttBizGWfkt3UaNCuBETJFOpzGbzTCZTNDr9dDv95FOp5HNZlEoFNwx+/0+xuOxa0tcE0zbgjVG6Y1Z0pxOp6cMrIDlYCXJK5VK4eGHHwYQrVOmFan592QyiXgqtNJpnenf6XQa+XweV69edaRVLpdRrVZRLBZRqVScN5HJZCKWm1p9Soh6zul0iitXrqBUKrkKAdyO8X92mHa77eLuR0dHkTDKZDLBcDjEa6+9hoODA/R6vVOeGkksmUyiWCxGlosAjgeEXq+Hvb09RxSUEy9ShamaiyRCr4qky/2LxSLe8Y534L777sPDDz+MyWTirpuhH1ZOUMLhcxoMBm4QIrmS+FhBfDqdotlsotlsotfreQ2VRV5lOp2OVFYoFAp48MEH8Z73vAeVSgWxWAx/8id/EshrieCzV2+Ghoj1cGkkqsGSzWZRLBadfJ79kIpCvtPRaOT6dL/fR6/Xc23OTkRmn7OkBfijCgHLw0qSF+eA6KRD4CQMBpwQWSx2srQB99WB3heO0oGf3hgtvFwu5/7XuSPqNWkoi+RFIiUJ5fN5tw29Pkp/6WHx2lh4lBYg74ekls1mT3mdmuRmSIXn5jasRKDzYbiNrmBrw3QaoqFsOZlMRsiLg0iv18NoNHITTjlYkYQ6nY57BlodfDweu3vitdAy5kq5DA2R5LkPrWg+P14Xf8rlMvL5PNLptEvQ89jD4RClUgm1Wi0k6e8QfCE8H3HY+VW+8L4t8QScnjemIWgex44bugwKoRGdUONw+VhJ8iI0lKXhAFUTaUzbt79iOp261VczmYwb4Ok10JrjgG1zSj6ZrQ0LdrtdJJNJ5PN5FxrkPvRKtFNqx6HH0el00Gw2cXh46M5jqwfwfLrffB5d4pz7cA2kTCYTCcOoEsuG6UajEYbDoXsOajnP53OMRiNsb29jOBy6UGwsFkO/30e320W/30en03H3TE+JxkIsFnPXUywW3bPk8+dzZ2hXiZ7P0/ee0uk0rl69ilKpFFlCnkq1V155BZPJBLlcLljbS4YvbMj3xjakBKEGm/ZFfkeDSY2U+XweaVMMJ9scKEnN7r9IOq9kF7AcrCx5MVxFaHJ1kRjCQgmPHYShLXpXzLeMRiOMRiPn8emkSSvi0Ouip0GMRiO0220XBmHnYXiQHk0+nwdwHP5KpVKuM+7t7aHT6aDT6UTyb7YUjnon7MTD4TASTptOp06pValUUK1WXfWCra0t543RQ+HA0Gq10O12sbOzg729PUdILJDK++bA0Ww23fUxj8XzE3x2w+HQeWa0hnkNuVwOlUoFm5ub7l2Vy2XkcjkXUjw8PHQ5QCV+Pu9SqYQHH3zQXU8ikYiEoqhoe/nllyNhyIA3Dyu2AU4bXfzMGo6JRALtdhvNZjPSfumBZTKZSG5KIwNaHxGIij54fBp23F9JNeDOYGXJy6cqA057Xmftv8gSHAwGyGazjjDYwDX8pksqLCIrq2CiB0MPL5fLuc5BsYOGQdiJ6Bn0ej0nrCgWiygWiy68SbLziRpICoPBAO1221mm8/kcpVIJxWIR+Xw+khdj5wbg6smxplypVMJwOES9Xke1WkWr1UKj0cDOzo7zXK21zHBlKpVCoVBwgwn/1lzc0dERWq0WOp2Oy1/QG2RNO61hR6LmtolEwi0Db0NNiUQC+Xze5Sw1JMvirwCQy+UiXmrAm4cVzzDczvepBuiikLUew+aV+X4TiYRr3zRIgBNDkx6XhvS1rzAMrdccBBvLx8qTlzYoX3jQdoJbwXg8xnA4RDabjchu6TXwx1apJnH6zsUwBzsMB1YlQFVAqZiAq8L2+32Xp+IAzME7n89HQhzACbGnUikXFkulUmg0Gs47KRQKjkB4buac2Ik1x1gsFp3HWKvVkMlk0Gg03Dybdrvt8lhac47kkMlknACGYhhVRE4mE4xGIxweHmJ/f9+FGHO5HAqFAiqViguDAnBeIXAyAZVen8+iZuiQQhFVX/IYDFUGq3u5WBSG1faq0RNfv6VxYXPNmrNiO9bPVPXrCxny/CQ1tkfuH4hr+VhZ8mKDsmFCFUucFTbUTmK36/V6Tnmoc0lIYEow9Kb0/Hb+CEOctPIptmi1Wsjn8y73Qm9gPp+j2Ww6b0kH4nw+786dy+UcgVEJyYFa68JRVTmZTFCv13Hjxg3s7e2h3W67Dqvyf1qnzMnx3rmseqvVwtHREZLJJNbW1lAoFJDL5ZDNZtFoNNBoNLC9vY3BYID5fI5yuYx6ve6EEA8//DAKhQISiYQL85Hw6IVdvXoVV65ccQOazufp9XqRcCKf+2AwQD6fd1MdXn75ZQyHw8jgQ0IuFApuCgRzdAcHB+4zetcBy8Uiw077LcPhKk6az+fIZrOoVCqun6iSF4BrO7ZPdzodl8/isfSHfZbtnwaRet469SJgOVhZ8gKi87oInyemnpHuq9DvdG6RxsdVsJFMJt2gyWNZWTaPS0JiZ9TBluSh6kNWB1ArkkpAzXPl83nnxdRqNWctcoCeTqdusT4eO5fLoVwuu9ybPo/JZOIG7kKhgNFo5K6Dz1NDNTxPIpHA1taWK80zm83Q7/ddjqFWq2FtbQ31eh333XcfUqmUk8LbnKLmGkh4a2trbvt2ux2Zt8aQIQCUSiVHsvF4HI1GA/P5HIPBIBJ+YmmhbDaLVqvlcnPj8RjpdBrD4dB5uQHLwyLvRcNyGjKn0TIajQDARQjYRm2f0qiDPe9sNsNwOHQ5Te3P9j2r4Cl433cOK0teSlqLPCzbWWx+yretWug2TMjPmBNaFKZTUtWwpRIZO6bmZYBj4my1Wi7cxkQ0OxHzMqlUCuVyGZVKBYVCwYW59FmoRJ9EzPwY1YQKDfVRJGIHBJ03R+UXQ3q5XM6FG/P5vAvxsIRPtVrF+vo6Wq0WhsMhOp2OIyKSrVYMqVQqjqR5fQx9WkXhfD53Apt0Ou3OS1LUZ0hC5rlJXHz3JP2A5cOKrOxnQDQfzTZBL0gNOO3fVmHMv/VcjHzQwNK+vUj8Zf8OWB5WmrysJ6ViBQCnLCodhH2Seg6gJBYq9NhZmAsjstlsxLOzCWm9PjZ+zlmiQIOeR6PRiIQ9tcoHB+vZbIZSqYRyuYxisYiNjQ3X+RqNhiPXwWAAABGPhvfIvFOpVIpMB8hkMo5MSYzqXdEzOzw8dElwKgkp5KC8PBaLodPpOJKgh1gul5FMJtHpdNDr9TCdTl24keEgWsXMx7EIK59XoVBwBM7r6PV6zghgodZqtYqdnR0n5NDqCTs7O7h69apbJh44zqXwnuPxeJjn9RZD274afPTy+a5puNHzYv+ZTCauXXA7AM4oYX5Mp31o+SirdCRUyBGk8svFypKXHVg4CCkpLUr62oSuL5zBcMVoNHLKQ1rotP5onVvVobX+rCXpSypr/T8AkXyWCg5KpRIqlQpKpZKrHsBwm+bfxuOxI2LdXysTFItFd1266B8n/MZiMScpZ9UL5gOBY9k/K1B0u11sbm4imUyiVCpha2sLu7u7kQUDgeMwH89LxWU2m3VzupSIqILUuWiazyOh8Rmp6iyfz+P69esAjnMeGppqNBp4/fXXkUgkUKvVMJ/P3XulgIO/A5YLzTWxL1iFoc5zVNUq3wlDvgwR08jKZDIuLAgg0m41JM2+bc9PotJcF43hINhYPlaavHSwV1ghhs/lV/Kz87RoZWlsnKEk/Z/zT9QyBE5XAtDGz1yZtTCV/ChNV8+L21BlmEgk3CRfrUuoHp+G9zgfhhU0dB4bQ24kSxIJw4z8rfJ7nosEqcaDXbpdOz5JiHkMDlZqUTMUaPOXmsRXotJBTy1o5spu3ryJfr8fIfZGo+HmjfH58vp8eZCANw8b6lcS0+/U8NPQvPYv28d0mgf7HM9jhR0qDrHX5DN4fWHIgDePlSUv28DU/dfvKE+3jZGJYfVONEHLxs05XxyQGYKi/FwTzBqH5zXo0gu8Pp1gae+FBEUFotZR5ITaZDKJyWSCTqcTmYvFYykBzOdz9Ho9J1VfW1tzObtkMulk58yD8TfJi1YqQ3gqOU6n0+h0OgDgrpeDEYmdJKzTADKZDPr9PobDIYrFYkTRqIl3Ci9oUdMT7Pf7ziumKITWOK/t6OgIW1tbmM/n2N7ejlSIn06nODg4wGQywfr6uiNy5jM5ty54XssDvVkV2/A9q2elRov9nkYGxRd85/SU+B75LkejUWTVBB5H53VpyF8NGEKN3NAelouVJa9bbUjWG9K/SWI2RMRwA8mLAzPnfY3HY5eHUsvRJnnV+uNx6ZmotWmJlbJ5EhIHfyBawoa5Icp7dc5TMpl06xmVSiU3cNOy5LYMD9LTZJ5KQ4oUhmilDc5V4zPM5XJO0s9QDkOtk8kE1WrVhQNJ/tls1n2vpJ/JZCK1B9X67vf7LlTI90SLm8+c15dMJlGv1/GOd7zDVSGn98xn8/zzz2NjY8OtuEtvO5DXckGjyvY9JRUr4GDb1jauVVAY0qYRRoOnXq+7qATfNY1OAG7KC69DjVnNjfEz9uuQA10uVpa8gKg775uvRWgHsOEgX4hRQeuNRGEltlrqaFH+jNegYURbONYn+iDUSmS4jV4NB2wNlVBZSKVeOp2OkASvkYSl18KBBIAjBYYRk8kkcrlc5JpJ4qpsJHkRWs9QCwTbKt+aC9Rz8r44EDHnqFU56MnyPfDYqVQK6+vrqFQqzovU66UAhfPnNCQccOegRh7h64Mq5ABO2uRsNnPGDd8bIw4aPtTizepdnQWf+jFg+QjkhdONy9f4NJZtxR2ak7K/WeuQSjid96WyeQ7WNufE8yhJ2rksQFQZqWHFWCzmjq35pmw26yTkNnzK+WOcvByLxZzCT3NrOn9NiQM4WWuLIVMNzZAkqBZkkV4+LzsHrNfrodFooFAoOGEEc3l63wwD2qUu9HsNF1HdqGuEkbSpnkwmk6jVaqjVauj3+2i1WpG5dgcHB4jH404cojLq4HktDxqZIHwydSAaLdF5hWzT2Ww2sj1zsgztZzIZjEYjlzKg0IptDvDXLDyLpFRcFbAcrDR5+RK36jkoODAvSsRbcgHgwhAs1MtBUnM/HKgVuooxQYtQiciXRGZH4yRfln1S1RUHec6t4kRdhrqoROR9sKBpt9t1pY8KhQKq1aqbyNvtdp10fDAYuHsgidJr4zwrXsfW1ha63S4ODg7cs6B3yFBrLpfDzs6Ou6Yf+IEfcHJ85i9IfvQwq9VqxLPiZGLmPvicVDWmJMuJ2WwTW1tb7pr39vbcQDQej7G3t4dms+lk+AwVBfJaHqzhqGFs/maeVvPEGlLMZrOoVqvI5/MYDoc4PDx0YWjOCUylUiiVSjg6Ol6stdPpOOWtKnwtcSq52pqWmmsLWB5WlryUGDQnop8t+s4H9UjsXA+dS6KERc9LJz9qB1X5La3Hs86tHgEl8AzBUfjAYypp2vlkWgk+Ho+7ChPz+dx5KKlUylWQZziO+QMNsapnFovF3JIuPFalUnH7NBqNSFUSnjOTyTiZ/Y0bN3DlyhUUi0V3H7weKv24Um65XHbz6zgA2VAjcyB6ndxOhTrxeNwtLrq/vx95ZnyvtM59JYYClgu2Masy5LMngbF9cHvO4+P70vbOKReFQsFN4WD1FACRPDVw2vvy5dx0+9AelouVJS/FeeRkJbE2NGiFFZrEBRAJPbCEEgde5ox8oUlaa4y9s1MqlOxUAEJCpCgil8s5oQVwoobkj4Ym2ZG5HUMqOhiwNmIul3OeD/flNfG++WwSiYTLOfEzrixN70ivjWDocTQaYWdnB9vb26jX68jlco5g+Gz4jJvNpqu6b0ObGuKkR0wyVetayZ0KSl2nTNsPvWoVgwTP686C713bC9uBL7QOwE2y58oI3I5tPp/Pu3AzvXo77ULP70s52PBmmKR8Z7DS5GXDchz4FzVKhg60ESpx0fPQcjT0rPr9PgaDgVPuqdiBcXQNhVhCJXnYfAqvld6L3hsH1Waz6dRUrLxBQlIhiU5qBo6tzlar5XI6uv18PketVnOS8Xa7jfF47KrMx+NxVzCXVi7Xw1KS5QRg3j+VXIVCwXk0XAYlHo+j2+3i2WefRa1Ww5UrV04NCNxnd3cXsVgM1WrVvR+eVwmP6rDbyVHp1AXmMfnMmeNUkUDAm4cNw9rCxzq3j+2bhKHtKx4/qfOpq33TuEqn05hOp04kRMEV+yT7u3p5BPu/DflrKiFgeVj6E33yySdPeQ9Xrlxx38/nczz55JO4du0acrkc3v/+9+Nb3/rWsi/jlqAdQsnCCiV0tj6hXpKFelJ6bFp7qjLUOUokBp2rpNsC0YnL6i1RZEBvSCfszufHxWQPDg5wcHDgJtwylKi5nqOjI3Q6HXS7XRdupAKQQg7eo1bonkwmrrMnEonI/QAnxERPjKIJLRRMq5jem1rRvK9EIoF+v4+DgwPcuHEDr732Gl599VW88soruHHjBm7evInd3V3s7Ozg+eefx40bNwDAzX1TA4AGhA6KJFrK9jlAMmxpq23oO9b2tCjEG/DGYPuhkogSi/YPviPt27pNsVh03rkqU9kntcqMHdOsR6f91Teu2LEl4M3jjnhe3//934+vfvWr7n8NdX32s5/Fb/zGb+Dzn/883v72t+PXf/3X8cEPfhDf+c53UCqV7sTleOGLT+v/GhpUK91+r1DRBv/nYMkOQTJiI9e8EAdrHQiVzOy5tBNrlQeGwHS+ET0CJqg5cddaj1rsl+oru7Iw76tYLLoQGcUp9DrVI1ES5vPwDUKca6YTh30W7NHREfr9vrOIec86gHH/2WyG9fV1R4ok4kWKQJIalYY8NslLK23YcK01PAKWC2tAWmh7tqF9QkmM8y7VG6MnrpPe9TiWmHT80LC0bm+JLmA5uCPklUwmI94WMZ/P8Vu/9Vv41Kc+hQ9/+MMAgC984QvY2trCl770JXz0ox+9E5fjhYYAtNgmEC0dZcMV1tLjPjaMoY2Y23NtLS5JzxyJjdNrMpqDIT0XvUbNvai3lUwm3UDLtbMAOFEEvSiupcXlVHq9nuvMJDl+x07ImolUHWqVenpaFFgQ/Jv761yuWq3mOjoT6fRSdV0k5vC0hJMvLKcez+HhoVMhvvOd70Q+n0etVnMFktWj1VCmnbpwdHSEg4MDHB4eotlsRnIYapBwblcYqO4MlJC0j9joA5c84XvyRU60bBqPyb9p2NCo1JQCDTvtj2q8aG6VfZGRhSCVXy7uCHl997vfxbVr15DJZPDYY4/hqaeewkMPPYQXX3wR29vb+NCHPuS2zWQyeN/73odvfOMbbzl5+QYZJoD1e7Wc2Ei1sSuJ2WOqAID5HIbUdKD0Sec14ayWvZIuOwlDcLQia7Wayyk1Gg10u120222nDJxMJvje976H+XyOjY2NSI4uFou5CcGco0bBh+YE1WvSKh4kQObQ5vPj8lEkj1arFalUr6EdPkd2enp/8XjcqflUrmxDOTrhmDmonZ0dpFIprK2tYWNjA6VSyZXq0nllrPOoRgerK+zu7joFpxo7fGeqsOR7D7hz0PJlamwA0agE3wNDg8BJqSm+e7YTrXfJ7TRKoIYoc2T8nH3Riq6IRZ5+wBvH0snrsccewx/+4R/i7W9/O3Z2dvDrv/7reO9734tvfetb2N7eBgBsbW1F9tna2sLLL7+88JiUfBO6COIbhbr1PihJWXHEomOpwky3U4vOEpdW2LBWP3AScmUnVSUTE8SclKvfcZl6khHncg2HQzf37ODgANVqFYlEAvV6PXK9JIL5fO7qv6kMXkmV16nhVU2Ij0ajiBDE581qGR1bUodenVrQ9AztO7Hvivm+RqMBAE78QQ9uNps5haKtpMDwa7fbRa/Xi6zRpZ6a5tBC2PCtw1nhdNtPfflJ7q/GEAVC1lOzoULNg/FabKhyUegyYDlYOnk9/vjj7u9HH30UP/qjP4q3ve1t+MIXvoAf+ZEfAeCvaHHWC3766afx6U9/eqnXaa0im6/ybceBlaGks4gKwKnjkbyYj2KYwSaZ1Won6XAw1e/p3ZCoOAmXMnauc1UsFtHtdpHL5dwKv4PBwFWH4JwoHXhJGMx92ftViTIHAHqB3JfXoHL5VqsV8W54H/TsSMh6v/SS+Px5DT6vzZKqnpdLsDzwwANOEj2dTtFutx056WDEklTNZtNNObCTXynq4PkZOgzktXyoOtRndLLtaa1KhTUseDxdQZwKYFsflG3KZ8DQk1NDznprIe+1fNxxqXyhUMCjjz6K7373u/jZn/1ZAMD29jauXr3qttnd3T3ljSk++clP4uMf/7j7v91uu7WW3iis56WNWgmIjdIXFlSZvP6vIQ0bbtBqG5xDxWOQyFQlqB4XB3YVJ5C8tMIEr5vXHo/HI1Xdb968iYODAwwGAxweHmI0GqFYLGJrawupVMoJGng+vRcdABhm06offD70YjqdDtrttgsh8rjMOeTzeRe24X7D4dCF9vjDkKdVWOq7s4RhrWUuDtrtdp2kn4IUXdeJVT7opapIw6pEKVTh577EfsByoCFzazhqiJnkoXlIGnT0qhj9YLvmXEN+znXjWL5MQ4M8v+az9Fo0amPnLAYsD3ecvEajEf7+7/8eP/ZjP4YHH3wQV65cwTPPPIMf/MEfBHBcBPZrX/saPvOZzyw8BiXUy4Ja7L7clQ1p6XcAvN/rPBNfEpi/dS6Thi50ANZrsJYiwc4DnHh07DA6SZZVPGKx42rrunoycz69Xg/b29tuHhYXyuR1cd6Sgse2ohQO6iqTV2/GHoOfaZ6K9655Aq1QwvvRAcV6YFYpps+ZFcP7/T5SqVQklKvVT3hO3ZfXpQV4dSC0g2vA8sG+qL+tl2MjAgytM2rCvK8agkB03iQjB+zXtp3pZ2flOFVcErA8LJ28PvGJT+Cnf/qncd9992F3dxe//uu/jna7jV/4hV9ALBbDE088gaeeegqPPPIIHnnkETz11FPI5/P4+Z//+WVfypmw3hEQHeSAk9n6NiTlAwdSJuz1b1qB7DgcGDm4qjdjPQhffkjvYT6fRwrbUnHFjs3z0NO5cuWKu8+DgwNXB/HVV19FsVhEPB7H5uZmhFxZq1BzTPP53EnotfP6yEPFHFaWzzDM0dHRqdJKJCtWzhiNRpEqGPQ86Z0xv2afrb5bncdlc6f6HhgKspVI+E7ocdIQ0XfGcwXcOdjcluZLFfSMOE8QgFvLjqFgazBS2cq5gcy3q7dvc7CLrlEjJwHLxdLJ69VXX8W//tf/Gvv7+9jY2MCP/MiP4C//8i9x//33AwB+7dd+DYPBAL/8y7+MRqOBxx57DH/2Z3/2ls/x0lAXgMjgRGGCyqWtZca/LbFwX5XL8juezwo0lCDZcVQGTA+LuScSSTabdWErreqheZ/pdIp8Pu+ILpFI4MqVKyiVStje3o4UIH3uuedweHiI6XSKcrkMAJHQoC4jMplMnOzeFxohiQDA1atXUSgUkEql0Ov1Irkyei6TyQSlUily7wQrlJAsWTx1Y2MD99xzD3K5nCMvelQ3btxAu9120xOUdEluakEDp6X3Gh6kt6fqQlUZMhSlHkDA8rEop0njkIYM2xZD3/zhIqbb29tORMT3zrY8GAxQLBaRTqddpIIFtameZb9UT0/Vtsyj2dB1wPKwdPL68pe/fOb3sVgMTz75JJ588slln/q2oGHDW7GSNTmrCiXfdvY8OueLA6Ym+oEoedq8Co+juTAeU0OKGiKht0NPjEQ2mx1LfAuFAu6//35HHiSHw8ND3LhxA/fee2+kxhufE71H7qPlomwYRVdn1jqBJHgOGkpiNtzIfBhDePF4HKVSCevr69jY2EC1WnXVEXiMVCrlykJxcGIY1FrZPm/a5hoXtQG+N33vOogF3BnYvqR9wNYzZHun+IJzHLm0DldeoKFFQ4l9qVwuo9lsuj5nxVlqzCpsOHGRdxbwxrGStQ1tQ/JZRL5Ba9G2dj+b59KQmXpYmiux+R9LYGrpEbodPQMqrjg3ipaoihFYw+2+++5Du93GcDh0HbrdbmM6naJQKKBSqaBQKLj7ItlwkjEnNWuYVe9ZBxINAWqSW/MSOu9K99Wq8MlkEpVKBbVaDfV63QlRSHJ8/sVi0REJvUcNCdp2oIOdkhfBd6HVUfgO9Bg2zxewPNipKPpOVOlqxVg6V5ACHJ3TxYIB9Mh1hQQuc2MXYrWRGM11nhVGDFgeVpK8COtF+aTtuh3JQcOOzGcRllz0mGzos9mJUo/EYgdSigF0cqX1GqgKnM/nLsxBBZ8mp/W4LKtE8cajjz6Kzc1N/PVf/zWazSbG4zGazSb+4R/+AbVaDZubm25pEYI1/g4ODiICCj4LLp+ueUCdUGrzRxz0OYDYkFCj0XA5vVKp5BZ+5JIsOpARvEfNdXW7XfcerXHiE8n4BkotG2Q9bxWmKKkHLAdKHjpXUvumqnSt8IL5XRpdw+HQLe3DKi5s26zryXJluqQO24ft6zQidUzg36EtLB8rTV605Gyugw3NKgh10FOBgua5LKyYgQOhTopV61EHW3og2ilVPKChE1qKlM7zvjSxrMt/cGmWUqmEeDyOhx9+GK+88gparRZ6vZ6rFM8cANc66na7btLuYDCIqLq4NAq9OD5fSo2ZnyMR2nzTaDRyeTlW92ZVfM7B0pWSWVTV5hFJHBzAtPqIhnGtx2tFF9oWNIyoxKb1F31TBgKWB31fVtSkFVn4jtnmKZFvt9uu4IFOXTk8PHRSeYbd+b7ZrxKJRCSkDZzvSbFN2hUfApaDlSUvO0frrIZo49pqkVtvTQdD65UpGMKzYUMdjK2ww/6v59SckyahVTzCTqSDPcMp165dc8IGEgAT2PP53Fmvs9nxQpf0HPkcSFKsnK/3oLUVWW4KgCv3xO84J4ckQ0kzl1uhilKrz3OQUREIj0FS1QUKrUJN8258hr6wsnqNCvVIfVMtApYH28/4W0UzGqLXuV+pVMpFHbQ9UozE8DiAiDGqeVw9L3GW2tC3fcDysLLkBZzkN2xRXV9iXmPcKn2339k8CT+zjdjmTpTA9DfDb9yOqjprzVH2C8AVz43FYk7oQKUUrUruwwGe8niqAg8PD11uazQaObUW74c/HDBisRh6vZ5bILJSqZyaN6UTQI+OjnB4eIhyuRyxgtW7OTw8xOHhIQ4ODiLPuVaroVKpIJVKodvtRpYu0eocWlWf3qtOulbRiIW+fx2cfOTE76ka5f4BdxbqZbN9sz2S0BiJYFvRkDwNDy1AzUiKCohiseO5kyqtVyGVgv0JgPPidD5mwPKwsuSlJKTeD78DojkmHYx0X9/nJEP7Wy1DxuitMorkoNUo6JFw0KckXC1ODsQ6h4zeEENwQDS0RW+H3kY2m0W1Wo1Ux+Dgr7JwX6konf/S7/eRz+cjoTqGXXhtPBZDhao0nM/n2N7ext7engsZZrNZZLNZbGxsuHXEVATCc3FODkmKz1XJ1w4ilqR4PXbOns2L6bHt8TTXGLBc+HKNGr7VpU34WyfUAyfL3JCYmNOaTqfI5XKuP6l4g2IQ9ebVo9frs9U4fNcd8OawsuRFaGP2xbNJLousbbutqo9UBGKXRPCFsPQ8NpRIa0/nE3FQZ+iNx9LaicPhMKJSVKK25Mu5Y9VqFc1m0y0+aT1C/s3cE61LhvUovCBx8XnQ66N1qkIUeme0pg8PD9FqtdxKzFy6ZWNjw0n4Seqa49ABg4MWBzHNeelz8JGX5iH1nheJNPSdWa86YDlQr9aGcLUNKGhY6DQRvksaPgAihpP+2LyXnk9/fNB8bpg+sXysLHlpg7Jzp7QxaqOzncXXaFXOC+DUcTU3ZSc5WuuNgy47DdWHHOw7nU4kIc3zdbvdSH6MBJdMJt13SpSz2cx5S7lcDoVCAbu7u27VYFVW6f+5XA61Ws0dn4PHeDxGo9FwYUrOp+E16IRqAK6GIOX33W4Xu7u7roTPbDZDLpfDxsYG3va2t7lFBK34Q8OZk8nE1TJUBaMKb0iUhHpQVh1pDQv7/vlOOGFcC7gGLAf0hO1707Cw9j9bbUZBQ5A5Mc77YoRAyZFt1gqvtE2p8WNJVHOrAcvDypIXEA3/3cp2qvgDovO2CEtedvCj9UdLTuPjltCAaMki9WJYf5CdOJ/PR/JKzGcBcF6RFsHlQMtKGNPp1A24+Xz+1P3wJ5lMuors1Wo1Ms9KRShKUPv7+5HwIcEBX8s1UfTB55pIJLCxsYEf/MEfxEMPPYR8Pu9Ujb1ezykTdckV4HhJmF6vF3l3WuKK59ffDGGS/JSgVFXJ58J75jvWPEnwvJaLs/KPVPRpuSb+TcMqm826xVfn8zl6vV4k9AdE52x1u92I+IjEpqXAbJqBn2m4mm3mPGViwO1jpcnL5qvUE3mjA89ZYSUNL9o4uU8Nx+OppacCEg6YlKIzJs88DEN7Wp1dOxO9Ei3CS0Lp9/un5lwxL8aFJUulUqQKh04r0FAjr9F+z0GfBEaJPomDYcz77rsPV65ccSIQnWIAwF0nxSx8jjqQ6bPX56ohQb0P9TZtW9HPVFGpc9hCfmP5oDEDnF56yPYf7UcMWXPKBAVPhUIhEvnQgtnpdNq9R1sNBzg9dlgC0/ZzVmgx4I1jpckL8CdR7WBGqNd1lkpt0QDGARXAKQ/L18BpaarnxWPyby7xsbm56UQJnGAZjx8vhVIsFp0FquTBYrdUE/K++v0+ut2um8fF81GVVS6X3XEZltFahvTWeK5kMolOp4Nerxe5DyVNkpwOErlcDvV6Hf/kn/wTXLlyBfl83lXhIBGSbCilV+m9zo+zRoQViRB24ivfg817qtemE2bfjOETcD5sGF49ax9BMD1AT4w1CkulEtbW1ly4ezQaodPpOI8LQCSMyDanuWPg7PSB9m1f6DLgzWFlyUsXq2NHsOIKH3n4GqFtvDrgW3Aw10XzNNyxqDNo8lclvUdHR+j1ek52zmVNVCShpZh4fXZFZ05EHo1GeOmllyITg+lxFYtFPProoyiXy06OT0k+81W0Xlutlstl5fN5dDqdyLNRT4/3p4NRLpfD29/+djz44IN4+OGHT9Wl8xHF4eFhxDNlaZ/pdIper3eKVObzkwnUJEI9ph0g7fns3yr0oJgkYHmw4XeqXlW1SxEG31mr1UKr1UIymcTrr7+OfD6PQqGAarXqDB72I04rYd613+9HFLc61YKelYbCeX2aO12UPw9481hZ8lIoIdHSsuElO/BpmPGNQMNahLXw7XdAlLxo/cXjcbTbbefxcNDWigN6L9yHv0mi9OL29/cdQQDHEv1yuYytrS3UajWXJ5xMJshmsxFBBgeWYrHoBBPtdttZtcwr+bwZGgvJZBLr6+vY2trC+vp6JByn3hmnFChp6DE4d6fb7XqXL2G4yXc9PkNCw1DqgWuIVJ9z8MCWCxKGbTc0/jRMp9un02lks1nU63Vn4OXzeRedoJFC0imXy8hkMigWiygWi24lbYo6gGjhXc2HEz4DNHhey8XKkteihqZhNeB0g/M1wLMGKVWoqVdgZbc+BZMeQ61NzjuiWGM8HqPT6bhjslYbOyev286lYgiGqkBOCm42my72H48fT/Ss1Wq4evUqSqUSOp2Omz/GnBeT4yyOWqlUnLAik8ng8PAQ7XYb/X7/VFgOQOSzdDrtiKtUKjkPUEONJCtW/bCKQBLqeDxGq9Vy899Iyrw3Pks7IC7ygjV0u+gdMyQZsFzoO9J+opELtkXNFZOIarUaqtWqq87CMmo0VtkXWHrs6OgIpVLJGUta8ozn5vnsuBFw57Gy5GWhISxg8YKCSgZnfcYQhs9j0w7HbW183IaqrIiAx2CH7vV6karwPAaXQ2EIjwP4cDh0UnuG5F544QXs7++j0+lE1I8PPvgg7rvvPmxubmJ3d9dV5eYgQA+t0Wggl8u5ZUoymQyq1SpqtRpisRgKhQKef/55NJtNl7vS3NFkMkGxWMTa2hoefvhhZDIZJ+EH4FSUHJh0EjHzGRSm5PN5V15qZ2cHrVYrIr3nQOULB1risgYFnz3Pr8aFCl7CQLZc2PA327iqBtlv2WfYzl9//XW8+uqruPfee7G2toZr1645YyseP5l4P51Osb+/j2azGanjyTaj7c/3fvX8/D+EC+8MVp68rFTeWvAKX85k0TY2HOZr0FYQoNvy71gsWnJKJdoa3tTc3MHBARKJhAt7AHCxfIbOWOmCua9XX30VBwcHLsRGlWK1WsW1a9cckajnpnlBSvfp6WiOgCqvfD6PSqXivBKtncj7z2QyLmxDj5DkSvLVBTBHo5Hbjtcxnx9Xz282m2g0Gjg4OHDzrxS+0K0+d7utvlc7KPFzGgZWqRnw5sE2roaCRjFUKEUDI5FIOEOJIee9vT28/vrrbpqHrnxwdHTkilMzTKihdasgVGNUr8l37YHElouVJS+fAs1HJvq9739feFEH9kUWmiUvPa4q6XyhRHYyLfWknlqn03E5LK4CzEm7TEqTELm+0c2bN104UEUatVrNrao8GAwiNd3U2tVrp8xec24aokyn066zq7qQye5cLhcRsqiEnYl1kqh6bxyEuPTF3t4eWq2WI91FlrJev16Lbxv+r89c3zufbyCv5cMXngVOV8mh6ErzYGxnrNrSbDZdO2T0gO1rMBg4Y6dUKkVKnalUf5HC0V4zrydguVhZ8gL8uS4LzYn4YD0l3/GVyHhMHZw5ENoJtPyOJEYZuNYZtKsm0+tgxfaDgwPUarXIOkM6wG9vb6PRaKDRaEQEIMlkEtVqFQ8++KCbtMm8GsOR6+vrzrNjNQ4+BxInCebg4ACNRsOJNjT/RmLS4rn6fJV47KRvhidZdYOLazL8SRKxqkEbAuRz1dybL4TLv/lelMC5RhTvPWD5sEaFEhS/t940PXKu3aWhYhp1SoTsT4we0Ptn+/RdC3B67qAaniEPunysLHlpvskmf/Vz9TQ0hLcoZLjIy+KgqOEHdj5b1ob7qLrNEqt6HbxGrSBBD4pKPw2PcIBtt9tORGHLGXEicjqddnO+GHphncFCoeAqwXMQITE2Gg03aLBOISvA61pm9rmRAOjl8bnw2LrGGd8XQ4mdTieyphifD3MZfJb67pnv4HPk9r7pBfqeVQDDSdj0bLWUUMDyoEaFGnnqvftCwIVCAel0GrVazRmAFCTRmNP3pQpWG57nOW3eS8OZ+r/9PGB5WFnyYohoUaM6Kxzgy4stGuTs9lawYWPni67Bdz0q06a3YnNnDGVp+I0lmDjY0+NQsuSkZp0PxuulKCKRSLhVZ+kB8tyskqHn04Uwtcq3XitJtdVquWvg/VmhRiwWc8dlhQ4ONr7c3KJ3rTJ7G4rSd2kHMB2YptOp8yj1WQYsD77+plBPTL0rGlr1et29F22DNGIIDXUzDG6XQDkvJ74okhOwPKwsebFhLhrQFllMNt5ujwechLZ0X5tE1sQvj6XSbcJa8JacGLripFgdiK20np4kc17aeXXQjsViKBaLiMfjaDQaGA6HKJVKKJVKyGQybh4Zw3Tj8dh5arw29YjomfB5DAaDiIKL5+bqzUdHR8jn81hbW0OxWHQ5Oz5bEib/7vf7GAwGroajLl2hhKd5M71ntbD5DNSw0GfvCwWpgnPROk8Bbx7sQ9q/2GfYp5hTBU6qYOTzeVSrVVQqlUh7bzQap+ZCAiceOUVGVKqyVibbBwsA2LbiE3Xo3MCA5WClyYsDp5U882+fF6Xf+RqjWvn6A8AVEGVM3TZuex4dRBclftXS1P2tUETlwPZeeQ0cHOx5OWAwtDifz12egOchyfG7mzdvujlVXMSPSq9Go+EmK9sQHgng+eefx+7uLvL5vCNSenFcJoX3xfugl0aRCslKJ3brtAbNb/nWZNLcmOa59JgkMIYrNa8ZCGy5YHvUNq75YK75pguuct6WzkfkHED2fRKV5rQY2md4HoALxTMPxt+MQvgUhdrHQih5uVhp8rLktOgz+wPc+mx5HeABv0zeDnJKbGrFcfDVeD+3s0Sl16piAx2Mqfjj8RUMu9EyZbiP5KBTDDig6wRiHo+eDsNq9NRsnkH/JkkBOJVDUs+JOTUOOsVi0Q0qsVjMeUQMYSoh2/e5iGh8OQ315ujBag7Pl6MMePNQkY32P/W4WO6J33PJIJYz075FIYYNN7LN0+CzxqZK53VbNaQ0px1wZ7Cy5GUbv4bqbKJV8yz8Xn/rPmeFGa203A7g2vBt6IH7W4tevydpaP5LBQ8kK3oo9IYYWtRkMz2gbDbrzsX5VcxtaR6M87zYsVmxOx4/njzc7XbRbrdduFCvXTu+dniVxKsSMZvNun1LpZIbtBjWZP05FgPW+WT6/n3E7SMzNUA0JEurnBPANacZBq07A1+UIh6Puzwsw8Zs31wKqFAooFAouNXIdSK+9bi1gDXbHT8DTrxqVulgZMGXo/ZFVAKWg5UlLx84QNGy18EcOPFGrHdmc1w+4vMRkxYU1YFPZfSaH1MrkZajtRr1f70vigj0c633p52YQgwShcb1OVFYwycAIlXdWaw3m82i2+1ie3sbzWYT7XYbsVgsMjlU5+OoRcufVCqFWq0WGUS4flcqlUK9XnfhInqITLLX63UMh0PUajVsb2+j3++j3++756AV5W1ewuc98R1q2JD5NoZMfcvaBCwHGvXQdmy9KO272pY4YZkiDq5pxxJmunSQKoJJTmyfNFp4TD23jYTM5/NTK0gELAcrSV6+vJbPmwLOVg35wog2TGjP6YNa6xykdVBXktN9dKC0YQ2Vemu40Sfl5fVrh2eJo4ODAyc11k5ow54akuMcsk6n4+ZdxWIxN/mYtRcBeImXxMfBguemICObzTqLulwuu1ChT+jCY5XLZUe+vF/NYfH+rTetz8rmMRkyJAmq4RAGquVD+4A+b4WNmmg7VUONuVR6a+q9MXKhAh/+1jmIvCabBtDPA+4cVpK8gMUkQ5yX0/LtZzuOJUklI0s29kfnMdnJy4CfvPRzJTxNbKskXDuaTv5l2E0rU9Cz0fCJepVaYmc6naLVaqHZbLo6hiQf1h1UstH75nVxYUnNZTApz5WcuU4Zn7EKSpSU4vE4isViRMhhScvOFbIDn6rUbL5LyYv7Bywf1qMB/CsDqGFil0uhsUXC4uT22WzmPHoSHT0zfdeZTMZ7XTY8GNrAncfKkhdhPScdsGz48KzQID/TAV0JgsdW0tLOpaENXzhNO4jmr2yn5XlYbYMDNr0Y1jRkRQseX0NoqVQqUoqJa3HRe6J1qqRK8uDz0JWFNUc1mUxc7sqGUzRZzgml6XQa6+vrqNfryOfzznvjPvTsuJ4Yr43XPhwOXaiQXh+ridjrtAaJrXmp2zPXNRgMTpFwGLjuDLS9K7S2oS2ZRsOHbUfnIqqog2FvLnjKd6rtQ+cXKjQ0CMD1LZtL0/YU8OaxsuSlg9VZVTNsWFH/VgvPF3ayohAb4tPvVLmkOS/gdJVzHey1EyrJWfEDoVJ7nf+l18OOx+eia2hxlVlVePG69X5JMPxMc3gM1wBw0mYeh96VrgzNgYc5KYpJjo6O0O12T6kK5/O5q0/H2o0cPNRCtqpBGhN8d75t+TfJUeevhVDRncUiz1YjDLYahvY5Elc8HndSeY1KJBIJtyArDREek8fVvDiAiJhjkdGihmLA8rCy5OXLSdmQhDbGRTksG6rw5c74t126geewuS4bKrTkZaFhLl/H1uuwA6z1JJX8+Dk9DbUcadXO53PXeXWuCwmZx9EVnavVqstpccIzPSoquNbW1txzopdJsmJ1fK0wr5ODGe7hz2Aw8BoWlpB871Wfnb5jil2s6AMIK+beKfjChiqQYDtVow9ApD1yzlYmkznVLrSupvZTGjYadtf25gvrL7r+gOVhZclLLet4PDpnyTZAG5bjdjY560v406rnedQDUSWUKu1sbkwHWBLMfD5331niVBWivQ/Nians204F0Lp9vEeG3ugJUThhPUSSliq4mLPK5/Oo1WrI5XIuaU7SUlAwwsm/XMxyd3fXTZDm+TiXJ5FIoNfrAUCk6kW73XaDj05eZo5M3995hoKGD5XQFwlQApYHX4gciBbA1soxwHF1DbZ5hpNp7HEVhel0ilwu59q99jPmwPg5+7MWcbaEavNtbBshbLhcrCx5WamtDlgcyDgI2TJSHOx8jVE7khVF+HJaZ+W4AJyyCnV1ZJW6s24gACeLZ8fR8CLvi/knnWDLsBonEWvIjxM9SRIqEQaOQ4vMKWQyGcxmM+TzeZRKJVQqFTdJNJvNuntgZYpYLOZCgfSo2u22CwkmEglXHJhV6Uk09OD4WSaTQSKRcNXC+TknmtpnsSjBru3BvkuWuyL5ad4y4M7BF9YHoqFw5l5p3HEOI9seya3ZbEaKKqtgg8KkdDodqZ2ppdassavjQwgRvjUIve3/D53j48OtWtE23m5VUT5FoZWfE7ovl3OgUg+Ay7uwA+qSC5r/ongDQKTjkbBUsKCCEyKdTqNYLKJUKiGXy0VyCFqlm5OEs9msI69CoeC8K94fvZ5er4fhcOiImYMERSK0jFkgmGFAEjUNCM1zkJzVOuY92Hfs8575t1Ua2pCh9UpVQXo77SXg1qHRB8LXd1RgpZ5xOp127YelvPhO1YjTHBaNPBU6+cQ9Noet7cfmlAOWg0Be8NcttCrBRCJxaj0eG75Q+MiQYUGrruPf1tsC4LyJfD6P9fV1lEolZx3qIN3tdp2VyFJMvA9WHGDHI+HZ0Bc7Gj0w3h9DfZVKJSIsUQWX1pYjwXFAH41Gbln1Xq+HROK4aO/u7q5LnheLxQjhxGIxV3R3Pj+R4qtghPktXjPncqkqkEqwQqGA+fx4rTO+U6ov9T3bgVBDRhqWosLRt6/mTgKWA19f84VmadSQMFSQxPA1yUhD6NpmOK2C/w8GA/T7fdcvVOShERUfQVmiDVgeVraH6dwQ33c2Ce8jOIX+bxWA9AY4mJPAtOSRztniYMxzZ7NZVCoVbGxsYH19PbK/ehxcT2o+n0fKIlWrVUeSzWYT+/v72Nvbw/b2doTE9B51jtTGxga2trawtrbmOivJSydD0xMkSXAAmM/n2N3dRbfbxWw2c6WjUqkUhsOhI2GWker1eshms+5+mD9QpRdDO5ThEwx5cnFMikISiYRbBsbmNTSXov8v8rjo5fL46rGFXNedg08kBZxeSVmjDlp7kkYQDVFVEvJ9agiYqxzQiKIRyHP55nb5xg5VsQYsDytLXoSG9vj/osHnvNAicFqlZs/F3778lpKXHosDPReBZEfQ4/DvdDodyQfNZjPU63W3D/NBnPvks1zt/CYN/zEHxdwYr5cEyGtVMqQ3xm2oPKR3oiIVDij0GG3OkZNENWeo5MLBCoATi+RyOadMZHkgfR/822eg2Dl7JGg1TpTA9H0EErsz0Gdvw3NKYGqA0AhhmFzfN98VDUueg6FtFYDYscJ+FvDWYaXJiw3YDlxs+DZM6NtfPavziM0SjZX06jURSmD5fN7VDWRntB4diUuTz6VSyV3fdDpFqVRCp9OJ5GmsqlEHiFwuh0KhgFwu58KMLAGlYbrRaOSS4/zJ5XLY2NhwdQ47nY67LnpPDO3xunldJGqSLQcdErFW+2CingNNOp12Za2y2SxarRZ6vV6EePi8tR3oc7fExefH1aPVe9acmYaDA5YL6+VoP/KpEG2OijlVK9iiQIOqVwp+ms1mJLep4UKe07ad4IG/NVhp8lJoeEgbvBUvLNqPnUYnwloys41aiUfDhDYE2Wq13AB/5coVVx4pnU5HFmWkuk4nz8bjx6WR2JG1bJNK+HleDaeppJw/jUbDkXu/38fa2pojVZ2wTK+JqsR8Pu8S4zw+w4aat+N9JRIJR17dbhelUskRBa1h7sNjqfSf3mI8Hke73XZlqqz3qzlH32R1bRPMdVmVIfenAREGrjsDnyiD0Motmp+ksaeeMwAUi0W0Wi0XPuS0D7Y9hq8PDw9PiTss7GeclMzv2L+CVH65WFny8nldZ4X87L6LpPLqgdnjWetMwxW6ve7HXBavtdvtuk6mogDtsFRU0UpstVqYzY4rYfd6PVcFXZdCV4GD5pgAYG9vz4UsueQISZFKQm7PBDefD6+Hg74OMqxywL9JdJqj03wcBwUt2cMwJQcpelbMc/Ge6TEuer++NmBzXxoy1NyK9aQD7gz03WleyZd/Imz9Su7Dtsk2oasia55M86PA6ZUgFpGZ5rm0rQQsDytLXoB/xj5xHpHZ8J793Bcj120WKdtsMno+nztvqt/vY29vL+KxkQjK5bIrZmvnafX7fZfvoRU5m80iRUcptCBJKpHt7OycIi/mkuihUVzBH173YDBAq9VCo9FwYTutN0cPq1aroVarubxdKpVyy430ej3nyTE8qKFNkh9JbD6fuwoKnBum9en03aq3q89ccyVKXqqGBKLes3p1AXcG5+WTff1YJxZzO32XJC+2MRIXhTk2FKmKw4C7h5UkLyuM8H0H+Eu92IGNn/G3tdztsTXPpYMigIi3w/No3o0CCVr5+XzeEUqlUnHzrKiW4kDfbDbR7/fR7XbRbDYjS9aTGLvdrvOodCImtzk4OMDf/d3fufwZB+t+v++8N60Uks/nnTXLn0qlAgDO82s2m44omVfQydq8Vy4iqHk5fUYE1x/T+TwchHzz2XwJffv8uQ/l1ZxU7ZujZz8PWD74XDWnyPfEtqvvQAsA6Pbj8Tiy/hdXTQDg+geNPfbB4XDowpJnRWvYDoK3dWexkuTls85sAhbAqc8sqXGQ1+MwVEdoQ18EazXq/gyL0duhAIHyeZZp4vIg2iFpdabTaQwGA0d29MJarZZbbdgW6NVrpvqPREArtVAooFqtIpfLufPbJUx0aXYSG61eelBaAd4qw4CTCca8L3p3vAeSJ8OLOmFb70tDutZ69ok1NOTkq2PoIygVlATcOaihYNu7fq7GEHBSZJr/M+QNIGJEaShd89lHR0deNalvTAkS+TuLlSQvYHH4Qa3vRY3zVpSFur+1zs6yyjUPx87H0kq6nHm5XEalUnEhNoYPNR+n8nISCydp6hpbiUTChf1sXkgVW0ommUwG1WoVm5ubqFQqbskSEpuW2CGp6jVyoFF5e7fbjSz8RwJRC5ae4HA4RLfbdZJ9Epf1nlSZ6cs52r8J65FpmOmsdx3Ch3cONqKhnu+iML+djsI2paRGhSHbpEYDrHhKRU7ajn0evVa2CSHG5WMlyWtR2JAdQOXvdnsrk+X3QLTqA4/hm5NlZbo22azXUSgUsLm56XJOtVrNVbbI5XIAEAlrafKang69IHo/SkSc6DudTrG7u+u8LL1XVdJdv34db3vb29xvhirtkij2eeu98XiU1fM89Xrduz9zXTpQ0DJm/UNeN4mt0Wjg8PAQvV4P3W4XrVbLS8w+4Q2PrQSmVeo52DE8q6GsQFh3DsxZaYRD+5KSC0lE88JaaUZrc/pWCdfyXz5oeyTR+QRB2h7OUy4H3B5WkrzUQvd5RGpxc3vgROq6aICyuRObA7OenA6OVnHIgTGfz6NarTpVH70wEoZagnY5euv5cc4Wr5VVMCii8Mn2ed3FYhHr6+t47LHHcP36daytrTn5uubvSGDWc1EBCD1FVXhZb9bmNPSZ8ZpoNTNXwUFnOp2iUqmgVquh3W5jZ2cH8/ncVdewXpdtE3qtKtQ4z3q24eWA5eKs56r9SPuEGmGMZLAepyUstkNtB3ZSs81pa9vmNS4qzBtyYMvFSpIX4JfdAqeLsvKzsxK0/NyS4VmCjUX78HMmmrUSuxWTWAmvegBa5kY7Lzuukoh6hLotj01Z/MbGBh544AGsr687paBauqrko+XKZ8vBfzKZOMKhR+ib1KsTkPnceFxCQ6MEn0Eul3Pf9Xo9dDodp9jUbRcZFNa4sOuZ+WANkYDlwheuV0LRPmrFVPxbIx/a1pW89BgaulbDjCFB2w99715TAQHLw8qSF+Cfk+UjH19O5FZhO5vPK7LhReBkhVZdxJFLiDBMkc1mHVFp1XTW+7MkpGGX+fykqgWvUy1PHahTqRTW1tbwwAMP4Pr1604VORgMIrkpDbVxHpkuI3F0dITBYODydCrmsGt6cZ0wPiuqJznRmPPHNKmu74/XmEgkXMiVdRStlaxrPKm3pYOXrdp/3jsPg9XyYfuMqkEJNaK4LcHyZXznmlcGTqY9aC5Ll9/RNqXhSyUvG03RKEYIGy4XK0tevsYNLJbHAydVGCzJnTVQMZxnLURr9em5VGCgxNXtdtHv95FOp5HP510YkEIMdi5b5UET0ZTbA4isx6XL2et9Ace1CavVKjY2NjCfz9Fut9Hv9yMqRRtmZTknXg+vgeQ7Ho/dZGbuz3ljsVjMeXYcKOgxKqlQpEHlYTabdedXC5ghV5Xf67PWd+zznn1iDfUWrWHje68Bbx7W87XvUt+75kf520YZNE1AQ9CGkbWvqxrRKletuIPH0M+CR75crCx5WcLxKZYWDT46MLER67aLciPWUltEoDyGzfVodexkMukGfyoSFynotJPZ+2POiyWPrFLRdnbmynS5Eg2VaZhNr5+eF9dLUnGLyubt3BwSLhedVDLhkiScq0ZiJumrDFrzgbx3m5e074P3RuPDtgHf3yGvcWehhpIN/fM7FVfpu6XoiIRmQ4g8vrZVNTZtPkz7M69h0TUHLB8rS16A32PSzzgIakM961gKSxI6WPqktXpO7m87iQ7AqVQKnU7H7c/1qoDTSzCoyEHDHSzlxOVT2El9+8xmM6fq6/V6aDabkUnTKvYgEep3SlK5XM6FCQeDQWQfEg6rx3PVW60gQhLnviQvXamahXn5HBjCtJa6hgYX5U60EoMdxOi52kEwYPlYJADS77VsmOalSF7pdBrz+dyVKbPvUxca1ekXwMk7Vg9Q+7KF9qUQRl4+Vpa8fGozhQ5sup2FWl6L5n/5SMpKvrk/O8hkMkEsFosseKh1/lRVNZsdF6LVMIoei9UCONiTLG7evInt7W3s7+9Harjp8enh7O3t4ZVXXsEDDzyAfr/vlIr6/EguvCZ9xlrUliskZzIZdDod9yz6/X6kTiKJU+XoJFESlnpovB6+B4YaDw4OTlWU19CtepW2HfAZaBhXScy2Kd/nAcuDviclHoVGA7QdWkGRVuVgPjQej7vpFqzSwvMC0ZW6SYJKdPY6eF79P2A5WFnyAvxFWH3fKW7Vglq0nc+yt+ENtS51kq56D/xNj6nVajnRBsFBn3PASIT0WlqtFvr9fsSDsgIWkkW/30er1UK73XbXpNdmPTUlQ13ET/MK+judTru8nJbJ4vdK8lpDkftxgOA2FLqQFH016mxY1oYR1Tu27zbktO4ufGFx/Y7QPKf2KX3POjFeQ4Vn5bB4PG0nFhrJ8F1nwJvDSpOXwtdY7WBntz+PxLRznTUwciDlb/WoaOmR4OiRqbqPeSStz0bS4FL3/OEKsYPBAO122ykGeb02dwXAhRdbrRYODw8xn88j4VSb/yGxUV6ey+UiljJ/NGzjW01aCU+tV3qg9LxszkHzZj7y4jOxg5i+E/4oMRKqgtR836K2E7A88B1pxXYfNPe4iLz0eLqisg/c144D2lasIWzbRmgTy8VKk5f1ggjfoOmDzVv5QE9ChQOa/9FCnzogMpTGlYc5IM9mM+dtsRBtPp8HcBKeYLknVoNQIqIAotPpuLWKfCouyuhJFFxT7B/+4R+wubnpVkZW74XnBoBms+nuiQSpeT0SQKfTcTUNS6VSpLo3oYINAE5Sz7wZ5fj9ft/lyzY3NxGPx9Hr9SJLomjukIOWEpWtrMH7txa0hjX5WQgX3jkoefBvhRpg2m/Zlpl3ZbjZhus1TaDvVBeuZL9l26UhyIgIgEgJNF53wJ3BypKXLz7NMJhvW7XUtOOc1zjVsvdVbgAQWUCPoTslMPXE+JuloFheSeXAOjdMPYdMJuM8kUaj4XJgPKYqKG2dRABuMcpSqeQqcqiqUPNx3J+Eq4s4cqIyj8n9dG0x4MRz5QrN9Cw1zENo7oOJea7crEvE+MKiOk/Nhm31962ECX0efMCdg5XGL4J6XfpegeM+omvjsT+pwQOcXtVBDU1979PpSXFg/q3XEbAcrCR5zedzDIdDACd17NQzUILRhqmWOa12bbRqses27CQsiGtJgnXXrFw3mUyi0+mg1WohkUi4On28TgoSgGj1dFaw4HF1cuZwOMTBwQH29vbQarUi+6sUXQd7duLpdIr9/X2nbByPx+j1ek4oQQUX82z0OEejkfOw2Pm5inK73Xbkpd4M57HN53MMBgNMJhO3CCf/p8fF59bpdFwOrNvtYn9/HwcHB2i3226SMYUjHJx0UU6tkcj71mQ8z0Mrnu9Qa1nyM77HgDsD+2zV01IjxW7j66PWYOW7VOWiioFolJGcNKTty3XreBKwPKwkeR0dHeE73/lO5H/CJmt9Igr1wnQb34/tACqs0PlMavlxv0QigZdeegmvvPIKEomE85YAOLk8Q2dchJFQAtN6fvS6Go2GK5W0KJ+n9wwcD9ovvPACvv3tb7sK8lwrS+dUaV6C98fQoV1rid4ft7fkBRyv/8XjM0xoQzzAsaqSlTvW19fRbDbR6/WcyISExfdND5aEpYYG/+dSLQQHLrW+9Uet7LM8gYDbxyLPVw0L7XNsx3w3ND5pkKgKUSvAM2qgUQ9GEDQ8rMIk+641RH4rdTEDbh8rSV6TyQTPP//8qc918D/r70XKorO2j8Vi6PV6EWk34Feu6f6pVArPPvssAHirPGgYQ/fTgVU7NwdwrdXnIy/f9fO4JBINKeo9aMe1ohX9jAMK97EJdt1Xn9Oid8DtGDbkPfoqnJxlgOjxlLx9KknC91kYsJYLS1L2c5vDZhidRhGl72qAqkiJ+6moyCce0uPbUDQAZ1RpLu28sGbA7WMlyWs+P64w/lbjrCUWAgICzoYaHNYw0LAzyULFUgxlqwHoM4qsItaSl5XIW6WhvR7+HULIy8dKkldAQMDlw1mTfdXrUeJiEWiGrqnUTaVSTvTEELx67gy563lVWMUwo3pTGlXg5765kwHLQSCvgICASwH1vGwoXHOsLAQdjx8XrqbXxYVLuQ+JTKXws9nMFbrOZrORcKGGDJlHtgpGemoMVfJ8ev0By8FtT0z5i7/4C/z0T/80rl27hlgshj/5kz+JfD+fz/Hkk0/i2rVryOVyeP/7349vfetbkW1GoxE+9rGPYX19HYVCAT/zMz+DV1999U3dSEBAwD9uLArTqffDECGnjJC8WJOT87z4m6FETuKnGphem+bMbJ5a89d2rphVKp81ATrgjeG2yavX6+EHfuAH8LnPfc77/Wc/+1n8xm/8Bj73uc/hr/7qr3DlyhV88IMfjOSYnnjiCXzlK1/Bl7/8ZXz9619Ht9vFT/3UT4WEZkBAgBc2x6QFAjQfRdKi51QsFp3nxYoyw+EQvV7PzT2kgImrJQDHal3+qECJ5+FiqhqWVJm+XQ/OTscJePO47bDh448/jscff9z73Xw+x2/91m/hU5/6FD784Q8DAL7whS9ga2sLX/rSl/DRj34UrVYLv//7v48/+qM/wk/8xE8AAL74xS/i+vXr+OpXv4qf/MmffBO3ExAQ8I8VmjtS4QRwvHJ2Pp9HoVDA0dERkskkisUitra23PzGwWDg5lNqOJBTJkg0/X4f+Xwem5ubKBaL6Ha7bh4kvTHmxHjOfr+P4XB4as4fAEdavvJSAW8cS61n8+KLL2J7exsf+tCH3GeZTAbve9/78I1vfAMA8M1vfhOTySSyzbVr1/Cud73LbRMQEBBgYb0slkcrl8solUrI5/OuGDVDhoVCwdXmBKJzGHXKCP8fDoduvmCxWHTL95Cw6Gmxig2vgat153I55/Vls9nI31z5IWA5WKpgY3t7GwCwtbUV+Xxrawsvv/yy2yadTqNWq53ahvtbjEajyNIg7XZ7mZcdEBBwCcAJwrFYzJFCoVBAvV539T21ygzJpN1uu8o0GrZjYWdOSuZ3/X7fEV+hUECv10MsFkM2m42ECIGTVRJ4fjtRX+dJqsgk4M3jjqgNfZNez3tpZ23z9NNP49Of/vTSri8gIOByIRaLoVgsunBdsVjE1atXUalUUKlUIkWXY7EYCoWCM5C73S4ODg7QarUiZccooNBamtPpFM1mE5lMBrlcDrVaza0Fx9XKVZQBnBTjTaVSp+TzVB7SIwzFm5eHpT7JK1euAMApD2p3d9d5Y1euXHEFXhdtY/HJT34SrVbL/bzyyivLvOyAgIALDno+/KFnREIBTkKCyWTSkZeqCn3VNVQ5SGLkQpQAUKvVkM1mIzJ57qtkRPJiOJGl26hWzGazqFarwfNaIpZKXg8++CCuXLmCZ555xn02Ho/xta99De9973sBAO9+97uRSqUi29y8eRPPPvus28Yik8mgXC5HfgICAlYHnEjMH5IYVydQaXoikUA+n0epVHKFl606UatpaEgvFos5FeLR0REqlQoymcypslC8Jn6mc8y09iWJLJvNolQqBfJaIm47bNjtdvG9733P/f/iiy/ib/7mb1Cv13HffffhiSeewFNPPYVHHnkEjzzyCJ566ink83n8/M//PACgUqngl37pl/Crv/qrWFtbQ71exyc+8Qk8+uijTn0YEBAQYJFOpxGLHa/PRYFGKpVyqwyQlMrlMqrVKmq1Gvb3990KB6VSyYX1tDB2LBZDp9Nx3hMA7O/vI5/Po1aroVAouDXndAkgABEi01wYwRBmqVRCqVQKYcMl4rbJ66//+q/x4z/+4+7/j3/84wCAX/iFX8DnP/95/Nqv/RoGgwF++Zd/GY1GA4899hj+7M/+DKVSye3zm7/5m0gmk/jIRz6CwWCAD3zgA/j85z8fqcgdEBAQoODAzwVHgRNhBHBCINVqFZlMBkdHR06sMRgMXCFpek8q1KB6kYu3chFKhg2z2Wxk6R4tvqsV65PJZCQfFo/HnRKRXmLAchCbX8JZc+12G5VK5W5fRkBAwFuETCaDD37wg05p+MADD6BcLrtq8VpL8JFHHsHm5iZKpRJeeuklNBqNU6uKz2Yzt5Ar12ejglHrHW5ubuJb3/oWDg4OXP6MKzNohQ1W6aDcnnUSk8kkNjY2UCwWkclk8JnPfAbdbvcuP82LhVar9YZSQaG2YUBAwIUHiYDzqnK5HIDosjMMBa6trSEWi6HRaKDZbLo147LZrPO+SDAEVYgkLV3/jRU7uAK4qhNt1Q0AkYVYc7mcV2If8OYRyCsgIOBSgMSSyWTcat6saDGdTpFOp1EqlVAulzEYDNDv9xGPx5HJZFxYj6RSLpdd0VwSCoUa+Xze1TYEjqt3jEYjdzyeVxev1ByayuO1xFTIdy0XgbwCAgIuBZQQEomEWwGZYcBUKuUqajAnRZJTGT2JZjgcRspLMR/GXBirwjO0aAvyarkqXQk9k8m487AiRyCv5SOQV0BAwIWHrpCdyWQcefCz+XzuCvGymgb/16K6DN8BJyuTT6dT1Go1NxdsPB5jMBhgPB67uoYqzmDocjAYOPUi53fpnDGWqWIBX7tiesCbQyCvgICASwEKKqg0ZAiPv1OplAvRUeGnld/T6XREbagV31OplFseJZvNYjgcot/vY3d314UX6V2xygZVhiQwnZzMeV7JZBK5XM5J8AOWh0BeAQEBFx70eNRz4ucMCXLZk6OjIwyHQ3S7XVfdgrUOWS1D6xwmEolIUd5Op4PRaITBYIBGo4F2u43hcIjpdOomIVNqD5yUttM6iTpJmdcdPK/lIpBXQEDApQBzSbpSslbN4JIkw+HQhfhYViqTyaBUKmFjY8NJ5LWeKsmOP1yYstlsot/vu5qJFHEw90axCL0snauqFTd43YG8lodAXgEBARcenN+VyWSc98VQnMraB4MB0um0q95TLBadsILhvNFohHa77XJZ/CFB8RwMT3IOma4DpmIQkidDlvP53IUqOcGZk5cDlodAXgEBARce9HqYUwJOiuQCx/kvCjS4iKSWk+KcK87TSqfTGAwGTvKeTCadWEMrZBSLRbceWL/fd+IPijN01WRfoV8KTEi0wfNaHgJ5BQQEXAqQvHTNLJIBya1QKGBtbc1tRxFFKpVyJZqOjo4cabHSvB5TS05ls1nnoTEcyWMCcERH0tISVqo0ZHHfQF7LQyCvgICACw+SE/Nei9YMTKfTyOVyTmlIryeZTLpCvrFYDNeuXXMkNh6P0Ww20Ww20W630e/3XYhPlzYBTuorTiYTN/mZYUKei2uP6QrLvJ4w12t5COQVEBBw4cHqGjpBWJczYS5rb28P3W7XkRs9I1a9oPeWSqWQy+WQyWRc9XiWchoMBq7axmg0ct4Zq2VQScgcF69Jj10oFJDNZpHP508JOQKWg0BeAQEBlwIkDYbnNNQXj8ed3J0Ti+lVcW0uAI5wGEbM5/MoFou4du2aIxwNA+r+JCmdgAycLGpJL43z0SjRp7eltRQD3jwCeQUEBFwakDTs4pBUG06nU0c2unoyvx8OhwDglj0BTvJT1WoV9Xod165dQz6fRywWQ6vVcmSo3hNl+vSqOEma1Tc4p4xLs8znc4zH44jIJODNIZBXQEDApQDnSrFaBsOHJCpVAgInCkTup+QzHo/RbredZ0Uvq9lsYjQaOTk+yZJ/azFeK8Bgjq1arSKXy0XqKKrXGLAcBPIKCAi4FNDKFSQU5rN0UrKWg8rlcpFwn1bjKJfLrgwUi/RyuRSGCXksPb79mwTFUKEugQIgQmABy0Mgr4CAgEsBhgdns9kpr4ehwel0imKxiLW1NWxsbGB9fd3NwyI5MdQ4Go0wGo3Q6/Xw6quvotvtOqWhikEon9fq8irTJ5mSuBhG5LkCad0ZhJWUAwICLjxisRgqlUpkIjBBgqFXRW+IHo8OcfZv/qgqUeeO8bfvM982i+oX8rPd3d0g3DAIKykHBAT8o8V8Pkez2bzblxFwgRD82YCAgICAS4dAXgEBAQEBlw6BvAICAgICLh0CeQUEBAQEXDoE8goICAgIuHQI5BUQEBAQcOkQyCsgICAg4NIhkFdAQEBAwKVDIK+AgICAgEuHQF4BAQEBAZcOgbwCAgICAi4dAnkFBAQEBFw6BPIKCAgICLh0COQVEBAQEHDpEMgrICAgIODSIZBXQEBAQMClQyCvgICAgIBLh0BeAQEBAQGXDoG8AgICAgIuHQJ5BQQEBARcOgTyCggICAi4dAjkFRAQEBBw6RDIKyAgICDg0iGQV0BAQEDApUMgr4CAgICAS4dAXgEBAQEBlw6BvAICAgICLh0CeQUEBAQEXDoE8goICAgIuHRI3u0LCAgIeOsQj8eRSqVOfR6LxSK/AWA+n7v/5/O593ixWCzynd3ffnYn4Dun7zvf9+d9/ma3fTP7+Z7fWcfwPeeznv3tvJfxeIzZbHbL278VCOQVELBCqFareOc734lYLIZ4/DjwEo/HkUgk3A8xm83cNjpo6qD6RskrkUi4Y8discjAGI/HMZ/PMZ1O3TXwR48bj8dPHZvH8ZEx953NZpjNZhFiPjo6cuez+/G8s9kM0+kU8/n8FInwuBZHR0entvP9zX15bP3f90x950smk0gkEu7d2udGcBu+A77HReR0dHSEv/3bv8XBwYH3+7uFQF4BASsEDlRKHIu8K36ugx8HfR1ULYHpYO/7XElvkcfH7zi4+ghSr9t3LYs8kXg87gjMPhN7Lt/5fNfqG/h9XtIiMuU16fkX7a/PhvuSjOw1LrpH++MzUs667ouAQF4BASsMnwexaNA/L2TF73UgXxT6soS5yCPR7X0EpoO/b3/rJSp4ndYzWXTeRfdviUu393lO+r+P9G4nPGfJyHf9SnC3izcaIn0rEMgrIGAFcd7Afda2dkCznpluf17OTENw5+XIOBj7jmkJ8yxPLRaLIZlMum3PIgsNL06n08h5NXzpIzAN/elnZ4U77XXH43F3Xn0G3IfH0negHq7vmvQZWk/aZ6TcShu5GwjkFRCwQvANYOdtD0QHQg1ZAWcTB3NoSmp6HEIHaN/g7gvlacjObq95HJs/4j5KRvb4vuu113OWp2nJSLfhtSWTSSee0VyjJeWjoyOXA2QOzXdcDdfa4/GcGlrkPvos7TO4yAjkFRCwYrCDm2JRmGuRl6HkZgfAs0JO54WyfOHLWzm2DUlaklEvyn6vx1Bi9nl7Po9tUY7KCigolEilUhHPyXqqJCz9mUwmjsQSicSpvKIP5xkpvmdw1j1dFATyCghYQWg+hERm80fn5Xr4nS+HRZwXktPBWwmDisNF59N99dzT6RSJROKUkEKPp9/xb35vB3I93lnPwu5D0LtKJBJIpVJIp9NO1ZlKpZBMJk+RmvUOlbhGoxHG4zEmkwmGwyHG47H7/iyPWo0Qm3O7yAR1FgJ5BQSsGKx82oaerFekXoBVsRGqTKRXw+3Vs7D7Knnaa/LlWuy+NuxFErDktchL4rUtGsCTyeSpY/muSb9LJpPIZDJIpVLIZDLIZDJIp9PudzKZdLJ2EhZJzXq9s9nMeVvT6RTj8Rij0Qij0Qj9fh+DwcD9PxwOXZiRsMe0UEPF3p/Pw75ICOQVELDCsKE79UYsfKRGLArz3UpuzXpIej77mQ1b8prU2/GJH+z57P+LQoj2+hYdi2QUj8eRzWaRzWaRTqcjf5PQ6HFxe4YQSTQa5iQZkbyOjo6c55XNZh159ft9pFIpR25HR0dnqiiB03nMRe/1oiKQV0DACuI8ifUiMjqPEOw+SnZneS7n/a8KukQicSZZnSe08P1vVZI+QcqiSduxWMx5Vel0GsViEblcDqlUKkJe/FHPS8UbNmTIH4YE5/PjydT8yefz6Pf7GA6HyGazSKVSGAwGiMViGA6Ht0Te+p7Pm+5w0RDIKyBgxcBB8TxJdTKZPJUTOcsqXyRisGTom+yr12ZFIDpXSXNBHOytt6jEZueD+XJXet0Md+r16LE0JKeeVaFQQD6fRzqdRqFQcPktDRUmEglkMhkvefm8X71e9cL492g0QjabxWg0Qj6fd55YPp9Ho9HAcDjEcDgEgMjUABoAFH343oX12sIk5YCAgLsOJQH+bXMtwMkgepZntmjyss2X6We+z33nP8sD5PXpHCefIlA9JRsaVC/HbuMjMx34SVK5XA65XA6FQgHZbBaZTAb5fN6FBpW4SF78W6XrPlm/vhuV9vPedbtYLBYh9Mlk4u6PhHuWytT37i46AnkFBKwgLAHwfxVsqEDDRzy2nNJZ6kDCkpolHB/pEb5clhKNkoD9To/pCyta4rKkxc8Z+svn8yiVSsjn88jlcigWi86rYshQhRhKXkrWSmSTycSdT5+73pO9Xn1HJCyGDTVfpvv55sRdRtz21Om/+Iu/wE//9E/j2rVriMVi+JM/+ZPI97/4i78YcYVjsRh+5Ed+JLLNaDTCxz72Mayvr6NQKOBnfuZn8Oqrr76pGwkICLg12OS8JQp6KxQIUCxwK9B8jZ5Pf9vPgZOwmA0bqlrRR2i+/BAH9aOjo4hSbzKZRPJXk8nEfW9DdUrIzEkVCgXUajWsr69jc3MTm5ubWF9fx9raGiqVCkqlkiO0fD6PQqEQCRumUqmIt7qIoHkPvG99BiQ7Hk9zaCoK4bVUKhWk0+lIEWYbEtRz2TJbVDhexLzXbZNXr9fDD/zAD+Bzn/vcwm3++T//57h586b7+dM//dPI90888QS+8pWv4Mtf/jK+/vWvo9vt4qd+6qduuYMEBAS8cZwVrrOeyaJCrrcj5uC5lGT0c53jtOh6rXe1aJvzCEHnTXE/kpd6XEog2WwWxWIR1Wo18lMul1Eul1EqlRxR0fvSuVvqXem1qoJwMpk4FaF+rjku+6PPT58Lz0upvpLX7eCizwG77bDh448/jscff/zMbTKZDK5cueL9rtVq4fd///fxR3/0R/iJn/gJAMAXv/hFXL9+HV/96lfxkz/5k7d7SQEBAbcIDpy+3IedxKoD3q3I3fUY/MyXy7LH1v18xGrDkovIS3+fFy70ye35txJDMpl0ooxisYhisYhsNuvChaoy1OdmZej22pQo+f9kMln47Czp2+fheyb00Jhz83nDlxl3JOf153/+59jc3ES1WsX73vc+/Of//J+xubkJAPjmN7+JyWSCD33oQ277a9eu4V3vehe+8Y1veMmLk/CIdrt9Jy47IGAlYMnLDrQ6+C4aGHkc3zEWEZBdEmXRBGKLW8mr8Tg2j+YTYFglnyW2o6MjN+iXSiVUq1Vks1nnYTE8l8vlXLjOt8CnDQ1a8iJh+SYW0xOlB6hhTYZQtcSVildsKSqV4fveCwBv1MtXcPgiYenk9fjjj+Nf/at/hfvvvx8vvvgi/uN//I/4//6//w/f/OY3kclksL29jXQ6jVqtFtlva2sL29vb3mM+/fTT+PSnP73sSw0IWDlo/siXi9KwoC+/pNvzNwdOwlZt8B3Pl8fyeWqLvCxLaL6QoY+47L1YIgOOJfC5XA75fB61Ws0pCeltkdhU6Wg9Vp57PB6fMhQ0bKmCCoYcmctSEuJz1pCi7959HpqGL/VZ2JwiyZDgtreiULwbWDp5/dzP/Zz7+13vehfe85734P7778f//J//Ex/+8IcX7reooQLAJz/5SXz84x93/7fbbVy/fn15Fx0QsIJQEjgvjOQjH9/fuu2i7fX8/PF5A/aY+p0N+/mIyXcuG07k/6xhmEgkIvO2OH+KQgitP2iPr/duw4K6vdYi5PkZemSeSr0lztGih0ZVIoUUlrh8ubFF1+r73LfPRQwz3nGp/NWrV3H//ffju9/9LgDgypUrGI/HaDQaEe9rd3cX733ve73HYCI0ICDgzUEHOlst4laS+mcNYjZkZ3NJVsmmHsOtVLlXUvSFuXz3puewknENW/I86XTaiTCoGMxms84j0onR3Jceiy2dpQpIAK7motYpZGkoldsrScbj8Uh9RRbnjcfjrooGz2HvlYIPGyrl8+E9LKpzedFxx8nr4OAAr7zyCq5evQoAePe7341UKoVnnnkGH/nIRwAAN2/exLPPPovPfvazd/pyAgJWGhzYaM0TNuTFv/U7i0Vyew2n6Xk1X2OP4Tuu3d/3N6/ThgY1BKZeiBVm8DP1tGq1GvL5PDKZjCvtRPJiuFDvkeSkJKjqRt4TSZr1CePxuDtntVpFPp93HpeqFJlPm8/nGA6HjtTG47HzvvT+OS2A5MVahzYkaAsln+Wl+bzZu43bJq9ut4vvfe977v8XX3wRf/M3f4N6vY56vY4nn3wS//Jf/ktcvXoVL730Ev7Df/gPWF9fx7/4F/8CAFCpVPBLv/RL+NVf/VWsra2hXq/jE5/4BB599FGnPgwICHhrcdbgZInjVsnFeiG+UkQ2dHje9fg+tzkfFTPo+X0iDXo2OjdLVYRaQFdJyzfNQMN06hHxnnXpmVgs5qpz8HxaMopenp1CYJdPsR4eCUsl+CQyhkYXwXrJtyqouVu4bfL667/+a/z4j/+4+5+5qF/4hV/A7/7u7+Lv/u7v8Id/+IdoNpu4evUqfvzHfxx//Md/jFKp5Pb5zd/8TSSTSXzkIx/BYDDABz7wAXz+85+/sInBgIB/7FAvxVY3JzFomPGsZUQIn4jDntPmxvRz/n2egOSsnI9u68sBUS1Iz0vJy3pBvCfrWXKgV/EFQ4ta9UI9pFQqhWKxGMmpKSlahaB9rqrYZMjQzhejZzYej53nxWPe6rwvnuMiel6x+UW8qnPQbrdRqVTu9mUEBFw61Go1PPLIIwCik1ntHCUdcK260Eci/E7X8FqE846nx130t+ZvrEpOJxnr55zwS5CsSqUS6vU6crmcE2vQCyK5KVnb58MfHp/Pg58pVMlYrVZdPt8SmIYk9fkOBgO37Mnh4SH29vbQ7XYxGAzQ6XRcMd5er4d+v49er+eK8/J+9PiL1lLjb5Lxt7/9bRweHi58p28GrVYL5XL5tvcLtQ0DAlYQVjxB+IQVACKDNT9fJLX3/e0LPS7KpywiN5vbsSRmYbdXgYKGCtX7YeFcHdytEMPm1WxYUqt36PyrVCrlCLNQKKBYLLo8mq7npfO+dErBdDp1i05yDS/+zd8kNpIWlYm+UOetKEwXVZ2/CAjkFRCworCDss7bsgRmFX9KPm9kYPORoH7uO6aVx/vUgnZ73zYsmEsPiB6XKv04cGvVd70GX3hShRsaaqPHk8vlUC6XI/k19bTUe2WFeJIbvTp6VoPB4BR5kbiGwyH6/b7LdS16luelaS4iYSkCeQUErBDUG1D1n4/EdB8OpoCfwLgdvwdwrkAAiBaKVU9Kj+O7NjuXTP/XY2iFCgDOy6pUKlhbW3NEwkrwJBpV+mlYUEOUKk8HECliTPLLZDKurFStVkO9XnerKXPlY11gEjghSObiEokEer0exuMxms0mut0uut0uDg8PnYelocJOp4N+v++Ow3XZYrGYW8PLNzdP3yFl/Ro6vGgI5BUQsGIgGbHigvUugJMBVD0wu0TKIlGFDUMSuq8N8+k+iybP8ni+HJlPtGGvgWttMc9FItMVjklYvnChnYDs+02SoIqQRXyz2SxyuRxisZjzkFKp1CmipRdGbzCVSqHX66HVamF/fx+NRiOSzxoMBhiNRpGc12AwOCVWuVUvSsOUF1WoQQTyCghYIVhZ+u3IoM8iGP1cicRXgJfHOouk7P++vBevf1H+S7eLx+OuHqHK01nVgiWf7GRk3zOwOS69BuBESVgqlbC2toZisejUgzr/yiduoYiG64BNp1O0220cHh7i8PAQrVbLeVqs+UryYhhR81z22n3P3OeF+QQvFw2BvAICVhg2bGhhQ4CqrlvkbSmUwBblt+wEY/5v5eCq3LPCCUvClIZzQjYrZ5TLZZfrUq+L+S4SGMmL8nNVNZKsdLmS+Xzu8lrVahXXrl1DuVxGsVhEv9/HYDDAcDh096CCEJ4nmUwik8m4qh7j8RgHBwf49re/jUajgVarhcFg4J7FYDBwXlir1UKv13OkmE6n3XnUUCExWbWhDQ3ad3IREcgrIGBFYYlkPp+7HJR6BPxcvRqby/KFEAmfB8bPdR9fqErJS6/TJ5ywHhgHaXpcJCuKM6z3pYs86r2rXJ1hNf2MKkJWna/X6yiXy0gkEhiNRuh2uy63xe3p3fG+kskkKpWKW35lf38fh4eHeP311/Haa69FyI8TjzudDrrdrgsh0uPSnBYJi9ftWzdt0VwyO+n6oiGQV0DACuKNLk6o8IWgFqnYVByy6Dsb9iOh2YUj1evyEZcOulTs0bPib/7oele+ortKytYz1Hlg1Wo1sooyvVSSFoUcvnlVSrDJZBKDwQC7u7vY39/Hzs4OOp3OqXJP4/HYqQoZKuRzPGue3e3kvi46AnkFBKwYVFgA+Jc4ARDxwizpLFKrnXde68Hpd2f9T+Ky3gCJzc6toldEUiCp6OKRJC+7zpiSlK0Ar6FLHrNYLGJ9fd1V5ZjP5y68NxqNvBOaeQ6SZ7lcdqHC119/Hc8995xTFmp4UmXy7XbbeVwqvLHQ90tv1HrWZ+13URHIKyBghUAJ96I5XhaL8lkajlJoKMwWzPWpDH3n8hGZTxxhhRKWGLi8iea2lLi0woiS9HmqRc4TYwHfXC4HAG6OFUnOhjYBOLLk86AKsVAooN1uY3d3F9/+9rexvb2N4XAYWb9rPB6j1+s5laGGCpWEeY2+EDDBMOJZ74Hh4ouKQF4BASsK9TB0kPJNVtbQ3huFEp5PvKGelO7jIylLYvo5RRrMX50XJrShQh9B6nNhKFJzZfo8gegcMB4nFotFCv3qKszj8Ri7u7vY3d1Fs9l0+S2GHCmvp/iDleL1mix58ZqtmvEsj8o+4/PCkHcTgbwCAlYQSkaLqmn4cCuW+K0U4bUDo3o6vknLGhbUYyq56N+6IjGJi16WnYy8SAGpKx4DiMyL47FYKxA48WaUoHUxSlUy8voYaux2u7h58yZu3ryJTqcTeR5cw6vf7zvxx3g8diSthGT/9kHv14YPbV5vUQ7zIiCQV0DAikOFFprzIamoJJ3Qyaz8X+Hz0M7Ka+my9r78lZIWw3I+z2s+nztFYT6fj+S2mPOyXpOqB/WZAIio87iGFsOGPAcnH1M+PxqNXO6LMng+v/F47EKZlUoF8fjxopJ///d/jxs3bqDdbrscFu+ToUJW02A4jwWDaYRoWSvrbfG5nZWr1Pu3qsTgeQUEBNxV6BwtDmYqVQf8A5oNq3F/G4byiTsIVRQuUvTpb1++yd6LHo/eDcnBN3fLFybUc/JcPB8L9fK+tE4hC/lmMhnMZjO3MCQrZ8Rix9U0SGQAnKy+WCwim826Cci7u7sYDAaYTCaOBJnnoqKQx+dzJlHx2fJ/6ynxXpXUNMfH32d51UEqHxAQcNdhQ3P2c0tGhC/ndTthJR8BWrKzgg2b61q0HXCi3PMtJmkXlDyLPK0wg6tOT6dT57UVCgVHXKlUygkngOOahDzW0dGR81CtV5hMJtHr9VzZp9Fo5A0Xao6L18XfGrJUgtZndZ7XdB4xXUTiAgJ5BQSsFDT3pBOASUJKKjYfpvOvFGcVblXl3qLvGYLktfhISnMwPKcSGz2tdDodCRdSaWgXllwkK+fxmJfKZrOR85F4KpUKSqWSK+PEH5Wi89mp1F0nKB8eHuLGjRu4ceMGut1u5N0wx9XpdDAYDNwxbJ6O92Wl8j4VKJ/fIhJfhCDYCAgIuDDghNnzvCZrdfu8Fp9qUM9jv/PN92Kuy5ffWrQdcLISspIXRRAqkafHpDL5s9a4UqGIekP6o2pG1iPUvBKPzwnGFFoMh0PE43G8+uqrTl3o87jodansXsODGkJcRMbch/fE3+epDrkdyfjNqEzvFAJ5BQQELMR54aez5oedN+AtIi67n4b1rPpPc1wkKBKaLbarHpKvkr5et3qCPgLTZU+oOFTitfkp4MQD43d7e3tot9un5oZNJhMnjad4A4iqB62i8FaIyP6/yEuz78R6wxcFgbwCAlYQNnSkoS6fZ3Ve6EitezvQcVC3MnJur16OnWdkj6PXxDW4crlcRBJvJyPbCbzAaYm4npPzp2azmfN8+D3zW+PxGJPJBJlMxnlxOqGYa3kpuVGpSAEGZfEkrel0islk4iposJ6hiinsFIdF98F75D4+aIhzES7qWl5AIK+AgJUDB2lVm3GA02XfNSR4njCDg6mKE4hF84gsQXFQ1muxsnkATpShoUJ6Viqi0JCe/jD3pfm2RaWnVJLPGoTpdBqxWAy9Xs8dq1gsOoJh9ffZbIZ8Pu8IggTFXJaWdqLEnrmufr+P0WjkzmvzaFb2rs/KJ7KxHpolPB+BMe/nK+Z7EXDxriggIOCuw+ayrALQbmu30YGWsASh+6vsXb0xfs+BmZ6MkhQHWBsq1GoaKpfXfNF5z8BHsPR2WOF9PB6fkqnrWlt8HgCcZ8ViuxqG5PG4Vpfmuuwz1evQPJitsqH7qAdtf/R7e76LKNYAgucVELBS4IBkw1CEVj4/ay7UorlaZ4Wo7D76HY9hxR6aC2NZJq1XSFLSsKEtBaXFeNUL0ukCqjJUqblPQAIce3+z2cxNSKaAIxaLudAiq71Xq1V3bK6K3Gw2MZlMInklWyler8eKQJSsbPgwFot5F7okzhJ2WLK8qMQFrCh5ZTIZvOc974ksMcBaYuPx2FWE9kmDfUlO39++OSO0FumOE2yAmUwG2WwW9XrdWYdsxOwUjFMzCT2fzzGZTLC/v+9m4PO8/NFrYidheRkem5Wtac1ye14/FV2bm5uR2Dyvx5YY4jHs88nn8y4k0m63T1ndNnHP61UrVAcThoooWeZ+nBjKY1YqFZTLZWxsbLikuSbjfdCBMZlMunBPu92OXK8+b17n0dERdnZ2LlzOgIOe5jt8RLRIas3vddDUd+bzVubz48m9GoLjfr53yv8tqWSzWRQKBUdEKsbg/wwl+khNi+Lq8+B26h0pYWpbYekmLhrJtb/G47G7zmq1ik6ng/F4jL29PTcHTNfg6vV6kXDhYDBAt9t1+S6ba9R5ahr69M35su9q0VjF/88yPC6iUINYSfJKp9P44R/+YTdrvdvtOtVPp9NBs9l0ah8gaoH41D2W0FR5xEEukUigVCqhXC67DqVufzweR6lUQqVSwdve9jY30bHRaLiGmslkMBwOkUgkkMvlUCwWnfX33HPPYXd315WX0ZVk1co+OjpyMXW17nK5HK5fv45isegqBKj1WSwWUa/X8fDDD7tlyDmIs0Np42fVARIJcyaVSgWDwQDtdhuvvfaasz45kGnSmz8kZX6v4HMolUqRzzUHkEgk8OCDD+LKlSu499570el03JpIfCaa/+DASiu/WCwil8thf3/f1Z9TklLi0rWW9vb2LiR50Qiy8muLNzNwqRWvBKnEdZ6SkW1Q27+ueqy1Cq0cXmsPamhRz6X7qxdmrz0ej7t3y4Ugx+MxCoWCMyRpZLFP0zucTCZotVru2DTctL1ru9FFKxc9Fx2LbEhQn/9Z+Sx7POthWvHHRcRKklc2m8X73/9+t6xAo9FwstWDgwNHAvQMODjr4KYWi200+p1OLlxfX8fm5qarAKAWVCwWw9raGq5evYof/uEfRrFYxHw+x2uvveZm+rOcTDqdRqlUQr1eB3AcX//Lv/xLfOc738HOzg76/X6kIrVWvSZBt1otNzEykUigVqvhHe94B2q12qmKAfF4HBsbG7jnnnvwQz/0Q+h0Oo7kdYDR55HJZJzFmk6nXRHRzc1NjMdjHB4e4tlnn3VJaxKuLnHOz7nEOTu3kvH6+jrq9TpqtVpE0UZLPxaLIZ/P473vfS/uv/9+3H///Xj99ddd1YL5/GTBQHq/tLZTqZRbq6lcLuPll1/Gt7/9bbzwwgtObcZjcACiYdDr9fDcc89FnuNFAJcJIXGrwWFDiIvCfDyObgecFnn4RASLhAG+KAeviQSkAg16WlZdyOiF5rl8isNYLBbZh9euc9C4XSaTcW2p1+u5SA0NQ0ZA2BY4kZmijWaz6Y7X7XbdvjR4GGLUXJcW/OVz0AiKL0/lE2qch0XbWW/sIpLYSpIXGy4tH1rvACIW1Gw2Q7vddp/7kqE+qLWtL58NVN19NuBkMukaMQdx3Z9EwJAGz88KAMVi0VUW6Pf7pxq/hrg4SJPgSKwMyeh29MrW1tYc8ZC89D59817oFZJIjo6OsLu768KTuVzOhVt07owOCLPZDLlc7lRpHOA4rLexsYFSqeSeA40B3nM2m8W1a9fwtre9Dfl8Hq+//jp2d3dPybLV+1Pjgx44CbXb7UaIVq1nbkvP6yJ2+EQigUKhgG63e0rxx2frU60R/MynAly0z6JQopIFcNIGVLSRTqeRy+XcD8krm81GQoOUzZOQtB2xPbAPU9yhIUj2QxoxvA+SJ89BY7TVarkIBQBUq1V3nslk4saYcrmMRqPhJPc66Vi9dRo83FeVftZoXjTuaJhc3/dZ3r/1jnkcNUAWvde7jZUlr1Qq5QZubfQcVEejEfL5vPNO9CVzYF603DlhB0cNi9nBmsREi15zEqrS0n3ZWDOZjAttZTIZd37mgBgW4/Vb9RVL6uj9kIAYrqHVSy+IIUeGXHQypd47z8VnQxJmXowkpZ3OhlOn0ymy2SxisVjkvAzpMccBRD3ffD6ParWKhx56CIlEAqPRCL1eL0KEdlDVgYfH0RAmiUnDnVo9gQRGK/yigQM6Q7lsGyTtRR6QEo16ANZCt4MlP7e/NZel+VlLpmx36nFp6NCu28V26xvo2Wcp4uDE5kQi4Txk5sA0N6fvMZlMYjgcotvtAoB75+yH2qbZhnluGjw8ng2Pa59R0tBq8fq3wkcudtuz2qOV3isuYjsGVpS8ALgF4DTuzfV1MpkM8vm8qwhtZ7n73HcL7djcTgnIpwbiIMjzqTJKj6EqLDZoklc6nXbXacNAhFp28/lJsVAN+6jFx8EikUjg8PDQWYgcOGKxWCTHpr+1I/L+uK3OwQHgvDB9zkyG874YBmVIjxa4rZjAPN36+jruueceAHBLqNtnq7X97HuZzWbO02KIR/NlvG6VTTPceRE7PQdnbdcM6fLegdPtXNsfSf8sj0rzvWcRoZKafV70eHQul4+8VLChg74N4TNioQpEW1aJ+2lIVa8rlUo5RSDDh/Syy+WyWyJFoyTalvQZsZ1ojlqv26YVbIFh37vVd6f5sNtpi773dRGxkuTFcBvJSa2xfD6PUqnkOmqv10Oz2YyE8tiglBx8jclakhoOoUXPgYDWnw6M9AB01VQOpoyb87gULRQKBe+5CR1cALgQSj6fjwzm2nHK5TJisRj6/T52d3fdtdEKVZJhSIReE5PdOkjQ+9LQIH8rsavgg9vqchcMFWpeg/vm83k89NBD2NjYQCKRcCIcWsh8hr1ezw1qfO+8FiXaQqEQ+V+fpVrV3F+J+CKBnhdzkjZ8p21a2wlwujYeoQO0VYX69tf/NSrB/9kms9ksSqWSMyhZyZ3fcTDXwrvaR23YXImbhpt6nD4BhHots9kMhULB5bd2dnbQbDaxvb2NnZ0d1/9KpRKm0ylGo5GTxPOa2O9JfOzvk8nEnV8J2EZ5fF6vpiFIWnrdvvJOdryynpx6zz4D+CJgZclLpaZswOqRMCzFvAy9Bpsf8CV4rSVH+EKGatWqyILbqHXMDk5rjaGwwWCAWq3mCIyCi0WDhTZ8PgeqHwntGKwSMBqNXDiNoVfeI8mf4Sgem6TGMOh8PnfKQb0m/c3Ors+U7yUWi3nXaeKxaIhsbW0hn88DAA4PD12uajweu4Gbnpgq2Xz5Hw7I6pWqh6kDr4YSL6LFyuvXEJt6XMQir90Hm7dScrdemZ3TZUOG9MgZvteQoYYO2V5tTsj+VmLWclE+r5I/9r3pvWkNRa61RcXydDp1E5B5bBoI6n1pBII/Nm+oBoTeBw1L21d1OyVlfm9xVtQIOD9XdhGwkuQFnAza+gLZ4DUUkc/nXQ6DDZGdTTu3HbC1wdiOrfvrIG5zYtZq0oGRf5PIYrGYE57Qi7OhSx0gCBVYACdERPBZAMdWtYbpaL1yH3qj/OHz1Y6ihKxEajufHVA1bKLLUNiBiNMIKpWK88SoLCR5MUenFi+PT/LVgdfmKmghc4CwobKz5o7dbeh98p3p9dLS1+19Rs0iUtKBVr/XzxYZLsCJZ8j+Z4nLGi3abvWa9V7UEOO9+9SSmmP1HZvXRs+PJKV9kOFj3U+vgeSlYWg+f+v5cXvrCVsFsc/QUMOPsIaZb5vLhJUlLw6CWlgTiE5apCKJlhEn5WpH9Ik2tDOoBaQDu+9zEpMd3EkSOtdJiYuDa7lcxubmJorF4sKwlY3D8z71uvk/PU/uQ3VfoVBwFQ4IDfPR66BnywmbDKNQ9s7jKpnzPYxGo0heit/Z6gnqecViMZRKJdRqNeTzefe8dF96GrxPqk3ppfF98Lny2L5J3YlEwi1voQRKy/qiel5UkE4mEzetwhIuvWQduO1xiEUWuhpK1khTwuM74iRjti++F7ZRLuBIT1mfrw1zqeFHQ8q2H/W2NPqyyKtRpSLbPufM9fv9SOiUYiklcyv80RyqEpDNWal3SeKyBKsqSd33LNiokf2O939RsZLkxZAX5yBp2RgAkUbK8MXR0RFyuRx6vZ5rQNZD8HldNuavg4QNNaoVqrP+KZ9X74Nxd15np9NBKpVCtVpFtVpFu92OyNB10CAZ6/IRJE4OzHofDNUxz8CYvcbWrcRZw239fj9yXxwsqNpTT1J/aMHSWtZQLwc7/h2LxVAsFp2IYzY7ltjTGlbi0cFN13fi5GVeuxWT8F3rNAO9HhorOjBfROhz1NyP5rz4Ww0si7Msdt8zsGFCHlONRUYPmNvUivEMF5OEtB/p9elAPp9H5/7ZeV9sq3octgHuq+2Ax8hms9jY2HD9+bXXXos8RxvxIHlpVEWNT21D2qZ9YBteRFC+fBaJ1BopPq/N7m8J/aJgJckLOCn9Y2PgQDQnph0rk8k4CbSN89+qpWPDZb7G6ouRk1j4w3wXw1+9Xg/VahWZTAblctl5GRrK0nOqtctGTdWZDmC0zoET65bWK/+mZajhEQDOsqTHOp1O0ev13HIPzBeQ5LRjEza0YcO8+red9lAoFFzokIMEn5caHurtKdFp8p/XSHKnNa8KMBveuajQNqQhYiWURaEl60n5LPezwoY6mHN/VflaSbyqDLUahhoher025KsEpKFSvT/2DRvappGqS5uoMpnzuPr9visDpedXY8beu5IZr1/bohKGfu77Xve3Y4h6loyG6Hv0tQ0e76KGvomVJK9Y7GSSMskL8IcImNzmnCJ6MyqIsI3OnkutMEJzQjyHkgB/dOIy53DF48fzX7rdrgtdNZtNN1HzypUruHHjRmTJBRuaU88FOD3pNBaLuQ7M4qOUp1MS3O/33eRRlqri80skEk4U0Wq1HGG1221XPYMhK/ucuL8+X31u/K33xP1TqRTy+Tzq9XqkFJdWMtjZ2XH3rB6TepO8fj4PFZ3QWwGOQ4+6rIW+w4sK64Xo8+XcR4tF+d1FHhbPYz0tJT+d/EsPix6Xb34XSUNDvzyPEplVPDLEnc/nneCCeU+tCs9IB9s886EcIyiD1/xbtVrFeDxGPp93/Y19wLZVzVnzvFbBrIasz7Dlu7Oelx1/lLz0fShpa+rCQo3ci4qVJC+Gzhg3Z0fh4AWcEBwHPlryJC+dl8GBSxuUWoI8nnpBwEneTaW+wMnSEdqJ9FzsUNVq1VmJ9GhKpRKuX7+O559/Ht1u18XirYWsnYAdlgOzLaVDC7NSqbjyUSRcltahykrFDSouAY69NFbX4HPs9/sRxR6fOz1LS7ixWCyi+NKwZb/fx87ODtrtNprNJur1OgqFAsrlsqumwKkQ3W7XTUDnRFNfG+G75RpNBAcbFcboj/UgLxLUwCAh8B0wzGqfuRKWtg0lkUVGmn7O7bUyPCXwbE/83E4ktiIF9bqst8v/eY+5XA7VatXlqkgebJ86xUS9j1QqhUKh4PJwdhkWGrasY6ik6ct18byUydvoC+A3gH3/Wy/ZvgP+WNGXHmNR3tDiIrbllSQvvjjOHdIq1JocVaueA7vOEVLBge3g1mKx1g8bveZtbOiADU/zKBp6YOiORDccDiNJ72KxiEaj4To9r5PHtyEUax3TOi+VSq78lKoNtS4cLWklLhti4v1zACkUCi6/oCEOX9iFz0O9Ymut0vPk9fE6WGCZn3MfzeHRQOH1apiRBEeDhWFEvU8drC4ycQGnnyOfB71ger1qLBE+S9ySls2p6P9s74u8Kv2bP5rz5Hl84UwlNQ2RKyFq/of3b5FOp12b4H62yK/mUvmsdNxQQ5W/7UR2m4cFzi47x+8t9PyEj5x87+wyYyXJCzjxegBEQgFWPswcB3+YrGeZIbW0gKjlZt12fkaC4cRgDV+phWlDLvQOOGhy8jAHHU64ZeFeTrrlNdicmfWy2MHU0s1kMqjX68jn8y72z0GGZKH7J5PJSNFbJSN2ag4oicTx5GF2eL1vfqZ/08LWgULzEJVKxQlsSqUScrmcC//Ri+P75iCUy+Xc9VcqFfcMuOIAf+vaS/oOLHH58nYXDWpkqcFmjQQfGWluRbe326jBpdBQIb0qm0eyBKayeNuOrMHD69GwLvuarnQQi8UiZbLUsMvlcs5j4n56fezf9J40MmKvU70uW0ZMPVw1lAmfx+XLawEnfc/Ckrx9n/rdRQ8TWqwkedGSphdRq9XcTHiSksaiNZzGqg6UYXNgA6LLJ9gEKgBXJaBYLDoxgXY0HYxp+evaW1pHLRY7ntdFzGbHRYQZKszn81hbW0OhUHAlkbRzAScDCT04X9iLRMeQmxYz1RI78Xgc5XLZDVicwNntdnF0dOTK6WjRY80J2IHO9/w1VKklgbjUzNramhtsdDI1SY/hI/WYO52OI1saEYlEwq2rRkt5e3vbtQtrcWs+46JDDQng+Pmyiot9Lxb0yOLxeITILfgs9FjMGZFMaFiwbamHYwUciwwwtlW9bhoZ7Ef5fB6VSsWFDPmeqTTlM+l2u5G5ZTrdg1GSQqHgPHXmmdvtNlqtVkSsoRESrabBii4M8QNR5SD7Jr1DwueJLYruKIkvyr/qM7SGlhqi6hGe5Q3eLawkeQEnMW82cA7y2WzWSdBt4lTVZcyRAf7Z6urhaKhQq2Nrw1Lrh94CRQQkMuAkGT2fz10Oht4W8zTsiBww2LE0R8NzAadVkBpTH4/H2N/fR6fTiUwrsJ6b7/nwOLVaDel02ok2tAwWr8OSK4UD/G3PqfkPDpRa4qtYLLoBjoOVhp90YKWXyGcznU5xcHAQ8aY4N00HAiV59bouuvVqrXt62jS82C7tdnyGGg2wUGNM87PsZxoaVNGIT1VIcYYSlV6PGlv6ngE4Q7NaraJUKrnC0yQj9ejVUNJzsh2rkUbDjOvZtVotNy3FhvXZNmyei8Ro+44NHyr4vb4TkpyN/uj2Cpsj0wiRNUQuIllZrCR50fNitQUmZPv9PrLZLHq93qkkqg7O9Fio/tOObAlL80q0OGllqvXK/VQIoHXP2OBpzQFw5KV5pNls5ibO8t46nY77Xxu7zRfYwYH5ov39/UjH0udoCZ7PheWqWPVdw34kYBY0Ve+IYVW1PvVdaE4LOJE1k+Q56DKfBZxUQNB3qHO5NNTDYzUajUiJJ815acfW52mNgIsKS0o6iJIEdGBUb53eqSVx3q8VBKgIwxcm9MnPdRK6fdb6XK3xQCIiQRUKBUdeuVzO5WSBkzyxbz4h2zXJix4+jSpOTeGafyQvvQbNdTFsreQFnLRrVR3zvjQXp3lphYYb7efWW/W1x7O+U8P2omJlyavf77sOShUcLexWq3VqYiIte1ZT58RlzdMAUWtGPZBEIhERPSiRqGfBhkxLjQM9B1YdQBkeZCgTOCFZzqEql8vY29uLkCM7Fe9FVWeaX+C2dg0yHezV6lUvkfsnEgmsr6+7iaepVMqtB3ZwcBCRo/PZadhVOybvgXk2EjmXVm80Gsjn88jn87hy5Qq2trZQLpdRrVZdfnA+nzs14uHhYeS58hoymQw2NjZwcHDg1l/igENvUK3gRYPLRQSNKH1HNMaULHSSrvXCuK/+bw2fROK4TJcNM6v3o5PkbdhQK2DwmDyXJSztI4nEcSHpWq2GtbU1XLt2LbI6uEYf2Ecmk4kzduLxuJsaov2cS/EMBgO0Wi3s7u5ie3vbLS6pITtdWYAL3vKHXr72Ndt2tN3btALvYVGIUd8z9+f7W5RPs9/Za7mo4fCVJC/gpDQQ6xaqiILFeDX+yw6jREYS4gAIRMN/HBg0ZMgBXEMTwMmkaSaQaalpOI25KYKDAa9LiYmdWRPyvD79W/N5qu7ydSh+rglstdDYkXW9L+aVuEAmBwc7oVNFK8CJV8e/7QBoMZ/PnVXL1bFffvll5HI5R2BcfZqijkql4qrN65wbvk8+C85Jo0W9yAO4DJ4X2y0HcBUN2PmOgF+G7cu1aH6RbaNQKDiPR8OEDO9qaJtE5ptIbNsrr1vnTfHeOK1jfX3dGS9sL4PBwEVdGMLmvjplRQmCoioO4q1WC/v7+9jb20O3242EATV0yXGFOWjmfDWPrPfGaICN9vA7QolL3wmfi81X67Y6NnHfRR404fPsLgpWkrwYKmE8ejgcOlJRdREbBHA6nkzrkbFra3lqqEvzQBryUgtLJcscPEmgqgjkuWKxmBN9MFwHnIQN9Vo5ICzyaOx2Gr7Q58XwD0lYF4GczWZu6fNsNot2u+2sT12YUTuX/k+S9j1HAKfmdflAD4nPj9fNRSnr9ToGgwE2NzeRTCadVJ9Jfh3QSV58HlQrage3gwVh/79IoOelK3OrAQNEw8g2TGqfgRphPA77j4YJ2V+0nXFbrTGqeVMNf/FcChIX+yANRBorrDijoWU1DBnqswYcz8VnxTYxGAzQbDbRbDbdvD8NEWpb5vkGgwEGg4HzxpRQfL/1Pek96/8aErTQ92YJUo0Muw+fp57/IocMgRUlL+DE0plOp26gBeAmtPb7fdeZgNMT+DQ/pasXa2fXgZaJcV8sWRsvLX0OLhQgqEXGAaBWq7mBgp2Saj6tVl0ul09ZW7QSubQKlVQ8j3pvLDlVq9Vw7733ukmlqpbks6RVe3h4iIODA+zv76PRaEQ6lYZYtQPyGNpRVe3F58dr4za8Zt47j6v5ht3dXadse+c734kHHngA999/P97+9re7vMXOzo6zkPW9ZbNZNBqNUwlvWvG8Hnb4i0pcwLHxU6lUIqFACpdoCKmS0ua/+FyB6CoMbA/6rtQI5LO3uS2diKwGmk9cozkl6zVQdFWpVPDQQw+hXq+7qi8sRdZqtSJ5KLYX9mGKqXgNNHy4dM6NGzfwwgsvOFWvTmynBwecGJCdTscJOugp2nmd2q9VhMJ707TEoryXhe6r/cF62rqvvlNey1mG4kXAypIXOx0AZxUx7LC+vu7CT+1223UkEhyhIgId7DURysbDYwPR+mvW+4rFYq74byqVckop5gWYQ5vP547UgOOQCOPzzIFRksuJ1Vr9wg6wHKjUA+E9Xrt2Devr606EoeFK7Rg2vJlOp1EsFpFMJtFutyNhUCUgC9v57FwyG9bkoEmFIY/B9zoYDNx1zmYzvPDCC9jf38f3vvc93HfffahWq8jn89jY2ECj0XD5UOb1bPUUffc2J7fIs70o4PuhR8L2wr8ZTldv2Ge40YvykRe/03Nqnouelk9VqEacFe3Y0CwJkarCer2Ora0t51lTecuwdafTieT0ZrNZhEw1VKiG0s2bN7G9vY0XX3wR29vbrkABcHrqAdvLYDBwpKfhQotFhpzdhm1Ro0EK7dPWqOT3fHf833palw0rSV4+wlCrjupDDcv5Ep62wxF2UNawocbsgZNGo7/pDTFvoJYsvTx2Bk1ak3y0YoVaVCRQ3U6vUePmDMXkcjnUajVUKpWIx6XeDfNTGmrQCd3FYjEymdNagUp+Nhylz5f3S6swFou5Ca/MVaqlyXP2er3IsiysQdfr9QAcGy9aOoiDp96jfUe+905c9IGAhGOVfgzhaT1MQt+xhtlUgau5KktEKsSwoUO19nl8NUJ4fpuH4/WXSiWsra2hVquhWq06Y5KqQOa3WIFGQ3HMAWrpJ04HmU6naDab2NnZwc7ODg4ODtyipqq6VE+JRhPb3q3UQGWOzY4x1jDSd8HPbFpjUYjVHseGfu2x7T6Ljnc3sZLkBZwINigGIAFMJhMUCgWsra1hOBxie3vbTeBMJBKRqhrsmFqQ1YIdVZetV/WadnZ25KOjI0eepVLp1Kq/QDR5rklirSbBfB7/53VoWIjXp1U1iHw+j83NTVy9etV5qTooxeNxZ1WysKpWpeD1VKtV95md6GsnJvPYajToebVzJZNJrK2tYWNjA+vr6yiXy5FQk1a1f/nll3F4eIi9vT3n2fb7fbz44os4ODhAuVzG1tbWqbAun6ENAfsGWhV8XOTQId+9GkQckLmEPfPBBA0FJS4lMg05q+dkSU5JTBWFPiNFiUsjGjwOjZZ6vY777rvPKW85Ibjf7zvyooCCHhv7AmtfsuYmyYvzuL7zne/ghRdewMHBAQ4PD92Ef+BkbTwNp1KZSLJkCFoNLm0zgL+ivP3NbZSs6JlaUte/bX6Wx+Mz1ver215UwlKsJHkxj8Gkq+YudELrtWvXsL29HcmfENpoNXxkwwj8nnkotcL0bwBONk4Ca7fbuHnzJhKJhKtqT/WUkp0OuOw89DYGg4Gb38L7ovdGibDN2QFwJZZqtZoLJZJwWJlE4+Lz+dxJgZlPoEyYlmU2m0Wz2Tw1F4hemCaZeS/czz5Xqsre+c53RqYf6EDIAYqTlrvdLg4PD/HSSy+5a6Xn1+v1cPPmTTdAE7wOC3s9F72jE3o/+oyAk1qfnFDe6XQihppO6fDla2zu1lZDodeug7d9ZzacxWvmD4mvUChgY2MDlUoF9Xrd1SNsNBpO7aqG23w+d22E18f+xEo0vJd2u40bN27g5s2beP7557G3t+dWQND8robm2DdoLFF5y1Cfelb6v4X15rVPWPJRkrEeKtu1PkslJF/qwOIiG2DAipIXgFONbjgcugY5Go2chbixseEGYi77zcaijYnWvi83QstTBzrrRWh+hY1eLWO1XjXUpjJhno//s6RTLpc7lQTm9j5JMkMiPAavnSSvikS1QrXKAI+jxWxVycVz62RO+2ysd6mdnQOYzuHxWbNKxjqPp91uu1wIt9NQrrVE7bvi51Z9p/tfRPC9Egwt0xPigMg8lM6h8oWxlLSsJ2/nbWnI0OYtzxso6flR7VqtVt2K2fF4PJLfpJhC6wdaz4c50lKp5IiL0YqbN2+6n8PDQ0dcNoyqSkN6+fTUdQkdPa9VAPKdnHXfPk9Mv/e1TV6jz4s661lf1Hbrw8qSFxsjw20MBySTSSc1TyQSuOeee9x8jf39/QgJqeiCVqx6YwAinVhDH7otwQGUEtzZbOYquqs0nR1HRQnsxBruoXfCgr1KdGpRq1XN/W3Yhx17PB673BLvXwlIPU0gOtmVoUklUv2b/ysBkuAoM2bnpxiEYdDhcOi8Jt6Pkh89DD77RqOB/f19twIur1cHOyC6/IS+P71WKySwRHqREI/HI94Q3/VkMnGhX+YRE4lEZLl6W4iag7LK21WUwYnHSlya9/IZCTaMqJGNTCbjvKXNzU03B8uGqnUdOyA6n5F/ZzIZN3k9n88DgDNqXnjhBbz++us4ODhAq9U6pW5UY0f7Ic/Pyc+MOFgD0YpZCG1r1sDwEZf2GxoX1rOy+9kcmQ8Xte1arCR50btirosqJIbrWEyz1+vhypUruP/++1EsFnF0dIT9/X0XCqO1RstLBzftJDr5EYiGDHWiJTupdnzOSUkmk055yPNSYAAgUqpqOByi2+26eny+vAJzdeyUDBkBcHJn1ktUD5OdlNtQhdnr9SJ5oHw+j06ng+l0isFg4K45lUpFxBs276XPxXqaHDTokZZKJUf0DA+xSjxLZJGMuR0t41gs5iYp67LvfB462Pmgg8KtWrUXAanU8TpwQLTt0VPo9XquRFoymYyQFwlMq7zwOZGUVPig4gcrEaeaz+dlAyf5ZPW2WHiZ/UnrZLL/aaUU9md67cxnM5e7vr7uCjgzRP/666/jxRdfxOHhocuN2rahCj4+G/Y19lfeg7YPmyO1xGXzrYTPGFKPFzipc8jj6jPQfWnAqnHoO+dlwEqSFwmLpWsYOrES4WQyiW63i3g8jkqlgnvuuQfASUNhJ9YOowO9CiCs5cbfVjLO/djxuXiiVivQsKHG2TmwMN7P3BcHIF6HtXCtBWyfFa1fKx/nsyJ5kFhYaofEwQomANzz1oFABwY+A32OJDLgROrNyaj8jDkPhmpIUJrj4X2rl8TlL+zcMW6v4OCiuR21qC8DidnwmXpNFG2wXTNErFMorBKR9611Cm2IW3/7QubAaTWhTn/QifHASYia71LbkBo5SrSE9il65+PxGLu7u05V2Gw2Xejeeio2UqIRCV/dQl9bIay3RNgwvj4fHyHpu/Vdoz22DYffSlu9iO15Zcmr1WqhUChE5nOwY9KyOjo6XkG3WCwil8vh6tWrbpDWgrmaG9AGphamLzym4QObP9PJuVqFnuEzXgMFEkxO8zNVPvX7/YinQ/g6gjZSGypT4tQBX6sXzOfzyJQADnjAyaCkg4sS16KOb8lLByAqQZPJZGRZE70etUKZd+P1s8oGP+PAwWdln4/Pi/WFdC5iZycWXTNwsv7VbDaLeMxqpPkmCZOwSI76Du0PYQdZbWc0UDRkToKlkaLvUo0HbZ9qWNJj54Rm5sq63S62t7exvb2Nvb09J1TREClhjS16rVqpRZ+rzXmdRTr6bux2Oj7Ye7XHsGFYS3y+kP1ZocKL2pZXkrwmkwn+/u//3slsk8mk81A0VDSfz138nMV7H3jgAVQqFQBAo9FwijWSHwc8/lilEEHyicViTrbPgZrS7NnsuGrGzZs3HRExvMNwBXBiTetgot6Uekn2h/fKiiL84X6c5Kzz3thhtdYiQzT0Au1ABJzE8nlt1oJXi1NXbLZJf66Hlslk0G633X3wWfP6WF6Ly8JoWJQdl/eRSqXcZGZ9/1b5xoGARKyTWy9D6IUGhb1W9Tx1DpbmAO0cQ83R6G8gWkJN/1YsijgwD5XP59076PV66Ha7kUgAr425YC3JpBPy2e5KpRK2trZQq9WQzWbR7/fRbDbx+uuv43vf+x4ODw/RbDZPhdasp0iDjZU7+v1+pCSbFUP5PC8dG/g/z6Xb8l5t+N6SlSVun6enz5rntO/SB9sHLgpWkrxmsxl2dnbQ6/VQqVRc4p+5Lg35saFqXqpUKuH69etIJpOuojmAyDwwOxjbxsFOqMclqHxk49ZOqD++MJYdSNR7sudXL0+tMBKrKrZ0wGZ4hDXybHLcEo56piR6zZvoufW3esC+BLbG7fnsbVhsPj+pHk7LXQ0L3hdzcdr5fYOEPkc+D86T0oFHB6aLBLY5m+y3YgyCbcrmanyCCz2eCn50IPXtoxOmdaFKhuOsl8fjsH0BcG2Sv3mPOidsc3MTpVLJva9Go4Hd3V28+uqrTmKv+Sye13owJEmtoMFrWuRpquel7UqfC7fzhf8WEYwlWB9R+kKeetyzcFG9LmCFyavZbLrZ8hQnMAEMwEmFraAgn8+7ybFctJLkwoagFS7sedWi0s81f6QhNG7LQVnzArxGPY4OTovi2fZzHbB57SQuDYWozJfn0XyaJtuZs9NJwyrGUAva5jvss7LQkJfOeeM8PO3IOkeN58pms24fPS+3J/i8dTBRz4uDuC9kdt6gcLegYT/CTlcgfOEu/q+kpM9IYUNmhBIBt6HIQxW1DIfbNmy9Pl67Tn3RPsNVkGu1mlPKDodDNBoN7O3tuVChFur1RUr4rFT4Ywvu2h/ts3rt9nno8/WFDG+lPS0KBfvuw+5zkUlqEVaSvKbTKQ4PD7Gzs4NyuYz77rsP2WzWxfgZ/orH427w5s94PHYS9HvuuQeVSgWJRMIVfqUHoKSnhKITPZWQGEYDonFtO7mRiMVipwQcOlFSvSHtcCRGeikkayWL2Wzm1Gb5fN5ZsMxlsfNz0U5awO12GwDc2mhAtEyUkguAUx4ncOJFaZJfa+Wp18mCwXy+rGjP90bLvd/vo1gsus8p1NFnQYLjNVrRAd+jJVp9Nxoiu6jkBUTX3/JZ7PR02bZo0PB+9R3ZCIMNefFzIJprm8/n7t1SjMT96D3R61L4jDW+ax5Pw8ipVAq1Wg31eh31eh2xWAyDwQB7e3t47rnnsLe3h/39ffT7/YiH6csN8Zo4+b7b7TqitGpKfSb8bYnJ3ov19m+HsG4V1mj1GdMWF9UYW0nyms/nrvRTsVjEPffcE5nQqHOHAJyqvwYgomxbX18HcFIIl0olHkNDYzYvoAQFnOSFdEDhtpSHswI26/Gp+pH13FgGqdVqnZpgqR1JPSMFi9NmMhl0u12nEuTEzlqt5p7ZYDDA7u6u+19zAjZcpB6lej7Wm+G16dwc/jBsw/ltJJrxeOzyMjqnLZPJnKo4z+du362GtPjOFlmnGpoEEFkF+KJ2eMK2ByBa/5L3bXMoalQogSmhL7p39hfNPfKZqbiC10fSpMHAdq5tHTiRrLN/0YhIp9OuAketVkM8Hndrvb3wwgvY3d1Fs9l0xEWosaleHfsWq9fQQFPC8/3YHNSbaRs6Lticsf5W+EK7/Pwy5GkXYSXJCzguxcQVdbXmHhCtNGGtOM2FqQdQKpUijVIFHJr418bmy0VZK5+DOL2MtbU1lxdgLT8O8PQ0SL7xeNytxqywVrfmFLTD0SPR/BdzcZy/xjwRVVy8X53UqvepIUj9znYwJVgb5qTQotfrufxIMpl00nkKbCjSYDiQ3pF91go9n/7oQL4opKSTcS8ycVlY74DPU70J3+CsRKZtynpY+tuXu2LoXXOhhE/ZSONIj2FzlSRhXb18Oj1e/ujw8BC7u7tot9tOiavQ90zyUq/LJ4n35fZ8z8N6V/oOfG3mVkKGPi/aQt+x3d6e57KEEFeSvOh5scpCp9OJzF9Ri1sVWBwo7YAajx/PA6MXxI6o6jfuo8lyHVBtUlzBeP3999+PjY2NSEFTgvOcqLCLx+NO8kwvBYgux0IvRoUOwEm+j4ToGzgoyOB5rl27hnq97vIUjUYjMuFbJ7ta61GfjXqEvD+GhDhIzOfHIozXXnvNPZtarYaNjQ23+F+9Xsfe3l7EAGG4V9+p9Ur1PvW6eK1qkds8kVaQWJSvuwhQBayGjoCT8BUHfy3pZcPZ3E7FKTa87RMMaRSCIWn2GU5xoJGo3rwaYXwP8XjcRQXYX9lXS6US1tfXUalUkMlk0Ol08Oqrr7rST61Wy7Vv9fitcIVtnoV+aRAqYam3rYRlw4W23y4is0Wwz1ffoxpNWolHz2sNRRK/z4C86Lgt8nr66afx3/7bf8M//MM/IJfL4b3vfS8+85nP4Pu+7/vcNvP5HJ/+9Kfxe7/3e2g0GnjsscfwX/7Lf8H3f//3u21GoxE+8YlP4L/+1/+KwWCAD3zgA/id3/kd3Hvvvcu7s1sAK0ffvHkzUlKGyjSGHdhZs9ksSqWSC6toopuVq3VwZsfXckv08HTODP+nhJueAgBHDOvr69jY2HBkNJ1OIzJtWoQM15Gs8vk8yuWyW9fIhhl0AOM5p9Opq9Jer9fx9re/HdVq1VUZYXI9Fou5uolHR0duPbHxeByZWMoipcPhEK1Wyw0UwAmZWi+QIcb5fO4IgVUWHnroIbfMu1ZqmM1m7troHVIR1mq13HPle2OCXnNeHCA112itVZ6LRE1yA06W6bBimosC9ZT0xzeIFgoF1xem02nEs+SAbe/TqizVa+Nz4xQMNbQ4Z08nFjNUZ2sU6rk4SFuyYLSCpZ96vR5eeuklvPjii9jb20Oj0YhUwuBxrIHCa2VtU82J2ryovQbfcz/re5tSOAtqdCgxKrFx3LLzFWkk6nYazr2I7daH2zIPv/a1r+FXfuVX8Jd/+Zd45plncHR0hA996ENuXSQA+OxnP4vf+I3fwOc+9zn81V/9Fa5cuYIPfvCDrqgtADzxxBP4yle+gi9/+cv4+te/jm63i5/6qZ96y+YTsJFSMXRwcBAplaQvkcsukARYa5ATl1OplGvgHMjYGKzwQOXDqkqzSkM9P+vMcbkRnZis/zPP5oNO8uT9673a8FgqlXL3TDJQoYl6rs1m06m29vf30Wq1XAfXElLacXge34++Hw5gfKY0KEqlEkqlkhOsMExJMuLzZyX+er2OtbU1FItF57VxlVsV19B4WSQS0PfiU+3pwGnnNF0U2LwroffB96x1CnWRSduGNYxoRS5sv6VSyfUfljlT447vi2IazePyh8o+DUnbXJuGPHmNk8kEnU4HN2/exMHBATqdjgt3a9hMPUIalOxbWqEe8AuLfMTkI7VF5OATetiQuQ8aklz0Ti3pa3jTd97LgNvyvP7X//pfkf//4A/+AJubm/jmN7+Jf/bP/hnm8zl+67d+C5/61Kfw4Q9/GADwhS98AVtbW/jSl76Ej370o2i1Wvj93/99/NEf/RF+4id+AgDwxS9+EdevX8dXv/pV/ORP/uSSbu1scMAaDofY3d3F2tqa84wUlNGn02lsbGxEyknFYjE3Q7/RaLjJuRxQ6cozv0MLT70NHksbnlqrlA8Dx5OiOa/FlwdQ9SGPx/lYFFbosvXamRiq4SBCcmDn52Rsih/43Egc7XYb0+nUCUnorbAqvq68e1YIhfdFS1s9Qv6vpYLUMo/FTiY3M2xLAi2VSmi329je3nZhYgCRyghaCcQSLIlKQ0mW6EhcHEAv6kBg74XQe6ahwOfMdgycTIWw0wJ4/xquogfOIrokhUQi4Yy90WiEYrGIdDqNWq2GRCLhFH0kLpsztvlLXjM9buZkgWPx0eHhIV555RUn0KBHqaFhNWpVDt/tdl1kw0dEvhwfP9e2tAhnhZit0nPRvtbwP0+Ioe+M1/2POmxo0Wq1AAD1eh0A3DLZH/rQh9w2mUwG73vf+/CNb3wDH/3oR/HNb34Tk8kkss21a9fwrne9C9/4xje85MWF5AhKst8s2HFarRbW1tbcy2McXeeGpFIpjMdjZ60DcLHvbDaL69evO4ut1Wqh2Wyi3W67AZ+FRIGTQZgeinYidmwOFBQmkGzVQyoWi24AzWazmEwmSCaPlwohYQKI3JN6DcBJNQAlVC1UO5vNXFV6zoljPuTee+91IcFUKoW1tTU3UPEZTafHJZheeOEFvPbaay4EYxc6tL85yGlIQ4mDnVrrOAJweS4VTsznc1eR4+GHH0Ymk8HBwQEODg7coESr3kdcGlLhc9CQpuZuaIWznNdFhg03WWtfSUwHeyC6AjmPoaFePi96XQwLag6xVCpFjDw1TLrdLmKxmCsAAESNOhqU6nnr5ySwfr+PGzdu4LXXXkOj0UC323X5Ku1z/JvhYP5w6Rxf6TI7cX7Rs1xEPHpP+izVQPXtZw1Xff7cXgVlSo5KrvRudZxYCfKaz+f4+Mc/jn/6T/8p3vWudwEAtre3AQBbW1uRbbe2tvDyyy+7bWhh2W24v8XTTz+NT3/602/0UheCgyAbKgAXNisWi1hfX0ehUMB0OnUTmrkNLT96Ohp2pBCCoTx6OWyU7LxqyWoDY2eiFahehQ1d8Hja8GgVHx0duVVhOQjx+m2SXb1JLg/DwUAbvHptvG+GV6fTqZsuYBP5DB8xD2WhZMtz6W9uo2uTkYD5HjgIal4CgDM6kslkJBenXgKNBkIHWR0QONhpG7K5AjuwXTQoOfN/JSwNJ6sn5tvHQp+HemAqZrIiGTU0eHwuf8KFSEl8wIkwRkNd+p40ZNjpdHB4eOhWQWb71YGb18I2pRU0uJYXECUu7bu3YqScl09Sw8zX9onzUit6HP6+VfHQZSIu4E2Q17/7d/8Of/u3f4uvf/3rp76zD/08t/m8bT75yU/i4x//uPu/3W7j+vXrb+CqT6AJWXp2bIiUvtfrdVfzjsugMP9kLTCtQs3QG0sO8ZloyFDVhbZxMeRFK5CkYiugq+hBQW+PIgpazQw/avhSLT8OzKxqoN6aEpyWdmI5rXQ67epDjsdjFwbSEJ4KTHjPPusQiC7xwOfH++Iz4n3rOmG8P9apBI6NgW63i/l8jsFggHK5HCE8zavps9D3BJwMxqrAtHnS8wapi4JFIVElJ+tV6T3pIMu2ws8JPQb7BHOTup8uGcT+wtwmBRf0sjUsy88150RvmEIS5mR16Ru9Vv0hqSpx0dDSMOB5ntQi2Gds+x6flSqVfV7TWcc/7zu9bu0v5+VBLyLeEHl97GMfw//4H/8Df/EXfxFRCF65cgXAsXd19epV9/nu7q7zxq5cuYLxeIxGoxHxvnZ3d/He977Xez6GAJYFHbDoedFbKBaLuPfee5FMJtFsNvGd73zHTcp94IEHIlUsSAqsDkCl33PPPYdOp+Ni+laQoSE6Xo92DpXYM+bOVYPn85OK9toZNL7earVQq9WQTqextbXlwo6ZTMblMOgx8UdDFQynxmLHa14xxMKBhQSmVQYYFuVgP5/PnTJzY2MDnU4H8/mxWAI4KY6rYUt2Ig6MSs5qGJC4c7mcs6hVtMJtWdOuXq8jn89jMBig2Wy6pL0qBTl4Ehoq9IkcSPSqFlViO08tdjdhQ0uLBikN29opIup56cBo8yYsnDydTnHfffehVCq5SclU+NIwi8ViLkxOL/nee+9Fp9Nxbc32ERqbbMMUfcRiMfT7fbz00ktuDb75fO6iD0pafG/Mr3EtPLYPFacAfmGF7/n6fiv5+UK1vmMtej8+orTECJx4a5Z09bjWoDzrmi4Kbou85vM5Pvaxj+ErX/kK/vzP/xwPPvhg5PsHH3wQV65cwTPPPIMf/MEfBHAcsvna176Gz3zmMwCAd7/73UilUnjmmWfwkY98BABw8+ZNPPvss/jsZz+7jHu6JSh5kXSYoL5586bLO9Xrdacw1JBZsVh0z2Q2m7lJj61WyxX9VTWa5k40JMWOqGSmDYxKRq6JZTsAO5/msDSMSS+R96z72capcXKW5uFAo0pIEojO3aLwgURAL4nPqdVqYXt7O1J1w1rs1tpUEqOFXi6XEYvF3DujhaxTFfRYKqKJxWKuKkk8fjyBu9lsRu5NBxUNK2kOkO9PrXU+Sx3ULrL3xfayaHDyEZN6o3YQ1Gen+9Kg4NzAer3upl0kkyeV4FV+T0NwNBqhXC47wc3+/r4z2jTMyHfNPsvzcooEq8xru2O75XOgQUjyZPtXZSUJ0pIA/1aCt8+S16relgX3Pct7sxEJ7qckaD87j2jPItCL3IZvi7x+5Vd+BV/60pfw3//7f0epVHI5qkql4iynJ554Ak899RQeeeQRPPLII3jqqaeQz+fx8z//827bX/qlX8Kv/uqvYm1tDfV6HZ/4xCfw6KOPOvXhWwGdZ0UPh8q+4XAYWfeHgg0NgXAZE6oNuYhdo9FwpWOsVetTJfFzbTjqxvPabOVqG37joML5YpSN69wtnd9hrWcgGicnCbDgrRY8pZdiDQAgKiFmGSvK0jn4MBRpwYHIWvXMb9AzjsVi7rlQQs3BitdNz47Pg947BSWj0QidTufUHB1VCSpRK1npfZPI7Pu71TzD3cJ5ISafhwWcGDg+C14Hbu6nMneG8vgeqUJl+2e7phKR7ZH5UkZI6BHynNxfw6Ccw0ki0oiALxKi7Yhhc7b1RXO5CLZbn+fpa+e+53XW9ha6nxKURgp813gr5GWv8Xau663GbZHX7/7u7wIA3v/+90c+/4M/+AP84i/+IgDg137t1zAYDPDLv/zLbpLyn/3Zn6FUKrntf/M3fxPJZBIf+chH3CTlz3/+829ZqEW9HzbcdrvtLPGtrS1Uq1Xce++9rvEz0V8sFpFIJNDr9fD8889jZ2cHN2/ejCx+yFAIOxYHNyaa1fPh9egkSe7HbTgwc1Inw3+U8ZM4GPoAjmX1/X4f7XbbiU3ojZEIBoOB65QaKgDgKmbTC9N8E69TSW02m6FYLLp8A5dbTyQSePbZZ/Hyyy+j0Wi4iu7AMdExPGmtWU1Mq3ikWCxiOBxiMplE5hdOp8elf3j96llono7q0Fwud0o0ZOffKeLxeGThT51Mq0Tuyx1cRPhCWmyXi0Lc/G3zWovyhWzzNDQmkwm63S729/cxHA5x5coVV76J9QVjsRgKhYIT1Ozt7bnFR+PxOFqtlhMFWQOR10tpPOt7aoFfG+al4UUVrE6eVvWo/lb48tb2mepnbJckWT2GbXM+D27Ru7S5s0XvQ/dRw1e96suC2w4bnodYLIYnn3wSTz755MJtstksfvu3fxu//du/fTunXyrYgDV3BBwPYJVKxQk14vG4yzfFYjFXVmZ7exsHBweu0QOnk+BANC5tVVzWulUlllpyOk/lgQceQD6fd3JgDqIkIh6XIVDOhaGkXTs8QyIkWE0Y87loCE9DLRpimU6nkWKuPDfFGs8//7ybVrEo5q5ejvVgeF+JRMJVueeAqCFRSqA1f8ccBsONs9nMLWK5u7vrBkE9H8ldoYMPvQItW8Tz8xld5EFA5yEC/tyJ3r9v8CPoSSlsTpegYIL1BIfDITY3N3H16tWIijSRSEQET6PRCP1+3y2Gms/nIyse8zpIupS3c56YDddbkQavRUPf2uascWWJalHuyRKbRm50jNDn7POClACtV+UjvbNEHnp82xe171mv8FbG/rcaF1fPe4dhQ1P60lSCrvNZGo0GdnZ23CJ2lN5ywGZ4QZcqt2EK33mZi1LyAE5i6BRQNJtN17lJXlqfT0v1MNFMj4FWrxKoLwmt3qjOZSJpaZ5LSU47E+eBAccD1v7+vstr2LklfA7cl//b8CihCkZen10CRq1P3guvi8n9wWCARqMRmf7AgcZ6XpoD07AsgMikbz4zXz7xokCFPT7hEBB9Ftp+eU9K8jpI6/PWd8H2rKIWAK6NlEolR0w8PwAXXtQwNvOVunqD9ifNYTNcyGNqWFxTBsyvsT1pfssShS/f5Qup6bPRsKh9vhY+8vCd7yzy0mPZ7/S4akT7vrfXdNGwsuRlRQiqKBsOh8jlcgDgvBsuo8DSMupl5HI5560xN0ZLjtuzg2jHseRoQzQMl5FUmDtix9B5M8BJYySpcXIoB51er+fyCbp8CImTv3n9DPUUCgW3DAQVhcwRqCVIT47PZDabod1u4+DgANPp1EnaVa6sBK73rdMRNJysE7in0+P5dwyNkugZ0uSEVVrylUrFLeVyeHiImzdvOstcPV07qPA9s+yThlhYrUPzYCrUuYhYZFVbo0r7hhUFaekw9U6UFBhi1XbPc9DA40T+Bx98EJVKxRWTJmFVq1UAx4RzcHCAarWKTCaD9fV1lwdmjorH73a7TsCjsIYkowea51LP21bW57NbRDz83hpAi7zcRaSj36sRYY0L3/vTEKolW1+btEa75sXP2u8iYGXJCzi9mCAbVrvddlY6rTjWw4vH467O4fr6OorFIsrlcmQdKargmENrNpvodDro9/uRhsGQHi16JSz9myTEfBu3ZyyfAzZLVLVarcixKRefz49l9Jqf0xwPyZu5qH6/j263i83NTZcL4ICg+5MclLjy+bwjCE6SZv5DvUMNu6lYgx4O70GVnjoJW6uO6/tkjoSKx0qlgmq1ikqlgv/7f/8vXn31VXS73UgFeNvx2UZUSJBIJNwAWigUMJ/P3XPXUkYXNWyo+SxLUpaw1ONhSJb51lKpFFHhqaBBpzgwX9vpdNBut10YkNUraCBNp1NcvXoV169fdx4Zowqc67W3t4ednR3E43HX92Kx6BpxNBhJXjqwq0iL4V8tQ7Uot+XzdM7yeHR/no/QkK0lIM1RqbfP730koqFIhRKXDcOfFRWwhrRey0XDypLXIvefL280Grn5JmqZseNyCRDWUGPOh3F9dgbmfzgYq6XP8wKnKxMosWhnUEuJliMHBF2WXPdljiwej+O5556LWFQ6UOtzofdCwrXXYPMgJBXNdw2HQxweHkY6qoaTFnUin8Wn22uRWGsJcx9azZz3w2cwnU7dPC8OyAo1Htjx7Q/vlaWPZrOTBUQ5ENsamRcJlryUuICoclWNBrZzeqFKWFq4WcPXJDuqPVknsN1uu/bbbDZx8+ZNAMeV7Ov1ujMUdEpJKpVyakMaNnzPJCT2BRqDQHTitJIy+w7flY4BNjQKRI3d87wv673o/nzGi8K1ut+i34vgI1kb1j/Pk1Iv+iJjZcnLNgJLEpPJxHkwbPCpVArVahWbm5v4vu/7Pvddv9+PxLVtmIUTg6n642BgB3X7t8bf1QommVAOzOQ0BxGW1eEAU6lUABwPDBr+IJTENEw5HA7dagAkYs2LqZVOUqGSL5lMotfrYWdnJ3Iu9Uo036bvRTu5Phd6YFq7jqE8raYxn88dceXzeVQqFee19vt97O/vu/yfgsfX98hr4HXr9+l02s0boxfmU8FdJJBgrcelAz2/Y1iY4WCSkNbI1FCbeqdKbIVCAaVSCdVqFe1226lCu90uRqMRut0ubty4geFw6N4tw5Lan7LZrCusOxwOUa/XT4XP6dVZo0zvjyF8RhPU0LBe5CKvi//rsX2wHvwiErLHtZ7ZolChGliLvMFFhGfPYbe96AS2suQFnJ7Ix8HfWvRszNlsFvfddx+2traQy+VcPJ+hDzYkWvQsh8OQWLlcdoo/NhwlOu1gVpHEbfWaqTCsVqsol8unLGJeixbJZbUBdmJCl6fgPfV6PacSZJiM4RgtZJtIJBy5kzBmsxm63S4ODg4c0fG+fIO75rI0zKGeEa9ZLXm1yvV4XA6F4VbO37t586ZTqdGoIKylzmdua/GpKIMGDZfIAeBq6V3EUAtFN+pt+UJF2o65RI7W5iT4zDivj8TG98LnRg+Ja7KVSiXs7u66ItatVsuFqafTKa5cuYKtrS13DJ4nm81iOBzitddeQ6fTcbU3mbtS40H7Cu9Vw5gUXAEnQiDt75pbVnK2sKRmyUSFKLbda7RH/7fb6LPWz3Qf+04WhTdtOHLRvgBcf72IWFnyUrLwEZUvdMCQGOdWqQehpXS4qrHGlylY4HLkHABVpAFEk+XacNgJdF+SGwmH6y4xhMljZTIZ50mp96bQ8/JH5eic6Mt7V5k974uEwlwhSQ6IVoJX657Pz3Yy64HpgMpBROvcURnGwUWviRNhh8Mh9vb2ImWs1CgAThSevmviddiBnvfA5WDo3VxE8gKihZzZ9tSzJNjm+O5tTovPX5+zRi80/MRnyfdVKpXcu+R6W0dHR2g2m3j55Zdd+L1Wq0VUu3zeVM/ymjQM6As305PU9fB886ysd6Sf+3I/vpCfJQZfO7BEZMPelkQUi85zq7Ceos9ztKHki4iVJC+ba9HGqh3UWmE6SGrBXXYeHkfLOLEEDpcroXpRLUQgupaVXqfOt6JkXb256XTqavZxAGGOjYNPrVZztQh1orLeu4bM2KlppfP+7QBFUqBIg9XaueS6rcHI46qVqfeq78PX6UmoOgjyh3kWDqy6NAa9Sq5BRkLVwchOT9Bj2cGQv0livOZUKoVyuQwAkXJEFwn6ngkbFtXnz2dID0f7BNszZe5K/EB0LSqC23BeF8PfjGJ0u1288sorTgDD4+o18m9dCVnFGL7BmW15NBq5WpiEj7B8uS5+z2Pq/7qvNUh1n0XemXpft0Iuegx77LOITPexHp7d7iITF7Ci5HWWNcQOZUMFVMJ1u92I2II/nFdFpR47Wj6fd/X9+v2+y8/Yjs7GR0JSNR8H1nw+76oBMIxJ0iNJUMVFcmF1d65bpjkMno+NmMei50Ri1JAgy/ToZNxYLHZKGMHipvosbF6R4L0yOc+kv/4NwFnoV65ciXhfFGJwUjkJjQMvQ03NZhN7e3vO0PBdC9+FDrx8Phr6VGWh9Zap7ryo5KUhY96r5mgpiCmXy6jVas4AsIYDJ++Xy2W3v4o81Eu2BXHZzvh86XW1Wi00Gg30ej3s7u6i1+thfX0d5XLZtXNtf2y3GjFQL42kxZAiw4WsFWrzdJbEdMI7hTk+6JiyqE3x/tUj5Xc2XHiWt7YoaqLXZsOtPgLVffU+eJ16zouIlSSv2Wzm1E70Eg4ODlxYkGIHzR+pTJq1DnWA499aXZ0WJq09VkBnpXibSyE0F5VMJrG7u4tSqeRWe6anwUUCeTx2Gh6TuYpvf/vb6Pf7uHnzJr773e+6QdbXyVQYksvlsL+/7843m81cwlyL4KZSKVQqFVe9vVAoYHt7G88//zxeeOEFV8bJZzTYpLCdIEoSTqVSePnll3F4eIhr164BgMvJMQw0Ho/d4EoyzWazKJfL6Pf72Nvbw3e/+130er2IZ2yvRz0v29HVK1VLX8mNqreLqDhk+Ey9GJvX4/PmO6dhRrIBosWTbXhMK7Sz7QDRKhiaK6OoZjabueobFF288sorGA6HqNVqiMfjrqINIwhKvkA0HcDvOKWCQg9GE3hN6m3z3tjnbejQFx7kcTTdYCMott1bstLPfX9b75/nUXLSd7LIa9LznhXW1FUWLqoHtrLkRa+AVqGGoHRyInDSUZmUJnFwcKJXxoag4Qx6BRrX1wm+1vLh9Wnn3t7edh1ct1WJMENhwOkkerlcdhU69vb2TjVabcg6CKdSKezs7KDb7boBiAotFZnwPIVCwT0bFuJtNpuRa1vkjWgHV2LVkG0mk8Frr73mJq6q1JkWP++bgy+FMyT4g4MD50H4Oq0OataStp1Z35++MzugXiTQe7GeCp+hRgQ0HM7naQdIHouDPUPFfB/WkyGZaK6Qc/E4cZ7HnEwmODg4cBGFYrGIXq/njEO9J70m25ZVhMRr8+W3bbjQ99uSBGGJTbdZFMbzkeAiLNrW9pmzwoB6fXw+Z13bWWHFi4CVJK/5fO5UdAHn49lnn73blxCwJHBA95Vw0kE9kUg4D4d5TM3NMKxMMmROjMehYUci0hA4SU3nctEg4URmEv9gMMDu7i4ajQby+fypsmvA6Zyd3ivna3a7XadkVPGFJR3r7Vgjykdcvv9vFT5CUdiw9SIy4Xu5nes4i7z02s7b7m5hJckrIGCVoYOgCh0ARObNURREr5oiC5JcLpdzlWXoAQ+HQ+epMf8LROdPMVSrYihOr1hfX3c5KVXaMqdpw102TKg5r+l06tZ64xQVzWVZZaSWgrIhY17/mwEjNHwHPlgPi14tcCLq8oXe9W/dZ9F5blUEch5p3k0E8goIWEGQhOiF6eRrDZ1RJp/P5yMLk1Jdqisb0PvRklEqXdfBnwID9Tw4N4/1DS30MzuAW7GGTiHRKRvqQfnCfPY7n0f2ZnCrobjzQo2Eldjzs9sN+flSFxc110UE8goIWCGQrHT5FgqE7DymeDyOcrmMarXqymBxv42NDVdxhZN9OZeRykLNh+k0EyXGWCzm8mqz2QylUgn5fP5UztCKIBRaGNkSF9f+6vf77jiaR1WisoSm5wYWy8f1udlt1FNiyPI8qMLYdy4gmnfVYy4SlNhjWDm/xUUnLiCQV0DASsF6XCQwhs0AOIUnK/GTTBgOZPhKV1jQ1arpoQEnoUEOlAxJ8jjq+amqkZPjlVAW5brspH+qPfv9vhNm6SCvIUyb/yKs5+WDhgEXwechngUrErtV6PXS6+J9WSLkNWho1IYGzzIWLgoCeQUErBgorFChAz0fJS5W5udKCZxXSFUhZedamoskRiLR6v46BYUyfCvyoBJRyURDe0pcSlb6P71JqlG1kob1tHh8H1lYlaH1SvmdTzL/RgZ+9fB8pLmIXBiqfTNQb+080r4oCOQVELCCUMLQwT2RSCCXy7kJyBRijEYjJ8JIJpNot9tuygSng9Cj0ZqR3IcTxkkeyWTSzRVUouE8LxKpnULhUxhqnkul8Tw+97VhUeAkD0cCt4TFv8/yiOxgr97PrZKY9YJuJeflO7YVXJw1XcNHvKou9E0buUgI5BUQsELQqhOc88RJu8lkEpVKBVevXsXa2lpkHhc9MYbKdP4XBRFUILKqC+tg6gRnVTZqLqjX66HX67niAUB0oixwerBW0tIJyywK0Ov1nGJyUXiQ4TXdxioNuZ16p7xuHyFyeyA6Z9FXzcK37yKisMRp/1fSskIUhYYNVa2pUyfsfNJAXgEBAXcVi0JtDPdxhYJisYjBYODEF4VCwc3l4moCLNPEfFMymUS3242UlKIogkSk5+bEYXpcrJ5BQtF97CRw+7lVGZKcraJQlZB2QvIi74znVALT5wkgUkZKcd6grxUyfJ/7juPLs9mJ2WflzOx3Pg+Mz/Z2c29vJQJ5BQSsGGxpKwBuuZJ6vY5yuexqWGYyGSQSCZRKJWQyGZdDYvUNVlyJx4+ryfR6PVc9hlVZgJPKHioSUcLi37ZoshUUKFH5iFhXM9BFJhke9K3VtSgPxuvQe/CRiCUeDdXZ8mO+cOLtEIR9Brq/EpAVqCy6h/M+t8e9SAjkFRCwQtDSVWpdFwoFrK+v4/r165FFS7kWWqVScYMyw3JU88ViMeeVsSAzizgzVDiZTNBsNt0EZG7H9bUGg4ErtKvFou2gqgpDldPrffFY9Aa1XqZvjpfPEyMWeTzqBQJYWMdS81eLJijb/+18OOvt2X2sh+QjrdvForXLLhICeQUErBh0Mc35fI50Oo1KpYKNjQ2Uy2Ukk0k3iZj5qnQ67Sqyd7tdt+QN817ZbDZS/1BJZjKZuNqSJCrW+aT35wvb+QQTlnxVKq81DH1loJSg1OOy87QWiTN8xGYViBqC04m+eg5bBcPCejo+j0/FK2cpJX3HvZWam9ZguIjeVyCvgIAVgoadGHqiZ7WxsYFCoeDmglElyMnElMfTW9LclK77RZUgCyArqZC8WDbKehRnDbqa41KloSUvXbPOqgd9lTRsaJDf+1SEvr8XeWxWMs/j8TPrkfmwSNChJKufnfUMz1M+nudlXjQE8goIWCEwhKcD/NraGh566CG84x3vQD6fx3g8jnheAFyYkHO7gJOVlElyuVwOpVIpsv5aLHYySVYrXxAczHXg1bwQvSutBsKKGlqSajweu2vUUJ16VhpCBKLhOV2jzxdWXKQM1OP51IRKYDyG3YfPRsOltwPrqepxiVudHK372/u5aAjkFRCwQqCnouG+Bx54AFevXkW1WnWTlEkqShScN8VKGFQTch08ijsoxef+nMPFUk0kL3povC69RlUP2jChijV4Dl2NWXNd9LBIwuqB2UVlCf1blzs6D1bwcZaS8KwQ5O3mqnzHW6QoVMOA+UVg8Zwxq/S8SAjkFRCwQrDkVSwWHXFxcrEOpLpqMeXnWvopnU4jn88jm806Wb0uicJ96Blxf/U8NP+m16nhQZ+6UEOGOm/NVs7wzfPS8KENASpulVDs3ColivNk7r7z2mP7rudWCEWf862Q4kUkqUUI5BUQsEJgLophvvX1dbztbW9DvV4/VeWB4gwSDheC5CKusVgMmUwG9fr/r71zC42r+sL4l0lncplOksY017YhCMVLpGC9tV4pGixELX1Rn+KLWDWFYl8EH+pbRdAnFUFEFArxwVYERYk0if9SCqUtWC+UYGsba2LaNJmZTDKZTLP/D7JO1lnZ58wktZmZnvWDQ5Ize87Zszucr2vtb69dj8rKSpSVlTnzWjQ/xiOo6elp61otadzgIsUNGtIlSdFgOp12Ijo+D0fiCtgjG+lmtKXtKJ0nkSYPr7Hm9yNRs7X3m+ujfw9gccEwv47NCi+jLN6O/80XJsv+eKVKiwUVL0UJECReVEnjzjvvRENDA6qrqx2XIVWnmJqacgrbZjIZzMzMOFETFeEl0aL1X9yQQZXc6SFKxXYpEiIx4ruKA1gyx5XJZBwx5GvFSCiTyaRrvy4SLrkbOo+4qF80Jlyg/NJ8UuxkipPuxR/+XLBtaVIvuDGD7skjS2Dpbs68j16uRWnFX0kdxmJAxUtRAgQ93GOxGNavX4+2tjZnr66ysjJXNEP1AWldl23DSmBxjZMUHb5bM7UjQZFGDCle3AIvIy9eRYNb4+lBLw0XsuyTzYgh56q80oW2FCC/D2/Hxct2HVu0w3/yyFBGefyetgXJXmlF2RebK1K+VqypRBUvRQkQ5eXlqK6uRmNjI9ra2rBp0yZUVVW5hItvJcLrBNI6LrLEA3Dmm8rLy11ORkrvkauR0n/0MJTVMPgDklfhkILIzSNUnWN2dtZ5gMuIiz/8uUmDRyEyBSgFDbDXKJTzXIQURy8TB0/bSSGlMZJ2fpkypH9Tm+FEpg8B/3qMEjmGxYaKl6IEiMrKSrS1teHhhx9Ge3s7GhoanJReOp3G1atXkUgkHFv87Oyss+arqqrKWc9Fcy/c/p7NZp25J0rz8YdeJBJxHtgUMZEhhO/VRY5BKrJLgsjTiKlUylkoTe5HGVnR3Bf9lA94uTkmvcf2oCb7PV+IbDN62FJ19FMKHvVDXgNYXEgsRdQmXvw6tnvQeR5d2ebEvChG4QJUvBQlUKxduxabN29GW1sb6urqUF5ejlQq5VS/4PUG+V5YtFiZb1hJD3KKsCgK4wuF+W7F0tpO7XikRmLolTYkUSTnIi2Epoc8ryVoi6AIr0XI8kHNU2o215/tfX4CZmtjm0/jIuUlViSoMoqT16br2tKa+c69FaOAqXgpSoCoqalBZ2cnmpqaHGs8pQWp3iBFXbwIL21xYqtQwUWHojB6sEYiEScioiiKR108vSiFS77GLfskrjz9x1OAMgoDlgqOV7QkIQFbWLBv+piPEHit+fKCixev3yjNGl71B2V6Ul57uX0pRlS8FCVAxGIx3HnnnVi7di2uX7+OdDqNeDyO6elpZ66LBIT+J08bTNKDkwQFWHQvUlSUSqUcY0ZVVRVqamqchcvxeNyx26fTaddaLVk1g8+L0U+y7lN0SH2TURQvwAu4q2bYqlvY8HL0cbzWci0Hv75wQZTGDYJb+W0CmqtvPA1arIuRvVDxUpQAQXNXJAbJZNIpuEvRjFx3xc0OFBFR1EPCAsCVYqTFy+FwGPPz805ER/egCvIkXuRMlLsjU+knira48HGRksV05VwSn/+yGSek+YJ+5wYHWwTjlVKTc1DynK2t7ZyMrLzms2RK0pbG9IoOedox31RiMaDipSgBgoSIoph4PO5KE1Iqj7vdeIoQWIy8+BwWOeQoxUiOxLKyfwvz0pwaCZGtyC7Nk/GKGzSPRoYSSjfS57AV2JUuQZ4m86pk4SUgXLi4bZxfU4oh/ykL88r5KK+oyCaUNDa8P14Wdz+kA9HWL2qXz/UKhYqXogSIbDaLZDKJa9euYWpqChMTE666gBRRERRx0TowYxY3laR2ZJoIhUKoqKhAdXU1IpEIotEoJicnEY/HMTk5iVQqtaRqBj2QuaUeWCxjxXdZjsfjTsQmoy5KHwJwCRsXCC8nng3ell/b9kD3c/7xcbQJnZeg8o1CeVtuSAmHw65rcWGTfZGpQds40CJ17nQs5ihMxUtRAgRtCjk1NYVkMulyBgKLVSOAxcoTvGwSrzHI1w6RG5HqHIZCIaTTaUxNTTnWe17aidZ22R6OPB3JN72k9KQULf7A5nNaXDBsm03ydnxNFX9oyzSjV0qOX0/+Le+ZzzxZPu24SMr5KlvaUiIjML97FCMqXooSICiFR+uxpBDxRbz8gc7Te/x/7/Q+qlBfXV2N8vJyZ80XTxfKWoaAdzVzchbyVCGlJUlQSah42tAmXIB3ik4W6PUqzOtFLoGRFSpIKHn7m1GeiffJq39+fS6FklEqXooSIK5fv+5EQrQvF6WoSLjoAc4rYvCK8tw6vrCwgHA4jJqaGsRiMVRXVyOZTGJmZgbj4+OIx+POQmRueZcPR27SoDkuEr9UKoVMJuOYLviCZOkitJkrbFGXH1L0OHJejQtRrjkiSu3l05ZX3uD3sl2TX1tGWvz9ch7OFoX6pRWLDRUvRQkQZI+nBxyJBc1B8fkiLhI0r0XnqMhuKBRCXV0dampqEIlEkE6nceXKFcTjcSQSCVchXR59ERTR8ciOFiGnUinHeu81NySFSxbY5Q9gW0TB6wLK6/qRj1HCKyWaD36Rmdc1eKrRy5Uo28hr+rkWiw0VL0UJEGS44OuqeFTE57yk8YGbBSjiWLNmDaLRKMrLy5HJZJBMJp1oidvuecqR94Xb5SmlyN2FtJ5MmhtsTkCZAuTvk8j0Iu+T3982/CKjlTr2VmKW8LqHl6tQphZtbdWwoShKUcDXTdG8El93RSWg+BwQpd2oIge3xJNJI51OI5VK4cqVK0gkEk6qj8+R8Ye7tMrzCh1k0qD9v2Q/+KJcOd9l2x2ZfudRFlWnkELltQ7K1oYLZr6sJJLxM4XYrs9TiPn0hT4T/fvw/yCoeCmKUhRks1mXgYJEgxs2eETDI5tIJOLMOfE5slQqhWvXriGRSODatWuOM1C6CiltJyMuLqKzs7OIx+PO3l08LcgL6VIUaEuNyX28lgP/zH6v87QrYXv48/fZsNn5gcVtZmzv5fNTXp9vzZo1Tn/4ztXy/TLK5hEp/0zFmD5U8VKUAEHmC77IGHD/L1zOQfG0IUVblOKbm5tDIpFAMplckiqUlTJswkXOQr7hJaUL6Z5yzRY9WEnUbK5Djk2Mcs1z8fkhW6moXCk6fk0/AwQXIBlh3cicEzeGyHVlXn2UfZAp3mJDxUtRAgSJhRQvaYkPhULIZrMIh8PO6yRekUjE5QicnJxEIpFwSjfxqI7EixeX5SJGIjc/P+/aR4zuKddz8fQgPyfLRPHPxOFibEOKBp8DzMc+Lg0iwGLZLPr8PO3od91859q8sK3/kn3g95KpXaJYbfMqXooSIGQZJkrZ2coslZeXOxUz6urqUFdXh7KyMqfI7sTEhLPgeXp62plL44Iloy0ehdG9qUwV1S7kURbfj4vPUfGqF4Db0MHTdtROmjj4T7oeYDdo8Ie3n+D4laXi/aT7cHfncvFbiyXXlXFy2e29HJkaeSmKUnB4SgxYOs9TXl6OSCSCyspKZ/1WXV0d1qxZg9nZWSQSCVy9ehXxeNxZ00WWeAAu4ZLpQi5ctPcXL9hLBXf5FidynRYXJOq/jLpkROb1WSXyPI+a/ESGP/R5GnA5aT8vV+By3Y8rxZZSLEbRIpYv+YqilDTyASgf5lTLsKqqCrFYDLFYDNFoFAsLC5iZmcHExIRj0KBKHbxKvC3K4geZNPjGkiRc0prPyz/Z0of8Na/zXgYU22f3wivVKNeWeb3XS/jyuTcfU8Ir8su1GFsKIf9Jry+3ykih0MhLUQIGWamlYYM79MLhMOrr69HQ0IBIJIL5+XlcvHgRV65cwZUrVzA1NeVY2/mmkgBc82l8fg2AI1x89+bp6WlX1EJpQi9hov7yVJ+saShdfDxqI+gedA3b/J+cI5NuR27G4OlW28JiSrnyvvNoh14jQwx/bz7/pvxz0fv4fl+ErRKHNHVQOznuxYSKl6IECB4RcTchsCgI0WgUdXV1WLduHYwxSCQS+Oeff3Dx4kVXqpBEiVeEp0XQ8h68bSaTwfT0NJLJpOMslOYMjs3GzaMoHkn6uQ7537b0G6X+cqXt5Pu5ANL48nO26wBuIaWiw7ItCZ7EluKztaE92Pza0EEGm2JOFXJUvBQlYHCXHz3kKUVXWVmJWCyGmpoaVFRUODstj42NYWJiwtlUkip02FKEFF1xRyEdNM9FzkReQQOwp+d4NCCFi7/X9j6/v+U5WovGWYmZQl7fTwz45+LRll8602at59iE1ese+czjFWPUBah4KUrgkMaCyspKxwZ/2223oampCbFYDADw999/459//sGlS5eQSCRcjkJpd+epQroHFzMSrZmZGSQSCacdbVrJHYQ8BSft8vKBTD9zmTE4uYTMSxC9yDdi4dZ7fn1ZGcQWuQFL57VIXCjCsqU/vQSZixw38djm2IpRwFS8FCVghEL/bhpJD0Iq8bR27Vps3LgRlZWVyGazOH/+PIaHh3Ht2jVMTk46c1vcVcgjK26/JyiNODc352zFQhXt+dwWYXMF+hke+JwUHV5pRz/80nx+gkL3lq7KXKYJ6hOPXCU0tvLzymt7pQV5OtDvM9PvXNSKdW0XR8VLUQKETBPSWq5YLIba2lpEo1Gk02kkEglcvnwZExMTzqaVfA2XV9TFTSD0Ok8VUgUO6gthSxd6OQW93idFSoqPbV0Y/5tER6bdbGlKbnCgfshztnby/nzBsi1y4wIn3y/HwCaq9LuXO1GmEWU0dqNp05uJipeiBAzpTKN5rrq6OoRCIYyPj+Py5cv4448/luzHJddscQGT1TQAOBU3aIsTulY+aSg/x53fQ1WaKvjD38tkwUVCzoPJtnIM5bjylB6/prwuPyidyB2O9DllW35tmQLmfbGt2eLnebQoPw+PnvmYFhNlphh7lYNEIoHa2tpCd0NRSo5wOIy1a9cuiST4QmCqTSjXbknyWdAqH7y2Nn7GilxzU37n8nktH5bzfil2ua4rxTCXy3G1kPejdXg3g3g8jpqammW/TyMvRQkQ8/PzmJycLHQ3FOWGKd6EpqIoiqJ4oOKlKIqilBzLEq+DBw/i/vvvRywWQ2NjI3bt2oVz58652rz00ktLHDoPPfSQq83c3Bz27t2LhoYGRKNRPPvss/jrr79u/NMoiqIogWBZ4jU0NITXX38dJ06cQH9/P7LZLLq6upBKpVztnn76aYyOjjrHd99953p93759OHLkCPr6+nDs2DFMT0+ju7vbt4yJoiiKojiYG2B8fNwAMENDQ865np4e89xzz3m+Z2pqyoTDYdPX1+ecu3z5sgmFQub777/P677xeNwA0EMPPfTQo8SPeDy+Iv25oTmveDwOAKivr3edHxwcRGNjIzZv3oyXX34Z4+PjzmunTp3C/Pw8urq6nHOtra3o7OzE8ePHrfehrcb5oSiKogSXFYuXMQZvvPEGHnnkEXR2djrnd+7ciUOHDuHo0aN47733cPLkSezYscPZ2ntsbAyRSATr1q1zXa+pqQljY2PWex08eBC1tbXOsXHjxpV2W1EURbkVWFG8Zox57bXXTHt7uxkZGfFt9/fff5twOGy++uorY4wxhw4dMpFIZEm7J5980rzyyivWa6TTaROPx51jZGSk4KGuHnrooYceN36satpw7969+OabbzAwMIANGzb4tm1paUF7ezuGh4cBAM3NzchkMksWSo6Pj6Opqcl6jYqKCtTU1LgORVEUJbgsS7yMMejt7cXhw4dx9OhRdHR05HzPxMQERkZG0NLSAgDYunUrwuEw+vv7nTajo6P45ZdfsH379mV2X1EURQkkywnTXn31VVNbW2sGBwfN6Oioc8zMzBhjjEkmk2b//v3m+PHj5sKFC2ZgYMBs27bNtLW1mUQi4Vxnz549ZsOGDebHH380p0+fNjt27DBbtmwx2Ww2r36o21APPfTQ49Y4Vpo2XJZ4ed38s88+M8YYMzMzY7q6usz69etNOBw2mzZtMj09PebSpUuu68zOzpre3l5TX19vqqqqTHd395I2fqh46aGHHnrcGsdKxUuryiuKoigFY6VV5UuytmEJ6q2iKIpiYaXP85IUr2QyWeguKIqiKP8BK32el2TacGFhAefOncNdd92FkZERtc5bSCQS2Lhxo46PBzo+udEx8kfHx59c42OMQTKZRGtrq+/O2F6U5GaUoVAIbW1tAKDrvnKg4+OPjk9udIz80fHxx298bsS7UJJpQ0VRFCXYqHgpiqIoJUfJildFRQUOHDiAioqKQnelKNHx8UfHJzc6Rv7o+Phzs8enJA0biqIoSrAp2chLURRFCS4qXoqiKErJoeKlKIqilBwqXoqiKErJUbLi9dFHH6GjowOVlZXYunUr/ve//xW6S6vO22+/jbKyMtfR3NzsvG6Mwdtvv43W1lZUVVXhiSeewK+//lrAHt98fvrpJzzzzDNobW1FWVkZvv76a9fr+YzJ3Nwc9u7di4aGBkSjUTz77LP466+/VvFT3Dxyjc9LL7205Dv10EMPudrcyuNz8OBB3H///YjFYmhsbMSuXbtw7tw5V5sgf4fyGZ/V+g6VpHh9+eWX2LdvH9566y2cOXMGjz76KHbu3IlLly4Vumurzt13343R0VHnOHv2rPPau+++i/fffx8ffPABTp48iebmZjz11FO3dG3IVCqFLVu24IMPPrC+ns+Y7Nu3D0eOHEFfXx+OHTuG6elpdHd34/r166v1MW4aucYHAJ5++mnXd+q7775zvX4rj8/Q0BBef/11nDhxAv39/chms+jq6kIqlXLaBPk7lM/4AKv0HVrRRioF5oEHHjB79uxxnbvjjjvMm2++WaAeFYYDBw6YLVu2WF9bWFgwzc3N5p133nHOpdNpU1tbaz7++ONV6mFhAWCOHDni/J3PmExNTZlwOGz6+vqcNpcvXzahUMh8//33q9b31UCOjzHG9PT0mOeee87zPUEaH2OMGR8fNwDM0NCQMUa/QxI5Psas3neo5CKvTCaDU6dOoaury3W+q6sLx48fL1CvCsfw8DBaW1vR0dGBF154AefPnwcAXLhwAWNjY65xqqiowOOPPx7IcQLyG5NTp05hfn7e1aa1tRWdnZ2BGbfBwUE0NjZi8+bNePnllzE+Pu68FrTxicfjAID6+noA+h2SyPEhVuM7VHLidfXqVVy/fh1NTU2u801NTRgbGytQrwrDgw8+iC+++AI//PADPvnkE4yNjWH79u2YmJhwxkLHaZF8xmRsbAyRSATr1q3zbHMrs3PnThw6dAhHjx7Fe++9h5MnT2LHjh2Ym5sDEKzxMcbgjTfewCOPPILOzk4A+h3i2MYHWL3vUElWlQeAsrIy19/GmCXnbnV27tzp/H7PPfdg27ZtuP322/H55587E6Q6TktZyZgEZdyef/555/fOzk7cd999aG9vx7fffovdu3d7vu9WHJ/e3l78/PPPOHbs2JLX9DvkPT6r9R0quciroaEB5eXlSxR6fHx8yf+GgkY0GsU999yD4eFhx3Wo47RIPmPS3NyMTCaDyclJzzZBoqWlBe3t7RgeHgYQnPHZu3cvvvnmGwwMDGDDhg3Oef0O/YvX+Ni4Wd+hkhOvSCSCrVu3or+/33W+v78f27dvL1CvioO5uTn8/vvvaGlpQUdHB5qbm13jlMlkMDQ0FNhxymdMtm7dinA47GozOjqKX375JZDjNjExgZGREbS0tAC49cfHGIPe3l4cPnwYR48eRUdHh+v1oH+Hco2PjZv2Hcrb2lFE9PX1mXA4bD799FPz22+/mX379ploNGr+/PPPQndtVdm/f78ZHBw058+fNydOnDDd3d0mFos54/DOO++Y2tpac/jwYXP27Fnz4osvmpaWFpNIJArc85tHMpk0Z86cMWfOnDEAzPvvv2/OnDljLl68aIzJb0z27NljNmzYYH788Udz+vRps2PHDrNlyxaTzWYL9bH+M/zGJ5lMmv3795vjx4+bCxcumIGBAbNt2zbT1tYWmPF59dVXTW1trRkcHDSjo6POMTMz47QJ8nco1/is5neoJMXLGGM+/PBD097ebiKRiLn33ntdVs2g8Pzzz5uWlhYTDodNa2ur2b17t/n111+d1xcWFsyBAwdMc3OzqaioMI899pg5e/ZsAXt88xkYGDAAlhw9PT3GmPzGZHZ21vT29pr6+npTVVVluru7zaVLlwrwaf57/MZnZmbGdHV1mfXr15twOGw2bdpkenp6lnz2W3l8bGMDwHz22WdOmyB/h3KNz2p+h3RLFEVRFKXkKLk5L0VRFEVR8VIURVFKDhUvRVEUpeRQ8VIURVFKDhUvRVEUpeRQ8VIURVFKDhUvRVEUpeRQ8VIURVFKDhUvRVEUpeRQ8VIURVFKDhUvRVEUpeRQ8VIURVFKjv8DeCQcXCG3QDAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_data[0], 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 모델 생성 - Attention-Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separable 합성곱 함수\n",
    "def separable_conv(x, inchannel,outchannel):\n",
    "  x = keras.layers.Conv2D(inchannel, (3,3), strides=1, padding=\"same\")(x)\n",
    "  x = keras.layers.BatchNormalization()(x)\n",
    "  x = keras.layers.Conv2D(outchannel, (1,1), strides=1, padding=\"same\")(x)\n",
    "  x = keras.layers.BatchNormalization()(x)\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resiual_units 함수 \n",
    "def resiual_units(input_x):\n",
    "  x = keras.layers.ReLU()(input_x)\n",
    "  x = separable_conv(x,x.shape[-1],128)\n",
    "  x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x = keras.layers.ReLU()(x)\n",
    "  x = separable_conv(x,x.shape[-1],256)\n",
    "  x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x = keras.layers.ReLU()(x)\n",
    "  x = separable_conv(x,x.shape[-1],512)\n",
    "  x = keras.layers.BatchNormalization()(x)\n",
    "  \n",
    "  input_x = keras.layers.Add()([x,input_x])\n",
    "\n",
    "  return input_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model middle_flow 함수\n",
    "def middle_flow(input_x):\n",
    "  #encoder\n",
    "  x = keras.layers.MaxPool2D((2,2),padding=\"same\")(input_x)\n",
    "  x = resiual_units(x)\n",
    "  x = keras.layers.MaxPool2D((2,2),padding=\"same\")(x)\n",
    "  x = resiual_units(x)\n",
    "  x = keras.layers.MaxPool2D((2,2),padding=\"same\")(x)\n",
    "  x = resiual_units(x)\n",
    "  \n",
    "  #decoder\n",
    "  x = resiual_units(x)\n",
    "  x = keras.layers.UpSampling2D((2,2),interpolation='bilinear')(x)\n",
    "  x = resiual_units(x)\n",
    "  x = keras.layers.UpSampling2D((2,2),interpolation='bilinear')(x)\n",
    "  x = resiual_units(x)\n",
    "  x = keras.layers.UpSampling2D((2,2),interpolation='bilinear')(x)\n",
    "  \n",
    "  x = separable_conv(x,x.shape[-1],512)\n",
    "  x = separable_conv(x,x.shape[-1],512) \n",
    "  \n",
    "  #sigmoid \n",
    "  x = keras.activations.sigmoid(x)\n",
    "  x = keras.layers.Multiply()([input_x,x])\n",
    "  x = keras.layers.Add()([input_x,x])\n",
    "\n",
    "  x = resiual_units(x)\n",
    "\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#골연령 측정 모델\n",
    "# entry flow model\n",
    "input = keras.Input(shape=(256,256,1))\n",
    "x = keras.layers.Conv2D(32, (3,3), strides = 2)(input)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.ReLU()(x)\n",
    "\n",
    "x = keras.layers.Conv2D(64, (3,3), strides=1)(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.ReLU()(x)\n",
    "\n",
    "#첫번째\n",
    "x1 = keras.layers.Conv2D(128,(1,1),strides=2)(x) \n",
    "x1 = keras.layers.BatchNormalization()(x1)\n",
    "\n",
    "x = separable_conv(x,x.shape[-1],128)\n",
    "x = keras.layers.ReLU()(x)\n",
    "\n",
    "x = separable_conv(x,x.shape[-1],128)\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = keras.layers.MaxPool2D((2,2),2,padding=\"same\")(x)\n",
    "\n",
    "x = keras.layers.Add()([x,x1])\n",
    "\n",
    "#2번째\n",
    "x1 = keras.layers.Conv2D(512,(1,1),strides=2)(x)\n",
    "x1 = keras.layers.BatchNormalization()(x1)\n",
    "\n",
    "x = separable_conv(x,x.shape[-1],512)\n",
    "x = keras.layers.ReLU()(x)\n",
    "\n",
    "x = separable_conv(x,x.shape[-1],512)\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = keras.layers.MaxPool2D((2,2),2,padding=\"same\")(x)\n",
    "\n",
    "x = keras.layers.Add()([x,x1])\n",
    "\n",
    "\n",
    "#middle flow model\n",
    "x = middle_flow(x)\n",
    "\n",
    "\n",
    "#exit flow model\n",
    "x1 = keras.layers.Conv2D(1024,(1,1),strides=2)(x)\n",
    "\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = separable_conv(x,x.shape[-1],728)\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = separable_conv(x,x.shape[-1],1024)\n",
    "x = keras.layers.MaxPool2D((2,2),2,padding=\"same\")(x)\n",
    "\n",
    "x = keras.layers.Add()([x,x1])\n",
    "\n",
    "\n",
    "x = separable_conv(x,x.shape[-1],1536)\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = separable_conv(x,x.shape[-1],2048)\n",
    "x = keras.layers.ReLU()(x)\n",
    "\n",
    "x = keras.layers.GlobalAvgPool2D()(x)\n",
    "\n",
    "x = keras.layers.Dense(1000,activation='relu')(x)\n",
    "x = keras.layers.Dense(256,activation='relu')(x)\n",
    "x = keras.layers.Dense(1)(x)\n",
    "\n",
    "model = keras.models.Model(input,x)\n",
    "model.compile(optimizer='adam', loss='mae', metrics=['mae','mse'], run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiuser/.conda/envs/junoflow/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('./tjnet_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)        [(None, 256, 256, 1)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d_201 (Conv2D)         (None, 127, 127, 32)         320       ['input_4[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_261 (B  (None, 127, 127, 32)         128       ['conv2d_201[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " re_lu_93 (ReLU)             (None, 127, 127, 32)         0         ['batch_normalization_261[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_202 (Conv2D)         (None, 125, 125, 64)         18496     ['re_lu_93[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_262 (B  (None, 125, 125, 64)         256       ['conv2d_202[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " re_lu_94 (ReLU)             (None, 125, 125, 64)         0         ['batch_normalization_262[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_204 (Conv2D)         (None, 125, 125, 64)         36928     ['re_lu_94[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_264 (B  (None, 125, 125, 64)         256       ['conv2d_204[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_205 (Conv2D)         (None, 125, 125, 128)        8320      ['batch_normalization_264[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_265 (B  (None, 125, 125, 128)        512       ['conv2d_205[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " re_lu_95 (ReLU)             (None, 125, 125, 128)        0         ['batch_normalization_265[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_206 (Conv2D)         (None, 125, 125, 128)        147584    ['re_lu_95[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_266 (B  (None, 125, 125, 128)        512       ['conv2d_206[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_207 (Conv2D)         (None, 125, 125, 128)        16512     ['batch_normalization_266[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_267 (B  (None, 125, 125, 128)        512       ['conv2d_207[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " re_lu_96 (ReLU)             (None, 125, 125, 128)        0         ['batch_normalization_267[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_203 (Conv2D)         (None, 63, 63, 128)          8320      ['re_lu_94[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling2d_18 (MaxPooli  (None, 63, 63, 128)          0         ['re_lu_96[0][0]']            \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " batch_normalization_263 (B  (None, 63, 63, 128)          512       ['conv2d_203[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " add_33 (Add)                (None, 63, 63, 128)          0         ['max_pooling2d_18[0][0]',    \n",
      "                                                                     'batch_normalization_263[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_209 (Conv2D)         (None, 63, 63, 128)          147584    ['add_33[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_269 (B  (None, 63, 63, 128)          512       ['conv2d_209[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_210 (Conv2D)         (None, 63, 63, 512)          66048     ['batch_normalization_269[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_270 (B  (None, 63, 63, 512)          2048      ['conv2d_210[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " re_lu_97 (ReLU)             (None, 63, 63, 512)          0         ['batch_normalization_270[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_211 (Conv2D)         (None, 63, 63, 512)          2359808   ['re_lu_97[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_271 (B  (None, 63, 63, 512)          2048      ['conv2d_211[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_212 (Conv2D)         (None, 63, 63, 512)          262656    ['batch_normalization_271[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_272 (B  (None, 63, 63, 512)          2048      ['conv2d_212[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " re_lu_98 (ReLU)             (None, 63, 63, 512)          0         ['batch_normalization_272[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_208 (Conv2D)         (None, 32, 32, 512)          66048     ['add_33[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d_19 (MaxPooli  (None, 32, 32, 512)          0         ['re_lu_98[0][0]']            \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " batch_normalization_268 (B  (None, 32, 32, 512)          2048      ['conv2d_208[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " add_34 (Add)                (None, 32, 32, 512)          0         ['max_pooling2d_19[0][0]',    \n",
      "                                                                     'batch_normalization_268[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " max_pooling2d_20 (MaxPooli  (None, 16, 16, 512)          0         ['add_34[0][0]']              \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " re_lu_99 (ReLU)             (None, 16, 16, 512)          0         ['max_pooling2d_20[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_213 (Conv2D)         (None, 16, 16, 512)          2359808   ['re_lu_99[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_273 (B  (None, 16, 16, 512)          2048      ['conv2d_213[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_214 (Conv2D)         (None, 16, 16, 128)          65664     ['batch_normalization_273[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_274 (B  (None, 16, 16, 128)          512       ['conv2d_214[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_275 (B  (None, 16, 16, 128)          512       ['batch_normalization_274[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " re_lu_100 (ReLU)            (None, 16, 16, 128)          0         ['batch_normalization_275[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_215 (Conv2D)         (None, 16, 16, 128)          147584    ['re_lu_100[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_276 (B  (None, 16, 16, 128)          512       ['conv2d_215[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_216 (Conv2D)         (None, 16, 16, 256)          33024     ['batch_normalization_276[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_277 (B  (None, 16, 16, 256)          1024      ['conv2d_216[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_278 (B  (None, 16, 16, 256)          1024      ['batch_normalization_277[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " re_lu_101 (ReLU)            (None, 16, 16, 256)          0         ['batch_normalization_278[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_217 (Conv2D)         (None, 16, 16, 256)          590080    ['re_lu_101[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_279 (B  (None, 16, 16, 256)          1024      ['conv2d_217[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_218 (Conv2D)         (None, 16, 16, 512)          131584    ['batch_normalization_279[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_280 (B  (None, 16, 16, 512)          2048      ['conv2d_218[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_281 (B  (None, 16, 16, 512)          2048      ['batch_normalization_280[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " add_35 (Add)                (None, 16, 16, 512)          0         ['batch_normalization_281[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'max_pooling2d_20[0][0]']    \n",
      "                                                                                                  \n",
      " max_pooling2d_21 (MaxPooli  (None, 8, 8, 512)            0         ['add_35[0][0]']              \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " re_lu_102 (ReLU)            (None, 8, 8, 512)            0         ['max_pooling2d_21[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_219 (Conv2D)         (None, 8, 8, 512)            2359808   ['re_lu_102[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_282 (B  (None, 8, 8, 512)            2048      ['conv2d_219[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_220 (Conv2D)         (None, 8, 8, 128)            65664     ['batch_normalization_282[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_283 (B  (None, 8, 8, 128)            512       ['conv2d_220[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_284 (B  (None, 8, 8, 128)            512       ['batch_normalization_283[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " re_lu_103 (ReLU)            (None, 8, 8, 128)            0         ['batch_normalization_284[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_221 (Conv2D)         (None, 8, 8, 128)            147584    ['re_lu_103[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_285 (B  (None, 8, 8, 128)            512       ['conv2d_221[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_222 (Conv2D)         (None, 8, 8, 256)            33024     ['batch_normalization_285[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_286 (B  (None, 8, 8, 256)            1024      ['conv2d_222[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_287 (B  (None, 8, 8, 256)            1024      ['batch_normalization_286[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " re_lu_104 (ReLU)            (None, 8, 8, 256)            0         ['batch_normalization_287[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_223 (Conv2D)         (None, 8, 8, 256)            590080    ['re_lu_104[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_288 (B  (None, 8, 8, 256)            1024      ['conv2d_223[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_224 (Conv2D)         (None, 8, 8, 512)            131584    ['batch_normalization_288[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_289 (B  (None, 8, 8, 512)            2048      ['conv2d_224[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_290 (B  (None, 8, 8, 512)            2048      ['batch_normalization_289[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " add_36 (Add)                (None, 8, 8, 512)            0         ['batch_normalization_290[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'max_pooling2d_21[0][0]']    \n",
      "                                                                                                  \n",
      " max_pooling2d_22 (MaxPooli  (None, 4, 4, 512)            0         ['add_36[0][0]']              \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " re_lu_105 (ReLU)            (None, 4, 4, 512)            0         ['max_pooling2d_22[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_225 (Conv2D)         (None, 4, 4, 512)            2359808   ['re_lu_105[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_291 (B  (None, 4, 4, 512)            2048      ['conv2d_225[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_226 (Conv2D)         (None, 4, 4, 128)            65664     ['batch_normalization_291[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_292 (B  (None, 4, 4, 128)            512       ['conv2d_226[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_293 (B  (None, 4, 4, 128)            512       ['batch_normalization_292[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " re_lu_106 (ReLU)            (None, 4, 4, 128)            0         ['batch_normalization_293[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_227 (Conv2D)         (None, 4, 4, 128)            147584    ['re_lu_106[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_294 (B  (None, 4, 4, 128)            512       ['conv2d_227[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_228 (Conv2D)         (None, 4, 4, 256)            33024     ['batch_normalization_294[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_295 (B  (None, 4, 4, 256)            1024      ['conv2d_228[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_296 (B  (None, 4, 4, 256)            1024      ['batch_normalization_295[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " re_lu_107 (ReLU)            (None, 4, 4, 256)            0         ['batch_normalization_296[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_229 (Conv2D)         (None, 4, 4, 256)            590080    ['re_lu_107[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_297 (B  (None, 4, 4, 256)            1024      ['conv2d_229[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_230 (Conv2D)         (None, 4, 4, 512)            131584    ['batch_normalization_297[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_298 (B  (None, 4, 4, 512)            2048      ['conv2d_230[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_299 (B  (None, 4, 4, 512)            2048      ['batch_normalization_298[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " add_37 (Add)                (None, 4, 4, 512)            0         ['batch_normalization_299[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'max_pooling2d_22[0][0]']    \n",
      "                                                                                                  \n",
      " re_lu_108 (ReLU)            (None, 4, 4, 512)            0         ['add_37[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_231 (Conv2D)         (None, 4, 4, 512)            2359808   ['re_lu_108[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_300 (B  (None, 4, 4, 512)            2048      ['conv2d_231[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_232 (Conv2D)         (None, 4, 4, 128)            65664     ['batch_normalization_300[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_301 (B  (None, 4, 4, 128)            512       ['conv2d_232[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_302 (B  (None, 4, 4, 128)            512       ['batch_normalization_301[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " re_lu_109 (ReLU)            (None, 4, 4, 128)            0         ['batch_normalization_302[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_233 (Conv2D)         (None, 4, 4, 128)            147584    ['re_lu_109[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_303 (B  (None, 4, 4, 128)            512       ['conv2d_233[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_234 (Conv2D)         (None, 4, 4, 256)            33024     ['batch_normalization_303[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_304 (B  (None, 4, 4, 256)            1024      ['conv2d_234[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_305 (B  (None, 4, 4, 256)            1024      ['batch_normalization_304[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " re_lu_110 (ReLU)            (None, 4, 4, 256)            0         ['batch_normalization_305[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_235 (Conv2D)         (None, 4, 4, 256)            590080    ['re_lu_110[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_306 (B  (None, 4, 4, 256)            1024      ['conv2d_235[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_236 (Conv2D)         (None, 4, 4, 512)            131584    ['batch_normalization_306[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_307 (B  (None, 4, 4, 512)            2048      ['conv2d_236[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_308 (B  (None, 4, 4, 512)            2048      ['batch_normalization_307[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " add_38 (Add)                (None, 4, 4, 512)            0         ['batch_normalization_308[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'add_37[0][0]']              \n",
      "                                                                                                  \n",
      " up_sampling2d_9 (UpSamplin  (None, 8, 8, 512)            0         ['add_38[0][0]']              \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " re_lu_111 (ReLU)            (None, 8, 8, 512)            0         ['up_sampling2d_9[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_237 (Conv2D)         (None, 8, 8, 512)            2359808   ['re_lu_111[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_309 (B  (None, 8, 8, 512)            2048      ['conv2d_237[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_238 (Conv2D)         (None, 8, 8, 128)            65664     ['batch_normalization_309[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_310 (B  (None, 8, 8, 128)            512       ['conv2d_238[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_311 (B  (None, 8, 8, 128)            512       ['batch_normalization_310[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " re_lu_112 (ReLU)            (None, 8, 8, 128)            0         ['batch_normalization_311[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_239 (Conv2D)         (None, 8, 8, 128)            147584    ['re_lu_112[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_312 (B  (None, 8, 8, 128)            512       ['conv2d_239[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_240 (Conv2D)         (None, 8, 8, 256)            33024     ['batch_normalization_312[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_313 (B  (None, 8, 8, 256)            1024      ['conv2d_240[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_314 (B  (None, 8, 8, 256)            1024      ['batch_normalization_313[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " re_lu_113 (ReLU)            (None, 8, 8, 256)            0         ['batch_normalization_314[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_241 (Conv2D)         (None, 8, 8, 256)            590080    ['re_lu_113[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_315 (B  (None, 8, 8, 256)            1024      ['conv2d_241[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_242 (Conv2D)         (None, 8, 8, 512)            131584    ['batch_normalization_315[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_316 (B  (None, 8, 8, 512)            2048      ['conv2d_242[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_317 (B  (None, 8, 8, 512)            2048      ['batch_normalization_316[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " add_39 (Add)                (None, 8, 8, 512)            0         ['batch_normalization_317[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'up_sampling2d_9[0][0]']     \n",
      "                                                                                                  \n",
      " up_sampling2d_10 (UpSampli  (None, 16, 16, 512)          0         ['add_39[0][0]']              \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " re_lu_114 (ReLU)            (None, 16, 16, 512)          0         ['up_sampling2d_10[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_243 (Conv2D)         (None, 16, 16, 512)          2359808   ['re_lu_114[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_318 (B  (None, 16, 16, 512)          2048      ['conv2d_243[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_244 (Conv2D)         (None, 16, 16, 128)          65664     ['batch_normalization_318[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_319 (B  (None, 16, 16, 128)          512       ['conv2d_244[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_320 (B  (None, 16, 16, 128)          512       ['batch_normalization_319[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " re_lu_115 (ReLU)            (None, 16, 16, 128)          0         ['batch_normalization_320[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_245 (Conv2D)         (None, 16, 16, 128)          147584    ['re_lu_115[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_321 (B  (None, 16, 16, 128)          512       ['conv2d_245[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_246 (Conv2D)         (None, 16, 16, 256)          33024     ['batch_normalization_321[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_322 (B  (None, 16, 16, 256)          1024      ['conv2d_246[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_323 (B  (None, 16, 16, 256)          1024      ['batch_normalization_322[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " re_lu_116 (ReLU)            (None, 16, 16, 256)          0         ['batch_normalization_323[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_247 (Conv2D)         (None, 16, 16, 256)          590080    ['re_lu_116[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_324 (B  (None, 16, 16, 256)          1024      ['conv2d_247[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_248 (Conv2D)         (None, 16, 16, 512)          131584    ['batch_normalization_324[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_325 (B  (None, 16, 16, 512)          2048      ['conv2d_248[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_326 (B  (None, 16, 16, 512)          2048      ['batch_normalization_325[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " add_40 (Add)                (None, 16, 16, 512)          0         ['batch_normalization_326[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'up_sampling2d_10[0][0]']    \n",
      "                                                                                                  \n",
      " up_sampling2d_11 (UpSampli  (None, 32, 32, 512)          0         ['add_40[0][0]']              \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_249 (Conv2D)         (None, 32, 32, 512)          2359808   ['up_sampling2d_11[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_327 (B  (None, 32, 32, 512)          2048      ['conv2d_249[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_250 (Conv2D)         (None, 32, 32, 512)          262656    ['batch_normalization_327[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_328 (B  (None, 32, 32, 512)          2048      ['conv2d_250[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_251 (Conv2D)         (None, 32, 32, 512)          2359808   ['batch_normalization_328[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_329 (B  (None, 32, 32, 512)          2048      ['conv2d_251[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_252 (Conv2D)         (None, 32, 32, 512)          262656    ['batch_normalization_329[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_330 (B  (None, 32, 32, 512)          2048      ['conv2d_252[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " tf.math.sigmoid_3 (TFOpLam  (None, 32, 32, 512)          0         ['batch_normalization_330[0][0\n",
      " bda)                                                               ]']                           \n",
      "                                                                                                  \n",
      " multiply_3 (Multiply)       (None, 32, 32, 512)          0         ['add_34[0][0]',              \n",
      "                                                                     'tf.math.sigmoid_3[0][0]']   \n",
      "                                                                                                  \n",
      " add_41 (Add)                (None, 32, 32, 512)          0         ['add_34[0][0]',              \n",
      "                                                                     'multiply_3[0][0]']          \n",
      "                                                                                                  \n",
      " re_lu_117 (ReLU)            (None, 32, 32, 512)          0         ['add_41[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_253 (Conv2D)         (None, 32, 32, 512)          2359808   ['re_lu_117[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_331 (B  (None, 32, 32, 512)          2048      ['conv2d_253[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_254 (Conv2D)         (None, 32, 32, 128)          65664     ['batch_normalization_331[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_332 (B  (None, 32, 32, 128)          512       ['conv2d_254[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_333 (B  (None, 32, 32, 128)          512       ['batch_normalization_332[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " re_lu_118 (ReLU)            (None, 32, 32, 128)          0         ['batch_normalization_333[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_255 (Conv2D)         (None, 32, 32, 128)          147584    ['re_lu_118[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_334 (B  (None, 32, 32, 128)          512       ['conv2d_255[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_256 (Conv2D)         (None, 32, 32, 256)          33024     ['batch_normalization_334[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_335 (B  (None, 32, 32, 256)          1024      ['conv2d_256[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_336 (B  (None, 32, 32, 256)          1024      ['batch_normalization_335[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " re_lu_119 (ReLU)            (None, 32, 32, 256)          0         ['batch_normalization_336[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_257 (Conv2D)         (None, 32, 32, 256)          590080    ['re_lu_119[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_337 (B  (None, 32, 32, 256)          1024      ['conv2d_257[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_258 (Conv2D)         (None, 32, 32, 512)          131584    ['batch_normalization_337[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_338 (B  (None, 32, 32, 512)          2048      ['conv2d_258[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_339 (B  (None, 32, 32, 512)          2048      ['batch_normalization_338[0][0\n",
      " atchNormalization)                                                 ]']                           \n",
      "                                                                                                  \n",
      " add_42 (Add)                (None, 32, 32, 512)          0         ['batch_normalization_339[0][0\n",
      "                                                                    ]',                           \n",
      "                                                                     'add_41[0][0]']              \n",
      "                                                                                                  \n",
      " re_lu_120 (ReLU)            (None, 32, 32, 512)          0         ['add_42[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_260 (Conv2D)         (None, 32, 32, 512)          2359808   ['re_lu_120[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_340 (B  (None, 32, 32, 512)          2048      ['conv2d_260[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_261 (Conv2D)         (None, 32, 32, 728)          373464    ['batch_normalization_340[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_341 (B  (None, 32, 32, 728)          2912      ['conv2d_261[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " re_lu_121 (ReLU)            (None, 32, 32, 728)          0         ['batch_normalization_341[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_262 (Conv2D)         (None, 32, 32, 728)          4770584   ['re_lu_121[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_342 (B  (None, 32, 32, 728)          2912      ['conv2d_262[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_263 (Conv2D)         (None, 32, 32, 1024)         746496    ['batch_normalization_342[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_343 (B  (None, 32, 32, 1024)         4096      ['conv2d_263[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " max_pooling2d_23 (MaxPooli  (None, 16, 16, 1024)         0         ['batch_normalization_343[0][0\n",
      " ng2D)                                                              ]']                           \n",
      "                                                                                                  \n",
      " conv2d_259 (Conv2D)         (None, 16, 16, 1024)         525312    ['add_42[0][0]']              \n",
      "                                                                                                  \n",
      " add_43 (Add)                (None, 16, 16, 1024)         0         ['max_pooling2d_23[0][0]',    \n",
      "                                                                     'conv2d_259[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_264 (Conv2D)         (None, 16, 16, 1024)         9438208   ['add_43[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_344 (B  (None, 16, 16, 1024)         4096      ['conv2d_264[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_265 (Conv2D)         (None, 16, 16, 1536)         1574400   ['batch_normalization_344[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_345 (B  (None, 16, 16, 1536)         6144      ['conv2d_265[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " re_lu_122 (ReLU)            (None, 16, 16, 1536)         0         ['batch_normalization_345[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " conv2d_266 (Conv2D)         (None, 16, 16, 1536)         2123520   ['re_lu_122[0][0]']           \n",
      "                                                          0                                       \n",
      "                                                                                                  \n",
      " batch_normalization_346 (B  (None, 16, 16, 1536)         6144      ['conv2d_266[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_267 (Conv2D)         (None, 16, 16, 2048)         3147776   ['batch_normalization_346[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_347 (B  (None, 16, 16, 2048)         8192      ['conv2d_267[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " re_lu_123 (ReLU)            (None, 16, 16, 2048)         0         ['batch_normalization_347[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " global_average_pooling2d_3  (None, 2048)                 0         ['re_lu_123[0][0]']           \n",
      "  (GlobalAveragePooling2D)                                                                        \n",
      "                                                                                                  \n",
      " dense_9 (Dense)             (None, 1000)                 2049000   ['global_average_pooling2d_3[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (None, 256)                  256256    ['dense_9[0][0]']             \n",
      "                                                                                                  \n",
      " dense_11 (Dense)            (None, 1)                    257       ['dense_10[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 78285913 (298.64 MB)\n",
      "Trainable params: 78220217 (298.39 MB)\n",
      "Non-trainable params: 65696 (256.62 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7:2:1 == train:valid:test \n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_data.reshape(1234, 256, 256, 1), y_data, \n",
    "#                                                   random_state=42, test_size=0.2)\n",
    "\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, random_state=42, test_size = 0.66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(987, 256, 256, 1)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filename = 'checkpoint-50-epochs-8-batchs.h5'\n",
    "checkpoint = ModelCheckpoint(filename, mointor='val_loss', verbose=1, \n",
    "                             save_best_only = True, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('./checkpoint-50-epochs-8-batchs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 3.3777 - mae: 3.3777 - mse: 17.2472\n",
      "Epoch 1: val_loss improved from inf to 1712.14185, saving model to checkpoint-50-epochs-8-batchs.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiuser/.conda/envs/junoflow/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 97s 594ms/step - loss: 3.3777 - mae: 3.3777 - mse: 17.2472 - val_loss: 1712.1418 - val_mae: 1712.1418 - val_mse: 2955200.0000\n",
      "Epoch 2/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 2.6214 - mae: 2.6214 - mse: 10.5827\n",
      "Epoch 2: val_loss improved from 1712.14185 to 75.05275, saving model to checkpoint-50-epochs-8-batchs.h5\n",
      "108/108 [==============================] - 52s 480ms/step - loss: 2.6214 - mae: 2.6214 - mse: 10.5827 - val_loss: 75.0527 - val_mae: 75.0527 - val_mse: 5750.9595\n",
      "Epoch 3/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 2.2051 - mae: 2.2051 - mse: 7.9137\n",
      "Epoch 3: val_loss improved from 75.05275 to 4.57895, saving model to checkpoint-50-epochs-8-batchs.h5\n",
      "108/108 [==============================] - 51s 473ms/step - loss: 2.2051 - mae: 2.2051 - mse: 7.9137 - val_loss: 4.5789 - val_mae: 4.5789 - val_mse: 44.6040\n",
      "Epoch 4/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 2.0854 - mae: 2.0854 - mse: 7.0510\n",
      "Epoch 4: val_loss improved from 4.57895 to 3.16758, saving model to checkpoint-50-epochs-8-batchs.h5\n",
      "108/108 [==============================] - 52s 483ms/step - loss: 2.0854 - mae: 2.0854 - mse: 7.0510 - val_loss: 3.1676 - val_mae: 3.1676 - val_mse: 15.8754\n",
      "Epoch 5/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 1.9783 - mae: 1.9783 - mse: 6.3132\n",
      "Epoch 5: val_loss improved from 3.16758 to 2.01726, saving model to checkpoint-50-epochs-8-batchs.h5\n",
      "108/108 [==============================] - 52s 486ms/step - loss: 1.9783 - mae: 1.9783 - mse: 6.3132 - val_loss: 2.0173 - val_mae: 2.0173 - val_mse: 6.4320\n",
      "Epoch 6/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 1.8513 - mae: 1.8513 - mse: 5.3344\n",
      "Epoch 6: val_loss did not improve from 2.01726\n",
      "108/108 [==============================] - 49s 457ms/step - loss: 1.8513 - mae: 1.8513 - mse: 5.3344 - val_loss: 3.8271 - val_mae: 3.8271 - val_mse: 20.4235\n",
      "Epoch 7/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 1.7966 - mae: 1.7966 - mse: 5.2427\n",
      "Epoch 7: val_loss did not improve from 2.01726\n",
      "108/108 [==============================] - 49s 449ms/step - loss: 1.7966 - mae: 1.7966 - mse: 5.2427 - val_loss: 2.3774 - val_mae: 2.3774 - val_mse: 8.6495\n",
      "Epoch 8/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 1.9701 - mae: 1.9701 - mse: 6.3695\n",
      "Epoch 8: val_loss improved from 2.01726 to 1.51806, saving model to checkpoint-50-epochs-8-batchs.h5\n",
      "108/108 [==============================] - 52s 477ms/step - loss: 1.9701 - mae: 1.9701 - mse: 6.3695 - val_loss: 1.5181 - val_mae: 1.5181 - val_mse: 3.4308\n",
      "Epoch 9/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 1.6548 - mae: 1.6548 - mse: 4.4852\n",
      "Epoch 9: val_loss did not improve from 1.51806\n",
      "108/108 [==============================] - 50s 461ms/step - loss: 1.6548 - mae: 1.6548 - mse: 4.4852 - val_loss: 2.7003 - val_mae: 2.7003 - val_mse: 9.8318\n",
      "Epoch 10/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 1.5939 - mae: 1.5939 - mse: 4.1581\n",
      "Epoch 10: val_loss did not improve from 1.51806\n",
      "108/108 [==============================] - 51s 474ms/step - loss: 1.5939 - mae: 1.5939 - mse: 4.1581 - val_loss: 3.4413 - val_mae: 3.4413 - val_mse: 17.2623\n",
      "Epoch 11/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 1.6340 - mae: 1.6340 - mse: 4.2460\n",
      "Epoch 11: val_loss improved from 1.51806 to 1.41853, saving model to checkpoint-50-epochs-8-batchs.h5\n",
      "108/108 [==============================] - 57s 525ms/step - loss: 1.6340 - mae: 1.6340 - mse: 4.2460 - val_loss: 1.4185 - val_mae: 1.4185 - val_mse: 2.9975\n",
      "Epoch 12/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 1.4939 - mae: 1.4939 - mse: 3.6922\n",
      "Epoch 12: val_loss did not improve from 1.41853\n",
      "108/108 [==============================] - 51s 473ms/step - loss: 1.4939 - mae: 1.4939 - mse: 3.6922 - val_loss: 2.7597 - val_mae: 2.7597 - val_mse: 10.2214\n",
      "Epoch 13/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 1.4497 - mae: 1.4497 - mse: 3.5167\n",
      "Epoch 13: val_loss improved from 1.41853 to 1.26199, saving model to checkpoint-50-epochs-8-batchs.h5\n",
      "108/108 [==============================] - 63s 588ms/step - loss: 1.4497 - mae: 1.4497 - mse: 3.5167 - val_loss: 1.2620 - val_mae: 1.2620 - val_mse: 2.7894\n",
      "Epoch 14/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 1.3948 - mae: 1.3948 - mse: 3.0826\n",
      "Epoch 14: val_loss did not improve from 1.26199\n",
      "108/108 [==============================] - 52s 480ms/step - loss: 1.3948 - mae: 1.3948 - mse: 3.0826 - val_loss: 4.7788 - val_mae: 4.7788 - val_mse: 29.8099\n",
      "Epoch 15/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 1.2306 - mae: 1.2306 - mse: 2.4552\n",
      "Epoch 15: val_loss did not improve from 1.26199\n",
      "108/108 [==============================] - 50s 468ms/step - loss: 1.2306 - mae: 1.2306 - mse: 2.4552 - val_loss: 1.3810 - val_mae: 1.3810 - val_mse: 2.7527\n",
      "Epoch 16/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 1.2170 - mae: 1.2170 - mse: 2.4829\n",
      "Epoch 16: val_loss did not improve from 1.26199\n",
      "108/108 [==============================] - 51s 470ms/step - loss: 1.2170 - mae: 1.2170 - mse: 2.4829 - val_loss: 4.7331 - val_mae: 4.7331 - val_mse: 28.8618\n",
      "Epoch 17/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 1.3125 - mae: 1.3125 - mse: 2.7329\n",
      "Epoch 17: val_loss did not improve from 1.26199\n",
      "108/108 [==============================] - 49s 457ms/step - loss: 1.3125 - mae: 1.3125 - mse: 2.7329 - val_loss: 2.0601 - val_mae: 2.0601 - val_mse: 5.7995\n",
      "Epoch 18/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 1.3475 - mae: 1.3475 - mse: 3.0545\n",
      "Epoch 18: val_loss did not improve from 1.26199\n",
      "108/108 [==============================] - 50s 460ms/step - loss: 1.3475 - mae: 1.3475 - mse: 3.0545 - val_loss: 3.8821 - val_mae: 3.8821 - val_mse: 19.7781\n",
      "Epoch 19/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 1.2165 - mae: 1.2165 - mse: 2.3629\n",
      "Epoch 19: val_loss did not improve from 1.26199\n",
      "108/108 [==============================] - 49s 454ms/step - loss: 1.2165 - mae: 1.2165 - mse: 2.3629 - val_loss: 4.4272 - val_mae: 4.4272 - val_mse: 25.6204\n",
      "Epoch 20/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 1.3516 - mae: 1.3516 - mse: 2.9045\n",
      "Epoch 20: val_loss did not improve from 1.26199\n",
      "108/108 [==============================] - 50s 459ms/step - loss: 1.3516 - mae: 1.3516 - mse: 2.9045 - val_loss: 1.9768 - val_mae: 1.9768 - val_mse: 5.4508\n",
      "Epoch 21/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 1.1640 - mae: 1.1640 - mse: 2.2491\n",
      "Epoch 21: val_loss did not improve from 1.26199\n",
      "108/108 [==============================] - 50s 459ms/step - loss: 1.1640 - mae: 1.1640 - mse: 2.2491 - val_loss: 2.2248 - val_mae: 2.2248 - val_mse: 6.6907\n",
      "Epoch 22/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 1.1931 - mae: 1.1931 - mse: 2.2478\n",
      "Epoch 22: val_loss did not improve from 1.26199\n",
      "108/108 [==============================] - 48s 447ms/step - loss: 1.1931 - mae: 1.1931 - mse: 2.2478 - val_loss: 1.5708 - val_mae: 1.5708 - val_mse: 3.8021\n",
      "Epoch 23/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 1.1310 - mae: 1.1310 - mse: 2.0613\n",
      "Epoch 23: val_loss did not improve from 1.26199\n",
      "108/108 [==============================] - 49s 455ms/step - loss: 1.1310 - mae: 1.1310 - mse: 2.0613 - val_loss: 1.2685 - val_mae: 1.2685 - val_mse: 2.6447\n",
      "Epoch 24/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 1.0880 - mae: 1.0880 - mse: 1.9498\n",
      "Epoch 24: val_loss improved from 1.26199 to 1.21552, saving model to checkpoint-50-epochs-8-batchs.h5\n",
      "108/108 [==============================] - 52s 482ms/step - loss: 1.0880 - mae: 1.0880 - mse: 1.9498 - val_loss: 1.2155 - val_mae: 1.2155 - val_mse: 2.3031\n",
      "Epoch 25/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 1.0398 - mae: 1.0398 - mse: 1.8009\n",
      "Epoch 25: val_loss did not improve from 1.21552\n",
      "108/108 [==============================] - 49s 454ms/step - loss: 1.0398 - mae: 1.0398 - mse: 1.8009 - val_loss: 2.9503 - val_mae: 2.9503 - val_mse: 10.6482\n",
      "Epoch 26/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 1.0362 - mae: 1.0362 - mse: 1.7757\n",
      "Epoch 26: val_loss did not improve from 1.21552\n",
      "108/108 [==============================] - 50s 464ms/step - loss: 1.0362 - mae: 1.0362 - mse: 1.7757 - val_loss: 2.9435 - val_mae: 2.9435 - val_mse: 10.6027\n",
      "Epoch 27/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.9757 - mae: 0.9757 - mse: 1.5489\n",
      "Epoch 27: val_loss did not improve from 1.21552\n",
      "108/108 [==============================] - 50s 465ms/step - loss: 0.9757 - mae: 0.9757 - mse: 1.5489 - val_loss: 2.0900 - val_mae: 2.0900 - val_mse: 5.9722\n",
      "Epoch 28/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 1.0819 - mae: 1.0819 - mse: 1.8757\n",
      "Epoch 28: val_loss did not improve from 1.21552\n",
      "108/108 [==============================] - 50s 463ms/step - loss: 1.0819 - mae: 1.0819 - mse: 1.8757 - val_loss: 1.6374 - val_mae: 1.6374 - val_mse: 3.7219\n",
      "Epoch 29/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 1.0362 - mae: 1.0362 - mse: 1.7175\n",
      "Epoch 29: val_loss did not improve from 1.21552\n",
      "108/108 [==============================] - 50s 458ms/step - loss: 1.0362 - mae: 1.0362 - mse: 1.7175 - val_loss: 2.3183 - val_mae: 2.3183 - val_mse: 7.6901\n",
      "Epoch 30/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 1.0128 - mae: 1.0128 - mse: 1.6286\n",
      "Epoch 30: val_loss did not improve from 1.21552\n",
      "108/108 [==============================] - 50s 463ms/step - loss: 1.0128 - mae: 1.0128 - mse: 1.6286 - val_loss: 1.5126 - val_mae: 1.5126 - val_mse: 3.6561\n",
      "Epoch 31/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.9629 - mae: 0.9629 - mse: 1.5432\n",
      "Epoch 31: val_loss did not improve from 1.21552\n",
      "108/108 [==============================] - 49s 458ms/step - loss: 0.9629 - mae: 0.9629 - mse: 1.5432 - val_loss: 1.2915 - val_mae: 1.2915 - val_mse: 2.5589\n",
      "Epoch 32/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.9454 - mae: 0.9454 - mse: 1.5085\n",
      "Epoch 32: val_loss improved from 1.21552 to 1.05344, saving model to checkpoint-50-epochs-8-batchs.h5\n",
      "108/108 [==============================] - 52s 484ms/step - loss: 0.9454 - mae: 0.9454 - mse: 1.5085 - val_loss: 1.0534 - val_mae: 1.0534 - val_mse: 1.6985\n",
      "Epoch 33/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.9104 - mae: 0.9104 - mse: 1.3383\n",
      "Epoch 33: val_loss did not improve from 1.05344\n",
      "108/108 [==============================] - 50s 461ms/step - loss: 0.9104 - mae: 0.9104 - mse: 1.3383 - val_loss: 1.2981 - val_mae: 1.2981 - val_mse: 2.5112\n",
      "Epoch 34/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 1.0412 - mae: 1.0412 - mse: 1.7963\n",
      "Epoch 34: val_loss did not improve from 1.05344\n",
      "108/108 [==============================] - 50s 460ms/step - loss: 1.0412 - mae: 1.0412 - mse: 1.7963 - val_loss: 1.2036 - val_mae: 1.2036 - val_mse: 2.3304\n",
      "Epoch 35/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.9081 - mae: 0.9081 - mse: 1.4083\n",
      "Epoch 35: val_loss did not improve from 1.05344\n",
      "108/108 [==============================] - 49s 454ms/step - loss: 0.9081 - mae: 0.9081 - mse: 1.4083 - val_loss: 1.1517 - val_mae: 1.1517 - val_mse: 2.3926\n",
      "Epoch 36/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.8509 - mae: 0.8509 - mse: 1.1893\n",
      "Epoch 36: val_loss did not improve from 1.05344\n",
      "108/108 [==============================] - 50s 461ms/step - loss: 0.8509 - mae: 0.8509 - mse: 1.1893 - val_loss: 1.2722 - val_mae: 1.2722 - val_mse: 2.7172\n",
      "Epoch 37/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.8798 - mae: 0.8798 - mse: 1.2561\n",
      "Epoch 37: val_loss did not improve from 1.05344\n",
      "108/108 [==============================] - 49s 456ms/step - loss: 0.8798 - mae: 0.8798 - mse: 1.2561 - val_loss: 2.1961 - val_mae: 2.1961 - val_mse: 6.2838\n",
      "Epoch 38/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 1.0043 - mae: 1.0043 - mse: 1.6261\n",
      "Epoch 38: val_loss did not improve from 1.05344\n",
      "108/108 [==============================] - 49s 453ms/step - loss: 1.0043 - mae: 1.0043 - mse: 1.6261 - val_loss: 2.4366 - val_mae: 2.4366 - val_mse: 7.9034\n",
      "Epoch 39/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.9153 - mae: 0.9153 - mse: 1.3602\n",
      "Epoch 39: val_loss did not improve from 1.05344\n",
      "108/108 [==============================] - 49s 458ms/step - loss: 0.9153 - mae: 0.9153 - mse: 1.3602 - val_loss: 3.1749 - val_mae: 3.1749 - val_mse: 12.3149\n",
      "Epoch 40/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.8770 - mae: 0.8770 - mse: 1.2367\n",
      "Epoch 40: val_loss did not improve from 1.05344\n",
      "108/108 [==============================] - 49s 458ms/step - loss: 0.8770 - mae: 0.8770 - mse: 1.2367 - val_loss: 1.5014 - val_mae: 1.5014 - val_mse: 3.3740\n",
      "Epoch 41/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.8949 - mae: 0.8949 - mse: 1.3377\n",
      "Epoch 41: val_loss did not improve from 1.05344\n",
      "108/108 [==============================] - 49s 457ms/step - loss: 0.8949 - mae: 0.8949 - mse: 1.3377 - val_loss: 3.7239 - val_mae: 3.7239 - val_mse: 17.5607\n",
      "Epoch 42/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.8428 - mae: 0.8428 - mse: 1.1580\n",
      "Epoch 42: val_loss did not improve from 1.05344\n",
      "108/108 [==============================] - 50s 462ms/step - loss: 0.8428 - mae: 0.8428 - mse: 1.1580 - val_loss: 1.9852 - val_mae: 1.9852 - val_mse: 5.7800\n",
      "Epoch 43/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.8084 - mae: 0.8084 - mse: 1.1109\n",
      "Epoch 43: val_loss did not improve from 1.05344\n",
      "108/108 [==============================] - 50s 459ms/step - loss: 0.8084 - mae: 0.8084 - mse: 1.1109 - val_loss: 2.9892 - val_mae: 2.9892 - val_mse: 11.3830\n",
      "Epoch 44/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.8016 - mae: 0.8016 - mse: 1.0719\n",
      "Epoch 44: val_loss did not improve from 1.05344\n",
      "108/108 [==============================] - 49s 452ms/step - loss: 0.8016 - mae: 0.8016 - mse: 1.0719 - val_loss: 2.5433 - val_mae: 2.5433 - val_mse: 8.2876\n",
      "Epoch 45/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.7817 - mae: 0.7817 - mse: 1.0098\n",
      "Epoch 45: val_loss improved from 1.05344 to 1.05041, saving model to checkpoint-50-epochs-8-batchs.h5\n",
      "108/108 [==============================] - 52s 481ms/step - loss: 0.7817 - mae: 0.7817 - mse: 1.0098 - val_loss: 1.0504 - val_mae: 1.0504 - val_mse: 1.9162\n",
      "Epoch 46/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.7164 - mae: 0.7164 - mse: 0.9013\n",
      "Epoch 46: val_loss did not improve from 1.05041\n",
      "108/108 [==============================] - 49s 458ms/step - loss: 0.7164 - mae: 0.7164 - mse: 0.9013 - val_loss: 4.0225 - val_mae: 4.0225 - val_mse: 19.2491\n",
      "Epoch 47/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.7750 - mae: 0.7750 - mse: 0.9848\n",
      "Epoch 47: val_loss did not improve from 1.05041\n",
      "108/108 [==============================] - 49s 451ms/step - loss: 0.7750 - mae: 0.7750 - mse: 0.9848 - val_loss: 2.0603 - val_mae: 2.0603 - val_mse: 5.6453\n",
      "Epoch 48/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.7845 - mae: 0.7845 - mse: 1.0131\n",
      "Epoch 48: val_loss did not improve from 1.05041\n",
      "108/108 [==============================] - 49s 455ms/step - loss: 0.7845 - mae: 0.7845 - mse: 1.0131 - val_loss: 2.9476 - val_mae: 2.9476 - val_mse: 10.5145\n",
      "Epoch 49/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.7520 - mae: 0.7520 - mse: 0.9451\n",
      "Epoch 49: val_loss did not improve from 1.05041\n",
      "108/108 [==============================] - 50s 460ms/step - loss: 0.7520 - mae: 0.7520 - mse: 0.9451 - val_loss: 2.1121 - val_mae: 2.1121 - val_mse: 5.8876\n",
      "Epoch 50/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.7807 - mae: 0.7807 - mse: 0.9770\n",
      "Epoch 50: val_loss did not improve from 1.05041\n",
      "108/108 [==============================] - 48s 448ms/step - loss: 0.7807 - mae: 0.7807 - mse: 0.9770 - val_loss: 2.9946 - val_mae: 2.9946 - val_mse: 11.0924\n",
      "Epoch 51/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.6752 - mae: 0.6752 - mse: 0.7805\n",
      "Epoch 51: val_loss did not improve from 1.05041\n",
      "108/108 [==============================] - 50s 460ms/step - loss: 0.6752 - mae: 0.6752 - mse: 0.7805 - val_loss: 2.3831 - val_mae: 2.3831 - val_mse: 7.3877\n",
      "Epoch 52/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.6865 - mae: 0.6865 - mse: 0.7986\n",
      "Epoch 52: val_loss did not improve from 1.05041\n",
      "108/108 [==============================] - 49s 452ms/step - loss: 0.6865 - mae: 0.6865 - mse: 0.7986 - val_loss: 1.6883 - val_mae: 1.6883 - val_mse: 4.4127\n",
      "Epoch 53/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.6104 - mae: 0.6104 - mse: 0.6341\n",
      "Epoch 53: val_loss did not improve from 1.05041\n",
      "108/108 [==============================] - 49s 452ms/step - loss: 0.6104 - mae: 0.6104 - mse: 0.6341 - val_loss: 1.1304 - val_mae: 1.1304 - val_mse: 2.0813\n",
      "Epoch 54/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.7178 - mae: 0.7178 - mse: 0.8366\n",
      "Epoch 54: val_loss did not improve from 1.05041\n",
      "108/108 [==============================] - 49s 453ms/step - loss: 0.7178 - mae: 0.7178 - mse: 0.8366 - val_loss: 2.1006 - val_mae: 2.1006 - val_mse: 5.8537\n",
      "Epoch 55/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.7709 - mae: 0.7709 - mse: 0.9506\n",
      "Epoch 55: val_loss did not improve from 1.05041\n",
      "108/108 [==============================] - 50s 459ms/step - loss: 0.7709 - mae: 0.7709 - mse: 0.9506 - val_loss: 2.9699 - val_mae: 2.9699 - val_mse: 11.0437\n",
      "Epoch 56/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.7487 - mae: 0.7487 - mse: 0.9307\n",
      "Epoch 56: val_loss did not improve from 1.05041\n",
      "108/108 [==============================] - 49s 454ms/step - loss: 0.7487 - mae: 0.7487 - mse: 0.9307 - val_loss: 3.3278 - val_mae: 3.3278 - val_mse: 13.0537\n",
      "Epoch 57/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.7387 - mae: 0.7387 - mse: 0.8462\n",
      "Epoch 57: val_loss did not improve from 1.05041\n",
      "108/108 [==============================] - 49s 458ms/step - loss: 0.7387 - mae: 0.7387 - mse: 0.8462 - val_loss: 1.0798 - val_mae: 1.0798 - val_mse: 1.9970\n",
      "Epoch 58/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.8413 - mae: 0.8413 - mse: 1.1249\n",
      "Epoch 58: val_loss did not improve from 1.05041\n",
      "108/108 [==============================] - 49s 458ms/step - loss: 0.8413 - mae: 0.8413 - mse: 1.1249 - val_loss: 1.1163 - val_mae: 1.1163 - val_mse: 1.8716\n",
      "Epoch 59/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.6893 - mae: 0.6893 - mse: 0.7552\n",
      "Epoch 59: val_loss improved from 1.05041 to 1.01405, saving model to checkpoint-50-epochs-8-batchs.h5\n",
      "108/108 [==============================] - 51s 471ms/step - loss: 0.6893 - mae: 0.6893 - mse: 0.7552 - val_loss: 1.0140 - val_mae: 1.0140 - val_mse: 1.7298\n",
      "Epoch 60/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.6767 - mae: 0.6767 - mse: 0.7492\n",
      "Epoch 60: val_loss did not improve from 1.01405\n",
      "108/108 [==============================] - 50s 465ms/step - loss: 0.6767 - mae: 0.6767 - mse: 0.7492 - val_loss: 1.5377 - val_mae: 1.5377 - val_mse: 3.5496\n",
      "Epoch 61/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.7044 - mae: 0.7044 - mse: 0.8343\n",
      "Epoch 61: val_loss did not improve from 1.01405\n",
      "108/108 [==============================] - 49s 457ms/step - loss: 0.7044 - mae: 0.7044 - mse: 0.8343 - val_loss: 1.0589 - val_mae: 1.0589 - val_mse: 1.8017\n",
      "Epoch 62/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.6269 - mae: 0.6269 - mse: 0.6789\n",
      "Epoch 62: val_loss did not improve from 1.01405\n",
      "108/108 [==============================] - 50s 463ms/step - loss: 0.6269 - mae: 0.6269 - mse: 0.6789 - val_loss: 1.3721 - val_mae: 1.3721 - val_mse: 2.7046\n",
      "Epoch 63/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.6387 - mae: 0.6387 - mse: 0.6909\n",
      "Epoch 63: val_loss did not improve from 1.01405\n",
      "108/108 [==============================] - 50s 460ms/step - loss: 0.6387 - mae: 0.6387 - mse: 0.6909 - val_loss: 1.1603 - val_mae: 1.1603 - val_mse: 2.2109\n",
      "Epoch 64/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.6527 - mae: 0.6527 - mse: 0.7244\n",
      "Epoch 64: val_loss did not improve from 1.01405\n",
      "108/108 [==============================] - 48s 447ms/step - loss: 0.6527 - mae: 0.6527 - mse: 0.7244 - val_loss: 1.4307 - val_mae: 1.4307 - val_mse: 3.0433\n",
      "Epoch 65/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.6786 - mae: 0.6786 - mse: 0.7717\n",
      "Epoch 65: val_loss did not improve from 1.01405\n",
      "108/108 [==============================] - 50s 461ms/step - loss: 0.6786 - mae: 0.6786 - mse: 0.7717 - val_loss: 1.0634 - val_mae: 1.0634 - val_mse: 1.8706\n",
      "Epoch 66/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.6596 - mae: 0.6596 - mse: 0.7156\n",
      "Epoch 66: val_loss did not improve from 1.01405\n",
      "108/108 [==============================] - 50s 460ms/step - loss: 0.6596 - mae: 0.6596 - mse: 0.7156 - val_loss: 2.1465 - val_mae: 2.1465 - val_mse: 5.8671\n",
      "Epoch 67/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.7435 - mae: 0.7435 - mse: 0.9158\n",
      "Epoch 67: val_loss did not improve from 1.01405\n",
      "108/108 [==============================] - 49s 452ms/step - loss: 0.7435 - mae: 0.7435 - mse: 0.9158 - val_loss: 1.7485 - val_mae: 1.7485 - val_mse: 4.0719\n",
      "Epoch 68/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.6394 - mae: 0.6394 - mse: 0.6744\n",
      "Epoch 68: val_loss did not improve from 1.01405\n",
      "108/108 [==============================] - 49s 456ms/step - loss: 0.6394 - mae: 0.6394 - mse: 0.6744 - val_loss: 3.2975 - val_mae: 3.2975 - val_mse: 13.2566\n",
      "Epoch 69/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.5581 - mae: 0.5581 - mse: 0.5220\n",
      "Epoch 69: val_loss did not improve from 1.01405\n",
      "108/108 [==============================] - 49s 455ms/step - loss: 0.5581 - mae: 0.5581 - mse: 0.5220 - val_loss: 1.0296 - val_mae: 1.0296 - val_mse: 1.6750\n",
      "Epoch 70/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.6707 - mae: 0.6707 - mse: 0.7462\n",
      "Epoch 70: val_loss did not improve from 1.01405\n",
      "108/108 [==============================] - 49s 456ms/step - loss: 0.6707 - mae: 0.6707 - mse: 0.7462 - val_loss: 1.8117 - val_mae: 1.8117 - val_mse: 4.5836\n",
      "Epoch 71/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.6945 - mae: 0.6945 - mse: 0.7942\n",
      "Epoch 71: val_loss did not improve from 1.01405\n",
      "108/108 [==============================] - 50s 460ms/step - loss: 0.6945 - mae: 0.6945 - mse: 0.7942 - val_loss: 2.2871 - val_mae: 2.2871 - val_mse: 7.2366\n",
      "Epoch 72/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.6282 - mae: 0.6282 - mse: 0.6590\n",
      "Epoch 72: val_loss did not improve from 1.01405\n",
      "108/108 [==============================] - 49s 455ms/step - loss: 0.6282 - mae: 0.6282 - mse: 0.6590 - val_loss: 1.4555 - val_mae: 1.4555 - val_mse: 3.1911\n",
      "Epoch 73/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.7115 - mae: 0.7115 - mse: 0.8102\n",
      "Epoch 73: val_loss did not improve from 1.01405\n",
      "108/108 [==============================] - 49s 451ms/step - loss: 0.7115 - mae: 0.7115 - mse: 0.8102 - val_loss: 1.6130 - val_mae: 1.6130 - val_mse: 3.7573\n",
      "Epoch 74/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.5786 - mae: 0.5786 - mse: 0.5658\n",
      "Epoch 74: val_loss did not improve from 1.01405\n",
      "108/108 [==============================] - 49s 455ms/step - loss: 0.5786 - mae: 0.5786 - mse: 0.5658 - val_loss: 2.3638 - val_mae: 2.3638 - val_mse: 7.4098\n",
      "Epoch 75/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.6798 - mae: 0.6798 - mse: 0.7340\n",
      "Epoch 75: val_loss did not improve from 1.01405\n",
      "108/108 [==============================] - 49s 455ms/step - loss: 0.6798 - mae: 0.6798 - mse: 0.7340 - val_loss: 3.1136 - val_mae: 3.1136 - val_mse: 11.5619\n",
      "Epoch 76/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.5922 - mae: 0.5922 - mse: 0.5686\n",
      "Epoch 76: val_loss did not improve from 1.01405\n",
      "108/108 [==============================] - 49s 452ms/step - loss: 0.5922 - mae: 0.5922 - mse: 0.5686 - val_loss: 2.3517 - val_mae: 2.3517 - val_mse: 7.2209\n",
      "Epoch 77/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.5836 - mae: 0.5836 - mse: 0.5815\n",
      "Epoch 77: val_loss did not improve from 1.01405\n",
      "108/108 [==============================] - 48s 449ms/step - loss: 0.5836 - mae: 0.5836 - mse: 0.5815 - val_loss: 1.7479 - val_mae: 1.7479 - val_mse: 4.1713\n",
      "Epoch 78/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.5485 - mae: 0.5485 - mse: 0.4930\n",
      "Epoch 78: val_loss did not improve from 1.01405\n",
      "108/108 [==============================] - 49s 455ms/step - loss: 0.5485 - mae: 0.5485 - mse: 0.4930 - val_loss: 1.0580 - val_mae: 1.0580 - val_mse: 1.8336\n",
      "Epoch 79/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.5523 - mae: 0.5523 - mse: 0.4976\n",
      "Epoch 79: val_loss did not improve from 1.01405\n",
      "108/108 [==============================] - 49s 456ms/step - loss: 0.5523 - mae: 0.5523 - mse: 0.4976 - val_loss: 1.0952 - val_mae: 1.0952 - val_mse: 1.9055\n",
      "Epoch 80/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.5398 - mae: 0.5398 - mse: 0.4967\n",
      "Epoch 80: val_loss did not improve from 1.01405\n",
      "108/108 [==============================] - 49s 450ms/step - loss: 0.5398 - mae: 0.5398 - mse: 0.4967 - val_loss: 2.9917 - val_mae: 2.9917 - val_mse: 10.7347\n",
      "Epoch 81/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.5576 - mae: 0.5576 - mse: 0.4851\n",
      "Epoch 81: val_loss did not improve from 1.01405\n",
      "108/108 [==============================] - 49s 456ms/step - loss: 0.5576 - mae: 0.5576 - mse: 0.4851 - val_loss: 1.0464 - val_mae: 1.0464 - val_mse: 1.6525\n",
      "Epoch 82/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.5539 - mae: 0.5539 - mse: 0.5109\n",
      "Epoch 82: val_loss did not improve from 1.01405\n",
      "108/108 [==============================] - 50s 464ms/step - loss: 0.5539 - mae: 0.5539 - mse: 0.5109 - val_loss: 1.1322 - val_mae: 1.1322 - val_mse: 2.1149\n",
      "Epoch 83/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.5090 - mae: 0.5090 - mse: 0.4426\n",
      "Epoch 83: val_loss did not improve from 1.01405\n",
      "108/108 [==============================] - 49s 457ms/step - loss: 0.5090 - mae: 0.5090 - mse: 0.4426 - val_loss: 1.0842 - val_mae: 1.0842 - val_mse: 1.9451\n",
      "Epoch 84/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.5174 - mae: 0.5174 - mse: 0.4671\n",
      "Epoch 84: val_loss did not improve from 1.01405\n",
      "108/108 [==============================] - 50s 465ms/step - loss: 0.5174 - mae: 0.5174 - mse: 0.4671 - val_loss: 2.4180 - val_mae: 2.4180 - val_mse: 7.3545\n",
      "Epoch 85/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.5570 - mae: 0.5570 - mse: 0.5370\n",
      "Epoch 85: val_loss did not improve from 1.01405\n",
      "108/108 [==============================] - 49s 458ms/step - loss: 0.5570 - mae: 0.5570 - mse: 0.5370 - val_loss: 1.4369 - val_mae: 1.4369 - val_mse: 3.2842\n",
      "Epoch 86/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.5244 - mae: 0.5244 - mse: 0.4535\n",
      "Epoch 86: val_loss did not improve from 1.01405\n",
      "108/108 [==============================] - 50s 466ms/step - loss: 0.5244 - mae: 0.5244 - mse: 0.4535 - val_loss: 1.2495 - val_mae: 1.2495 - val_mse: 2.4429\n",
      "Epoch 87/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.5403 - mae: 0.5403 - mse: 0.4860\n",
      "Epoch 87: val_loss improved from 1.01405 to 0.93701, saving model to checkpoint-50-epochs-8-batchs.h5\n",
      "108/108 [==============================] - 51s 477ms/step - loss: 0.5403 - mae: 0.5403 - mse: 0.4860 - val_loss: 0.9370 - val_mae: 0.9370 - val_mse: 1.4993\n",
      "Epoch 88/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.5507 - mae: 0.5507 - mse: 0.4866\n",
      "Epoch 88: val_loss did not improve from 0.93701\n",
      "108/108 [==============================] - 50s 465ms/step - loss: 0.5507 - mae: 0.5507 - mse: 0.4866 - val_loss: 1.7272 - val_mae: 1.7272 - val_mse: 4.0775\n",
      "Epoch 89/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4345 - mae: 0.4345 - mse: 0.3263\n",
      "Epoch 89: val_loss did not improve from 0.93701\n",
      "108/108 [==============================] - 48s 449ms/step - loss: 0.4345 - mae: 0.4345 - mse: 0.3263 - val_loss: 1.6479 - val_mae: 1.6479 - val_mse: 3.7143\n",
      "Epoch 90/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.5272 - mae: 0.5272 - mse: 0.4592\n",
      "Epoch 90: val_loss did not improve from 0.93701\n",
      "108/108 [==============================] - 49s 455ms/step - loss: 0.5272 - mae: 0.5272 - mse: 0.4592 - val_loss: 1.0959 - val_mae: 1.0959 - val_mse: 1.8369\n",
      "Epoch 91/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.5563 - mae: 0.5563 - mse: 0.5141\n",
      "Epoch 91: val_loss did not improve from 0.93701\n",
      "108/108 [==============================] - 48s 447ms/step - loss: 0.5563 - mae: 0.5563 - mse: 0.5141 - val_loss: 1.0486 - val_mae: 1.0486 - val_mse: 1.7809\n",
      "Epoch 92/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4757 - mae: 0.4757 - mse: 0.3824\n",
      "Epoch 92: val_loss did not improve from 0.93701\n",
      "108/108 [==============================] - 49s 455ms/step - loss: 0.4757 - mae: 0.4757 - mse: 0.3824 - val_loss: 1.0243 - val_mae: 1.0243 - val_mse: 1.6910\n",
      "Epoch 93/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4714 - mae: 0.4714 - mse: 0.3637\n",
      "Epoch 93: val_loss did not improve from 0.93701\n",
      "108/108 [==============================] - 49s 454ms/step - loss: 0.4714 - mae: 0.4714 - mse: 0.3637 - val_loss: 0.9942 - val_mae: 0.9942 - val_mse: 1.6432\n",
      "Epoch 94/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.5140 - mae: 0.5140 - mse: 0.4305\n",
      "Epoch 94: val_loss did not improve from 0.93701\n",
      "108/108 [==============================] - 49s 455ms/step - loss: 0.5140 - mae: 0.5140 - mse: 0.4305 - val_loss: 2.5909 - val_mae: 2.5909 - val_mse: 8.2578\n",
      "Epoch 95/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4962 - mae: 0.4962 - mse: 0.4063\n",
      "Epoch 95: val_loss did not improve from 0.93701\n",
      "108/108 [==============================] - 49s 458ms/step - loss: 0.4962 - mae: 0.4962 - mse: 0.4063 - val_loss: 1.2067 - val_mae: 1.2067 - val_mse: 2.3053\n",
      "Epoch 96/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4853 - mae: 0.4853 - mse: 0.4009\n",
      "Epoch 96: val_loss did not improve from 0.93701\n",
      "108/108 [==============================] - 49s 456ms/step - loss: 0.4853 - mae: 0.4853 - mse: 0.4009 - val_loss: 1.0485 - val_mae: 1.0485 - val_mse: 1.8137\n",
      "Epoch 97/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.5142 - mae: 0.5142 - mse: 0.4304\n",
      "Epoch 97: val_loss did not improve from 0.93701\n",
      "108/108 [==============================] - 49s 457ms/step - loss: 0.5142 - mae: 0.5142 - mse: 0.4304 - val_loss: 1.5353 - val_mae: 1.5353 - val_mse: 3.5336\n",
      "Epoch 98/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4494 - mae: 0.4494 - mse: 0.3357\n",
      "Epoch 98: val_loss did not improve from 0.93701\n",
      "108/108 [==============================] - 51s 469ms/step - loss: 0.4494 - mae: 0.4494 - mse: 0.3357 - val_loss: 1.4080 - val_mae: 1.4080 - val_mse: 2.8766\n",
      "Epoch 99/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4890 - mae: 0.4890 - mse: 0.3912\n",
      "Epoch 99: val_loss did not improve from 0.93701\n",
      "108/108 [==============================] - 49s 458ms/step - loss: 0.4890 - mae: 0.4890 - mse: 0.3912 - val_loss: 1.3666 - val_mae: 1.3666 - val_mse: 2.6901\n",
      "Epoch 100/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4203 - mae: 0.4203 - mse: 0.2880\n",
      "Epoch 100: val_loss did not improve from 0.93701\n",
      "108/108 [==============================] - 49s 451ms/step - loss: 0.4203 - mae: 0.4203 - mse: 0.2880 - val_loss: 1.3889 - val_mae: 1.3889 - val_mse: 2.8475\n",
      "Epoch 101/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4330 - mae: 0.4330 - mse: 0.3201\n",
      "Epoch 101: val_loss did not improve from 0.93701\n",
      "108/108 [==============================] - 50s 464ms/step - loss: 0.4330 - mae: 0.4330 - mse: 0.3201 - val_loss: 1.0648 - val_mae: 1.0648 - val_mse: 1.7980\n",
      "Epoch 102/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.5154 - mae: 0.5154 - mse: 0.4309\n",
      "Epoch 102: val_loss did not improve from 0.93701\n",
      "108/108 [==============================] - 50s 466ms/step - loss: 0.5154 - mae: 0.5154 - mse: 0.4309 - val_loss: 1.3975 - val_mae: 1.3975 - val_mse: 2.7755\n",
      "Epoch 103/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4458 - mae: 0.4458 - mse: 0.3201\n",
      "Epoch 103: val_loss did not improve from 0.93701\n",
      "108/108 [==============================] - 49s 450ms/step - loss: 0.4458 - mae: 0.4458 - mse: 0.3201 - val_loss: 1.0565 - val_mae: 1.0565 - val_mse: 1.7540\n",
      "Epoch 104/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4322 - mae: 0.4322 - mse: 0.3016\n",
      "Epoch 104: val_loss did not improve from 0.93701\n",
      "108/108 [==============================] - 49s 457ms/step - loss: 0.4322 - mae: 0.4322 - mse: 0.3016 - val_loss: 3.2902 - val_mae: 3.2902 - val_mse: 12.4495\n",
      "Epoch 105/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4874 - mae: 0.4874 - mse: 0.3878\n",
      "Epoch 105: val_loss did not improve from 0.93701\n",
      "108/108 [==============================] - 50s 467ms/step - loss: 0.4874 - mae: 0.4874 - mse: 0.3878 - val_loss: 1.0598 - val_mae: 1.0598 - val_mse: 1.9341\n",
      "Epoch 106/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4633 - mae: 0.4633 - mse: 0.3541\n",
      "Epoch 106: val_loss did not improve from 0.93701\n",
      "108/108 [==============================] - 49s 456ms/step - loss: 0.4633 - mae: 0.4633 - mse: 0.3541 - val_loss: 1.5896 - val_mae: 1.5896 - val_mse: 3.4241\n",
      "Epoch 107/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.5043 - mae: 0.5043 - mse: 0.4226\n",
      "Epoch 107: val_loss did not improve from 0.93701\n",
      "108/108 [==============================] - 50s 465ms/step - loss: 0.5043 - mae: 0.5043 - mse: 0.4226 - val_loss: 1.0648 - val_mae: 1.0648 - val_mse: 1.9142\n",
      "Epoch 108/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4579 - mae: 0.4579 - mse: 0.3454\n",
      "Epoch 108: val_loss did not improve from 0.93701\n",
      "108/108 [==============================] - 50s 461ms/step - loss: 0.4579 - mae: 0.4579 - mse: 0.3454 - val_loss: 1.1972 - val_mae: 1.1972 - val_mse: 2.3729\n",
      "Epoch 109/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.5440 - mae: 0.5440 - mse: 0.4689\n",
      "Epoch 109: val_loss did not improve from 0.93701\n",
      "108/108 [==============================] - 49s 451ms/step - loss: 0.5440 - mae: 0.5440 - mse: 0.4689 - val_loss: 1.7657 - val_mae: 1.7657 - val_mse: 4.2744\n",
      "Epoch 110/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4371 - mae: 0.4371 - mse: 0.3197\n",
      "Epoch 110: val_loss did not improve from 0.93701\n",
      "108/108 [==============================] - 49s 455ms/step - loss: 0.4371 - mae: 0.4371 - mse: 0.3197 - val_loss: 0.9897 - val_mae: 0.9897 - val_mse: 1.5191\n",
      "Epoch 111/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4515 - mae: 0.4515 - mse: 0.3397\n",
      "Epoch 111: val_loss did not improve from 0.93701\n",
      "108/108 [==============================] - 50s 460ms/step - loss: 0.4515 - mae: 0.4515 - mse: 0.3397 - val_loss: 1.2892 - val_mae: 1.2892 - val_mse: 2.4200\n",
      "Epoch 112/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4873 - mae: 0.4873 - mse: 0.4187\n",
      "Epoch 112: val_loss did not improve from 0.93701\n",
      "108/108 [==============================] - 49s 456ms/step - loss: 0.4873 - mae: 0.4873 - mse: 0.4187 - val_loss: 1.0187 - val_mae: 1.0187 - val_mse: 1.7353\n",
      "Epoch 113/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4237 - mae: 0.4237 - mse: 0.2861\n",
      "Epoch 113: val_loss did not improve from 0.93701\n",
      "108/108 [==============================] - 49s 456ms/step - loss: 0.4237 - mae: 0.4237 - mse: 0.2861 - val_loss: 1.4762 - val_mae: 1.4762 - val_mse: 3.3112\n",
      "Epoch 114/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4952 - mae: 0.4952 - mse: 0.3834\n",
      "Epoch 114: val_loss did not improve from 0.93701\n",
      "108/108 [==============================] - 50s 461ms/step - loss: 0.4952 - mae: 0.4952 - mse: 0.3834 - val_loss: 1.9319 - val_mae: 1.9319 - val_mse: 4.9519\n",
      "Epoch 115/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4351 - mae: 0.4351 - mse: 0.3251\n",
      "Epoch 115: val_loss did not improve from 0.93701\n",
      "108/108 [==============================] - 49s 449ms/step - loss: 0.4351 - mae: 0.4351 - mse: 0.3251 - val_loss: 1.2390 - val_mae: 1.2390 - val_mse: 2.4131\n",
      "Epoch 116/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4390 - mae: 0.4390 - mse: 0.3148\n",
      "Epoch 116: val_loss did not improve from 0.93701\n",
      "108/108 [==============================] - 50s 465ms/step - loss: 0.4390 - mae: 0.4390 - mse: 0.3148 - val_loss: 1.2875 - val_mae: 1.2875 - val_mse: 2.4308\n",
      "Epoch 117/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4099 - mae: 0.4099 - mse: 0.2715\n",
      "Epoch 117: val_loss did not improve from 0.93701\n",
      "108/108 [==============================] - 49s 455ms/step - loss: 0.4099 - mae: 0.4099 - mse: 0.2715 - val_loss: 1.2071 - val_mae: 1.2071 - val_mse: 2.2342\n",
      "Epoch 118/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4493 - mae: 0.4493 - mse: 0.3253\n",
      "Epoch 118: val_loss did not improve from 0.93701\n",
      "108/108 [==============================] - 48s 447ms/step - loss: 0.4493 - mae: 0.4493 - mse: 0.3253 - val_loss: 1.2863 - val_mae: 1.2863 - val_mse: 2.6164\n",
      "Epoch 119/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4409 - mae: 0.4409 - mse: 0.3159\n",
      "Epoch 119: val_loss did not improve from 0.93701\n",
      "108/108 [==============================] - 49s 456ms/step - loss: 0.4409 - mae: 0.4409 - mse: 0.3159 - val_loss: 1.0294 - val_mae: 1.0294 - val_mse: 1.7226\n",
      "Epoch 120/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4054 - mae: 0.4054 - mse: 0.2588\n",
      "Epoch 120: val_loss improved from 0.93701 to 0.91297, saving model to checkpoint-50-epochs-8-batchs.h5\n",
      "108/108 [==============================] - 51s 475ms/step - loss: 0.4054 - mae: 0.4054 - mse: 0.2588 - val_loss: 0.9130 - val_mae: 0.9130 - val_mse: 1.4090\n",
      "Epoch 121/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4025 - mae: 0.4025 - mse: 0.2644\n",
      "Epoch 121: val_loss did not improve from 0.91297\n",
      "108/108 [==============================] - 49s 457ms/step - loss: 0.4025 - mae: 0.4025 - mse: 0.2644 - val_loss: 1.0012 - val_mae: 1.0012 - val_mse: 1.6343\n",
      "Epoch 122/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4367 - mae: 0.4367 - mse: 0.3106\n",
      "Epoch 122: val_loss did not improve from 0.91297\n",
      "108/108 [==============================] - 50s 461ms/step - loss: 0.4367 - mae: 0.4367 - mse: 0.3106 - val_loss: 0.9668 - val_mae: 0.9668 - val_mse: 1.5932\n",
      "Epoch 123/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4193 - mae: 0.4193 - mse: 0.2884\n",
      "Epoch 123: val_loss did not improve from 0.91297\n",
      "108/108 [==============================] - 50s 465ms/step - loss: 0.4193 - mae: 0.4193 - mse: 0.2884 - val_loss: 0.9582 - val_mae: 0.9582 - val_mse: 1.5732\n",
      "Epoch 124/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.3699 - mae: 0.3699 - mse: 0.2203\n",
      "Epoch 124: val_loss did not improve from 0.91297\n",
      "108/108 [==============================] - 49s 457ms/step - loss: 0.3699 - mae: 0.3699 - mse: 0.2203 - val_loss: 1.8545 - val_mae: 1.8545 - val_mse: 4.5344\n",
      "Epoch 125/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4197 - mae: 0.4197 - mse: 0.3011\n",
      "Epoch 125: val_loss did not improve from 0.91297\n",
      "108/108 [==============================] - 49s 454ms/step - loss: 0.4197 - mae: 0.4197 - mse: 0.3011 - val_loss: 1.0012 - val_mae: 1.0012 - val_mse: 1.6534\n",
      "Epoch 126/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4045 - mae: 0.4045 - mse: 0.2589\n",
      "Epoch 126: val_loss did not improve from 0.91297\n",
      "108/108 [==============================] - 49s 456ms/step - loss: 0.4045 - mae: 0.4045 - mse: 0.2589 - val_loss: 1.0186 - val_mae: 1.0186 - val_mse: 1.6587\n",
      "Epoch 127/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.3966 - mae: 0.3966 - mse: 0.2589\n",
      "Epoch 127: val_loss did not improve from 0.91297\n",
      "108/108 [==============================] - 49s 451ms/step - loss: 0.3966 - mae: 0.3966 - mse: 0.2589 - val_loss: 0.9567 - val_mae: 0.9567 - val_mse: 1.5498\n",
      "Epoch 128/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4189 - mae: 0.4189 - mse: 0.2844\n",
      "Epoch 128: val_loss did not improve from 0.91297\n",
      "108/108 [==============================] - 50s 459ms/step - loss: 0.4189 - mae: 0.4189 - mse: 0.2844 - val_loss: 1.1244 - val_mae: 1.1244 - val_mse: 1.9801\n",
      "Epoch 129/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4310 - mae: 0.4310 - mse: 0.3130\n",
      "Epoch 129: val_loss did not improve from 0.91297\n",
      "108/108 [==============================] - 50s 465ms/step - loss: 0.4310 - mae: 0.4310 - mse: 0.3130 - val_loss: 1.7393 - val_mae: 1.7393 - val_mse: 4.0333\n",
      "Epoch 130/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4163 - mae: 0.4163 - mse: 0.2776\n",
      "Epoch 130: val_loss did not improve from 0.91297\n",
      "108/108 [==============================] - 49s 457ms/step - loss: 0.4163 - mae: 0.4163 - mse: 0.2776 - val_loss: 1.2665 - val_mae: 1.2665 - val_mse: 2.5290\n",
      "Epoch 131/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4618 - mae: 0.4618 - mse: 0.3563\n",
      "Epoch 131: val_loss did not improve from 0.91297\n",
      "108/108 [==============================] - 49s 456ms/step - loss: 0.4618 - mae: 0.4618 - mse: 0.3563 - val_loss: 0.9497 - val_mae: 0.9497 - val_mse: 1.5637\n",
      "Epoch 132/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4204 - mae: 0.4204 - mse: 0.2721\n",
      "Epoch 132: val_loss did not improve from 0.91297\n",
      "108/108 [==============================] - 49s 455ms/step - loss: 0.4204 - mae: 0.4204 - mse: 0.2721 - val_loss: 1.0986 - val_mae: 1.0986 - val_mse: 1.8295\n",
      "Epoch 133/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.3717 - mae: 0.3717 - mse: 0.2254\n",
      "Epoch 133: val_loss did not improve from 0.91297\n",
      "108/108 [==============================] - 49s 449ms/step - loss: 0.3717 - mae: 0.3717 - mse: 0.2254 - val_loss: 1.0793 - val_mae: 1.0793 - val_mse: 1.9156\n",
      "Epoch 134/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4327 - mae: 0.4327 - mse: 0.3119\n",
      "Epoch 134: val_loss did not improve from 0.91297\n",
      "108/108 [==============================] - 50s 464ms/step - loss: 0.4327 - mae: 0.4327 - mse: 0.3119 - val_loss: 1.2049 - val_mae: 1.2049 - val_mse: 2.3215\n",
      "Epoch 135/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4261 - mae: 0.4261 - mse: 0.2919\n",
      "Epoch 135: val_loss did not improve from 0.91297\n",
      "108/108 [==============================] - 49s 450ms/step - loss: 0.4261 - mae: 0.4261 - mse: 0.2919 - val_loss: 1.9810 - val_mae: 1.9810 - val_mse: 4.9222\n",
      "Epoch 136/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4132 - mae: 0.4132 - mse: 0.2706\n",
      "Epoch 136: val_loss improved from 0.91297 to 0.91116, saving model to checkpoint-50-epochs-8-batchs.h5\n",
      "108/108 [==============================] - 53s 488ms/step - loss: 0.4132 - mae: 0.4132 - mse: 0.2706 - val_loss: 0.9112 - val_mae: 0.9112 - val_mse: 1.4049\n",
      "Epoch 137/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4020 - mae: 0.4020 - mse: 0.2578\n",
      "Epoch 137: val_loss did not improve from 0.91116\n",
      "108/108 [==============================] - 50s 465ms/step - loss: 0.4020 - mae: 0.4020 - mse: 0.2578 - val_loss: 1.6922 - val_mae: 1.6922 - val_mse: 4.1311\n",
      "Epoch 138/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4323 - mae: 0.4323 - mse: 0.3029\n",
      "Epoch 138: val_loss did not improve from 0.91116\n",
      "108/108 [==============================] - 49s 458ms/step - loss: 0.4323 - mae: 0.4323 - mse: 0.3029 - val_loss: 1.5159 - val_mae: 1.5159 - val_mse: 3.1848\n",
      "Epoch 139/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4008 - mae: 0.4008 - mse: 0.2662\n",
      "Epoch 139: val_loss did not improve from 0.91116\n",
      "108/108 [==============================] - 49s 456ms/step - loss: 0.4008 - mae: 0.4008 - mse: 0.2662 - val_loss: 1.7631 - val_mae: 1.7631 - val_mse: 4.1193\n",
      "Epoch 140/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.3911 - mae: 0.3911 - mse: 0.2574\n",
      "Epoch 140: val_loss did not improve from 0.91116\n",
      "108/108 [==============================] - 49s 455ms/step - loss: 0.3911 - mae: 0.3911 - mse: 0.2574 - val_loss: 1.6489 - val_mae: 1.6489 - val_mse: 3.7124\n",
      "Epoch 141/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.3595 - mae: 0.3595 - mse: 0.2131\n",
      "Epoch 141: val_loss did not improve from 0.91116\n",
      "108/108 [==============================] - 50s 464ms/step - loss: 0.3595 - mae: 0.3595 - mse: 0.2131 - val_loss: 1.2460 - val_mae: 1.2460 - val_mse: 2.3772\n",
      "Epoch 142/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4136 - mae: 0.4136 - mse: 0.2726\n",
      "Epoch 142: val_loss did not improve from 0.91116\n",
      "108/108 [==============================] - 49s 454ms/step - loss: 0.4136 - mae: 0.4136 - mse: 0.2726 - val_loss: 1.0328 - val_mae: 1.0328 - val_mse: 1.7418\n",
      "Epoch 143/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.3778 - mae: 0.3778 - mse: 0.2307\n",
      "Epoch 143: val_loss did not improve from 0.91116\n",
      "108/108 [==============================] - 50s 466ms/step - loss: 0.3778 - mae: 0.3778 - mse: 0.2307 - val_loss: 1.0642 - val_mae: 1.0642 - val_mse: 1.7701\n",
      "Epoch 144/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4309 - mae: 0.4309 - mse: 0.2914\n",
      "Epoch 144: val_loss did not improve from 0.91116\n",
      "108/108 [==============================] - 49s 454ms/step - loss: 0.4309 - mae: 0.4309 - mse: 0.2914 - val_loss: 1.3707 - val_mae: 1.3707 - val_mse: 2.6346\n",
      "Epoch 145/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.3531 - mae: 0.3531 - mse: 0.2051\n",
      "Epoch 145: val_loss did not improve from 0.91116\n",
      "108/108 [==============================] - 48s 449ms/step - loss: 0.3531 - mae: 0.3531 - mse: 0.2051 - val_loss: 1.0453 - val_mae: 1.0453 - val_mse: 1.7416\n",
      "Epoch 146/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.4123 - mae: 0.4123 - mse: 0.2733\n",
      "Epoch 146: val_loss did not improve from 0.91116\n",
      "108/108 [==============================] - 49s 453ms/step - loss: 0.4123 - mae: 0.4123 - mse: 0.2733 - val_loss: 1.5597 - val_mae: 1.5597 - val_mse: 3.5932\n",
      "Epoch 147/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.3435 - mae: 0.3435 - mse: 0.1920\n",
      "Epoch 147: val_loss did not improve from 0.91116\n",
      "108/108 [==============================] - 49s 454ms/step - loss: 0.3435 - mae: 0.3435 - mse: 0.1920 - val_loss: 1.1147 - val_mae: 1.1147 - val_mse: 1.8908\n",
      "Epoch 148/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.3487 - mae: 0.3487 - mse: 0.1949\n",
      "Epoch 148: val_loss did not improve from 0.91116\n",
      "108/108 [==============================] - 49s 456ms/step - loss: 0.3487 - mae: 0.3487 - mse: 0.1949 - val_loss: 0.9564 - val_mae: 0.9564 - val_mse: 1.6000\n",
      "Epoch 149/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.3510 - mae: 0.3510 - mse: 0.2015\n",
      "Epoch 149: val_loss did not improve from 0.91116\n",
      "108/108 [==============================] - 48s 449ms/step - loss: 0.3510 - mae: 0.3510 - mse: 0.2015 - val_loss: 1.1366 - val_mae: 1.1366 - val_mse: 2.0464\n",
      "Epoch 150/150\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.3739 - mae: 0.3739 - mse: 0.2195\n",
      "Epoch 150: val_loss did not improve from 0.91116\n",
      "108/108 [==============================] - 49s 455ms/step - loss: 0.3739 - mae: 0.3739 - mse: 0.2195 - val_loss: 1.1174 - val_mae: 1.1174 - val_mse: 2.0037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fef12c243a0>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=8, epochs=150, callbacks=checkpoint, validation_data=(X_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 147ms/step\n"
     ]
    }
   ],
   "source": [
    "test_prediction = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae : 1.111\n",
      "rmse : 1.400\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "print('mae : {:.3f}'.format(mean_absolute_error(test_prediction, y_test)))\n",
    "print('rmse : {:.3f}'.format(np.sqrt(mean_squared_error(test_prediction, y_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mini batch : 16\n",
    "+ crop 이미지 전처리 안했을 때 mae : 1.139, rmse : 1.422\n",
    "+ crop 이미지 전처리 했을 때 mae : 1.880, rmse : 2.354\n",
    "+ 앞으로 전처리 안하고 진행\n",
    "\n",
    "mini batch : 8\n",
    "+ mae : 1.111, rmse : 1.400"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "junoflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
